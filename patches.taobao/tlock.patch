From: Coly Li <bosong.ly@taobao.com>
Date: Wed Nov 16 19:38:43 CST 2011
Subject: [PATCH]tlock: port tlock_kern kernel module to 2.6.32 version
Patch-mainline: based on redhat 2.6.32
Reference:

tlock kernel module "tlock_kern" is used by Taobao tdbm application.
In order to update the OS kernel used by tdbm application to 2.6.32,
this kernel module is ported from 2.6.18 kernel to 2.6.32.

This patch adds 2 wrapper routines in kernel:
- tlock_fput_light
- tlock_fget_light
because fput_light() and fget_light() are not exported as KABI any
more, these two wrappers are exported as KABI for tlock_kern module.

The tlock module is modified to use tlock_fput_light() and
tlock_fget_light() currently.

Comparing tlock_kern source code in 2.6.18, this patch also fixes
2 possible memory leak. 2.6.18 tlock_kern code does not handle
some errors properly, in this version it should be a little better.

In 2.6.32 kernel, tlock_kern kernel module is shipped with kernel rpm,
when kenrel rpm is installed, the .ko file can be found as
/lib/modules/<kernel version>/kernel/drivers/misc/tlock_kern.ko

Orignal author of tlock_kern code is Robin Dong <sanbai@taobao.com>,
I do appreciate for his help during the code porting.

Signed-off-by: Coly Li <bosong.ly@taobao.com>
Cc: Robin Dong <sanbai@taobao.com>
---
Index: linux-2.6.32-220.17.1.el5/fs/file_table.c
===================================================================
--- linux-2.6.32-220.17.1.el5.orig/fs/file_table.c	2012-05-17 02:20:36.772553439 +0800
+++ linux-2.6.32-220.17.1.el5/fs/file_table.c	2012-05-17 02:20:53.400635891 +0800
@@ -320,6 +320,18 @@
 	return file;
 }
 
+/* wrapper routines for Taobao tlock */
+struct file *tlock_fget_light(unsigned int fd, int *fput_needed)
+{
+	return fget_light(fd, fput_needed);
+}
+EXPORT_SYMBOL(tlock_fget_light);
+
+void tlock_fput_light(struct file *file, int fput_needed)
+{
+	fput_light(file, fput_needed);
+}
+EXPORT_SYMBOL(tlock_fput_light);
 
 void put_filp(struct file *file)
 {
Index: linux-2.6.32-220.17.1.el5/include/linux/sched.h
===================================================================
--- linux-2.6.32-220.17.1.el5.orig/include/linux/sched.h	2012-05-17 02:20:36.732553235 +0800
+++ linux-2.6.32-220.17.1.el5/include/linux/sched.h	2012-05-17 02:20:53.404635905 +0800
@@ -2735,6 +2735,10 @@
 	return task_rlimit_max(current, limit);
 }
 
+/* For taobao tlock */
+void read_lock_tasklist(void);
+void read_unlock_tasklist(void);
+
 #endif /* __KERNEL__ */
 
 #endif
Index: linux-2.6.32-220.17.1.el5/kernel/pid.c
===================================================================
--- linux-2.6.32-220.17.1.el5.orig/kernel/pid.c	2012-05-17 02:20:36.764553398 +0800
+++ linux-2.6.32-220.17.1.el5/kernel/pid.c	2012-05-17 02:20:53.404635905 +0800
@@ -422,6 +422,7 @@
 {
 	return pid_task(find_pid_ns(nr, ns), PIDTYPE_PID);
 }
+EXPORT_SYMBOL(find_task_by_pid_ns);
 
 struct task_struct *find_task_by_vpid(pid_t vnr)
 {
Index: linux-2.6.32-220.17.1.el5/kernel/sched.c
===================================================================
--- linux-2.6.32-220.17.1.el5.orig/kernel/sched.c	2012-05-17 02:20:36.760553374 +0800
+++ linux-2.6.32-220.17.1.el5/kernel/sched.c	2012-05-17 02:20:53.404635905 +0800
@@ -1040,6 +1040,17 @@
 }
 early_param("nortsched", parse_nortsched);
 
+void read_lock_tasklist(void)
+{
+	read_lock(&tasklist_lock);
+}
+EXPORT_SYMBOL(read_lock_tasklist);
+
+void read_unlock_tasklist(void)
+{
+	read_unlock(&tasklist_lock);
+}
+EXPORT_SYMBOL(read_unlock_tasklist);
 
 static inline u64 global_rt_period(void)
 {
Index: linux-2.6.32-220.17.1.el5/drivers/misc/tlock/Makefile
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.32-220.17.1.el5/drivers/misc/tlock/Makefile	2012-05-17 02:20:53.408635927 +0800
@@ -0,0 +1,5 @@
+#
+# Makefile for misc devices that really don't fit anywhere else.
+#
+
+obj-$(CONFIG_TLOCK)		+= tlock_kern.o
Index: linux-2.6.32-220.17.1.el5/drivers/misc/tlock/queue.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.32-220.17.1.el5/drivers/misc/tlock/queue.h	2012-05-17 02:20:53.408635927 +0800
@@ -0,0 +1,549 @@
+/*
+ * Copyright (c) 1991, 1993
+ *	The Regents of the University of California.  All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. All advertising materials mentioning features or use of this software
+ *    must display the following acknowledgement:
+ *	This product includes software developed by the University of
+ *	California, Berkeley and its contributors.
+ * 4. Neither the name of the University nor the names of its contributors
+ *    may be used to endorse or promote products derived from this software
+ *    without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ *	@(#)queue.h	8.5 (Berkeley) 8/20/94
+ * $FreeBSD: src/sys/sys/queue.h,v 1.32.2.5 2001/09/30 21:12:54 luigi Exp $
+ */
+
+#ifndef _SYS_QUEUE_H_
+#define	_SYS_QUEUE_H_
+
+#if 0
+#include <machine/ansi.h>	/* for __offsetof */
+#endif
+
+/*
+ * This file defines five types of data structures: singly-linked lists,
+ * singly-linked tail queues, lists, tail queues, and circular queues.
+ *
+ * A singly-linked list is headed by a single forward pointer. The elements
+ * are singly linked for minimum space and pointer manipulation overhead at
+ * the expense of O(n) removal for arbitrary elements. New elements can be
+ * added to the list after an existing element or at the head of the list.
+ * Elements being removed from the head of the list should use the explicit
+ * macro for this purpose for optimum efficiency. A singly-linked list may
+ * only be traversed in the forward direction.  Singly-linked lists are ideal
+ * for applications with large datasets and few or no removals or for
+ * implementing a LIFO queue.
+ *
+ * A singly-linked tail queue is headed by a pair of pointers, one to the
+ * head of the list and the other to the tail of the list. The elements are
+ * singly linked for minimum space and pointer manipulation overhead at the
+ * expense of O(n) removal for arbitrary elements. New elements can be added
+ * to the list after an existing element, at the head of the list, or at the
+ * end of the list. Elements being removed from the head of the tail queue
+ * should use the explicit macro for this purpose for optimum efficiency.
+ * A singly-linked tail queue may only be traversed in the forward direction.
+ * Singly-linked tail queues are ideal for applications with large datasets
+ * and few or no removals or for implementing a FIFO queue.
+ *
+ * A list is headed by a single forward pointer (or an array of forward
+ * pointers for a hash table header). The elements are doubly linked
+ * so that an arbitrary element can be removed without a need to
+ * traverse the list. New elements can be added to the list before
+ * or after an existing element or at the head of the list. A list
+ * may only be traversed in the forward direction.
+ *
+ * A tail queue is headed by a pair of pointers, one to the head of the
+ * list and the other to the tail of the list. The elements are doubly
+ * linked so that an arbitrary element can be removed without a need to
+ * traverse the list. New elements can be added to the list before or
+ * after an existing element, at the head of the list, or at the end of
+ * the list. A tail queue may be traversed in either direction.
+ *
+ * A circle queue is headed by a pair of pointers, one to the head of the
+ * list and the other to the tail of the list. The elements are doubly
+ * linked so that an arbitrary element can be removed without a need to
+ * traverse the list. New elements can be added to the list before or after
+ * an existing element, at the head of the list, or at the end of the list.
+ * A circle queue may be traversed in either direction, but has a more
+ * complex end of list detection.
+ *
+ * For details on the use of these macros, see the queue(3) manual page.
+ *
+ *
+ *			SLIST	LIST	STAILQ	TAILQ	CIRCLEQ
+ * _HEAD		+	+	+	+	+
+ * _ENTRY		+	+	+	+	+
+ * _INIT		+	+	+	+	+
+ * _EMPTY		+	+	+	+	+
+ * _FIRST		+	+	+	+	+
+ * _NEXT		+	+	+	+	+
+ * _PREV		-	-	-	+	+
+ * _LAST		-	-	+	+	+
+ * _FOREACH		+	+	+	+	+
+ * _FOREACH_REVERSE	-	-	-	+	+
+ * _INSERT_HEAD		+	+	+	+	+
+ * _INSERT_BEFORE	-	+	-	+	+
+ * _INSERT_AFTER	+	+	+	+	+
+ * _INSERT_TAIL		-	-	+	+	+
+ * _REMOVE_HEAD		+	-	+	-	-
+ * _REMOVE		+	+	+	+	+
+ *
+ */
+
+/*
+ * Singly-linked List definitions.
+ */
+#define SLIST_HEAD(name, type)						\
+struct name {								\
+	struct type *slh_first;	/* first element */			\
+}
+
+#define SLIST_HEAD_INITIALIZER(head)					\
+	{ NULL }
+
+#define SLIST_ENTRY(type)						\
+struct {								\
+	struct type *sle_next;	/* next element */			\
+}
+
+/*
+ * Singly-linked List functions.
+ */
+#define	SLIST_EMPTY(head)	((head)->slh_first == NULL)
+
+#define	SLIST_FIRST(head)	((head)->slh_first)
+
+#define SLIST_FOREACH(var, head, field)					\
+	for((var) = (head)->slh_first; (var); (var) = (var)->field.sle_next)
+
+#define SLIST_INIT(head) {						\
+	(head)->slh_first = NULL;					\
+}
+
+#define SLIST_INSERT_AFTER(slistelm, elm, field) do {			\
+	(elm)->field.sle_next = (slistelm)->field.sle_next;		\
+	(slistelm)->field.sle_next = (elm);				\
+} while (0)
+
+#define SLIST_INSERT_HEAD(head, elm, field) do {			\
+	(elm)->field.sle_next = (head)->slh_first;			\
+	(head)->slh_first = (elm);					\
+} while (0)
+
+#define SLIST_NEXT(elm, field)	((elm)->field.sle_next)
+
+#define SLIST_REMOVE_HEAD(head, field) do {				\
+	(head)->slh_first = (head)->slh_first->field.sle_next;		\
+} while (0)
+
+#define SLIST_REMOVE(head, elm, type, field) do {			\
+	if ((head)->slh_first == (elm)) {				\
+		SLIST_REMOVE_HEAD((head), field);			\
+	}								\
+	else {								\
+		struct type *curelm = (head)->slh_first;		\
+		while( curelm->field.sle_next != (elm) )		\
+			curelm = curelm->field.sle_next;		\
+		curelm->field.sle_next =				\
+		    curelm->field.sle_next->field.sle_next;		\
+	}								\
+} while (0)
+
+/*
+ * Singly-linked Tail queue definitions.
+ */
+#define STAILQ_HEAD(name, type)						\
+struct name {								\
+	struct type *stqh_first;/* first element */			\
+	struct type **stqh_last;/* addr of last next element */		\
+}
+
+#define STAILQ_HEAD_INITIALIZER(head)					\
+	{ NULL, &(head).stqh_first }
+
+#define STAILQ_ENTRY(type)						\
+struct {								\
+	struct type *stqe_next;	/* next element */			\
+}
+
+/*
+ * Singly-linked Tail queue functions.
+ */
+#define STAILQ_EMPTY(head) ((head)->stqh_first == NULL)
+
+#define	STAILQ_INIT(head) do {						\
+	(head)->stqh_first = NULL;					\
+	(head)->stqh_last = &(head)->stqh_first;			\
+} while (0)
+
+#define STAILQ_FIRST(head)	((head)->stqh_first)
+
+#define	STAILQ_LAST(head, type, field)					\
+	(STAILQ_EMPTY(head) ?						\
+		NULL :							\
+	        ((struct type *)					\
+		((char *)((head)->stqh_last) - __offsetof(struct type, field))))
+
+#define STAILQ_FOREACH(var, head, field)				\
+	for((var) = (head)->stqh_first; (var); (var) = (var)->field.stqe_next)
+
+#define STAILQ_INSERT_HEAD(head, elm, field) do {			\
+	if (((elm)->field.stqe_next = (head)->stqh_first) == NULL)	\
+		(head)->stqh_last = &(elm)->field.stqe_next;		\
+	(head)->stqh_first = (elm);					\
+} while (0)
+
+#define STAILQ_INSERT_TAIL(head, elm, field) do {			\
+	(elm)->field.stqe_next = NULL;					\
+	*(head)->stqh_last = (elm);					\
+	(head)->stqh_last = &(elm)->field.stqe_next;			\
+} while (0)
+
+#define STAILQ_INSERT_AFTER(head, tqelm, elm, field) do {		\
+	if (((elm)->field.stqe_next = (tqelm)->field.stqe_next) == NULL)\
+		(head)->stqh_last = &(elm)->field.stqe_next;		\
+	(tqelm)->field.stqe_next = (elm);				\
+} while (0)
+
+#define STAILQ_NEXT(elm, field)	((elm)->field.stqe_next)
+
+#define STAILQ_REMOVE_HEAD(head, field) do {				\
+	if (((head)->stqh_first =					\
+	     (head)->stqh_first->field.stqe_next) == NULL)		\
+		(head)->stqh_last = &(head)->stqh_first;		\
+} while (0)
+
+#define STAILQ_REMOVE_HEAD_UNTIL(head, elm, field) do {			\
+	if (((head)->stqh_first = (elm)->field.stqe_next) == NULL)	\
+		(head)->stqh_last = &(head)->stqh_first;		\
+} while (0)
+
+#define STAILQ_REMOVE(head, elm, type, field) do {			\
+	if ((head)->stqh_first == (elm)) {				\
+		STAILQ_REMOVE_HEAD(head, field);			\
+	}								\
+	else {								\
+		struct type *curelm = (head)->stqh_first;		\
+		while( curelm->field.stqe_next != (elm) )		\
+			curelm = curelm->field.stqe_next;		\
+		if((curelm->field.stqe_next =				\
+		    curelm->field.stqe_next->field.stqe_next) == NULL)	\
+			(head)->stqh_last = &(curelm)->field.stqe_next;	\
+	}								\
+} while (0)
+
+/*
+ * List definitions.
+ */
+#define MLIST_HEAD(name, type)						\
+struct name {								\
+	struct type *lh_first;	/* first element */			\
+}
+
+#define MLIST_HEAD_INITIALIZER(head)					\
+	{ NULL }
+
+#define MLIST_ENTRY(type)						\
+struct {								\
+	struct type *le_next;	/* next element */			\
+	struct type **le_prev;	/* address of previous next element */	\
+}
+
+/*
+ * List functions.
+ */
+
+#define	MLIST_EMPTY(head) ((head)->lh_first == NULL)
+
+#define MLIST_FIRST(head)	((head)->lh_first)
+
+#define MLIST_FOREACH(var, head, field)					\
+	for((var) = (head)->lh_first; (var); (var) = (var)->field.le_next)
+
+#define	MLIST_INIT(head) do {						\
+	(head)->lh_first = NULL;					\
+} while (0)
+
+#define MLIST_INSERT_AFTER(listelm, elm, field) do {			\
+	if (((elm)->field.le_next = (listelm)->field.le_next) != NULL)	\
+		(listelm)->field.le_next->field.le_prev =		\
+		    &(elm)->field.le_next;				\
+	(listelm)->field.le_next = (elm);				\
+	(elm)->field.le_prev = &(listelm)->field.le_next;		\
+} while (0)
+
+#define MLIST_INSERT_BEFORE(listelm, elm, field) do {			\
+	(elm)->field.le_prev = (listelm)->field.le_prev;		\
+	(elm)->field.le_next = (listelm);				\
+	*(listelm)->field.le_prev = (elm);				\
+	(listelm)->field.le_prev = &(elm)->field.le_next;		\
+} while (0)
+
+#define MLIST_INSERT_HEAD(head, elm, field) do {				\
+	if (((elm)->field.le_next = (head)->lh_first) != NULL)		\
+		(head)->lh_first->field.le_prev = &(elm)->field.le_next;\
+	(head)->lh_first = (elm);					\
+	(elm)->field.le_prev = &(head)->lh_first;			\
+} while (0)
+
+#define MLIST_NEXT(elm, field)	((elm)->field.le_next)
+
+#define MLIST_REMOVE(elm, field) do {					\
+	*(elm)->field.le_prev = (elm)->field.le_next;			\
+	if ((elm)->field.le_next != NULL)				\
+		(elm)->field.le_next->field.le_prev = 			\
+		    (elm)->field.le_prev;				\
+} while (0)
+
+/*
+ * Tail queue definitions.
+ */
+#define TAILQ_HEAD(name, type)						\
+struct name {								\
+	struct type *tqh_first;	/* first element */			\
+	struct type **tqh_last;	/* addr of last next element */		\
+}
+
+#define TAILQ_HEAD_INITIALIZER(head)					\
+	{ NULL, &(head).tqh_first }
+
+#define TAILQ_ENTRY(type)						\
+struct {								\
+	struct type *tqe_next;	/* next element */			\
+	struct type **tqe_prev;	/* address of previous next element */	\
+}
+
+/*
+ * Tail queue functions.
+ */
+#define	TAILQ_EMPTY(head) ((head)->tqh_first == NULL)
+
+#define TAILQ_FOREACH(var, head, field)					\
+	for (var = TAILQ_FIRST(head); var; var = TAILQ_NEXT(var, field))
+
+#define TAILQ_FOREACH_REVERSE(var, head, headname, field)		\
+	for ((var) = TAILQ_LAST((head), headname);			\
+	     (var);							\
+	     (var) = TAILQ_PREV((var), headname, field))
+
+#define	TAILQ_FIRST(head) ((head)->tqh_first)
+
+#define	TAILQ_LAST(head, headname) \
+	(*(((struct headname *)((head)->tqh_last))->tqh_last))
+
+#define	TAILQ_NEXT(elm, field) ((elm)->field.tqe_next)
+
+#define TAILQ_PREV(elm, headname, field) \
+	(*(((struct headname *)((elm)->field.tqe_prev))->tqh_last))
+
+#define	TAILQ_INIT(head) do {						\
+	(head)->tqh_first = NULL;					\
+	(head)->tqh_last = &(head)->tqh_first;				\
+} while (0)
+
+#define TAILQ_INSERT_HEAD(head, elm, field) do {			\
+	if (((elm)->field.tqe_next = (head)->tqh_first) != NULL)	\
+		(head)->tqh_first->field.tqe_prev =			\
+		    &(elm)->field.tqe_next;				\
+	else								\
+		(head)->tqh_last = &(elm)->field.tqe_next;		\
+	(head)->tqh_first = (elm);					\
+	(elm)->field.tqe_prev = &(head)->tqh_first;			\
+} while (0)
+
+#define TAILQ_INSERT_TAIL(head, elm, field) do {			\
+	(elm)->field.tqe_next = NULL;					\
+	(elm)->field.tqe_prev = (head)->tqh_last;			\
+	*(head)->tqh_last = (elm);					\
+	(head)->tqh_last = &(elm)->field.tqe_next;			\
+} while (0)
+
+#define TAILQ_INSERT_AFTER(head, listelm, elm, field) do {		\
+	if (((elm)->field.tqe_next = (listelm)->field.tqe_next) != NULL)\
+		(elm)->field.tqe_next->field.tqe_prev = 		\
+		    &(elm)->field.tqe_next;				\
+	else								\
+		(head)->tqh_last = &(elm)->field.tqe_next;		\
+	(listelm)->field.tqe_next = (elm);				\
+	(elm)->field.tqe_prev = &(listelm)->field.tqe_next;		\
+} while (0)
+
+#define TAILQ_INSERT_BEFORE(listelm, elm, field) do {			\
+	(elm)->field.tqe_prev = (listelm)->field.tqe_prev;		\
+	(elm)->field.tqe_next = (listelm);				\
+	*(listelm)->field.tqe_prev = (elm);				\
+	(listelm)->field.tqe_prev = &(elm)->field.tqe_next;		\
+} while (0)
+
+#define TAILQ_REMOVE(head, elm, field) do {				\
+	if (((elm)->field.tqe_next) != NULL)				\
+		(elm)->field.tqe_next->field.tqe_prev = 		\
+		    (elm)->field.tqe_prev;				\
+	else								\
+		(head)->tqh_last = (elm)->field.tqe_prev;		\
+	*(elm)->field.tqe_prev = (elm)->field.tqe_next;			\
+} while (0)
+
+/*
+ * Circular queue definitions.
+ */
+#define CIRCLEQ_HEAD(name, type)					\
+struct name {								\
+	struct type *cqh_first;		/* first element */		\
+	struct type *cqh_last;		/* last element */		\
+}
+
+#define CIRCLEQ_ENTRY(type)						\
+struct {								\
+	struct type *cqe_next;		/* next element */		\
+	struct type *cqe_prev;		/* previous element */		\
+}
+
+/*
+ * Circular queue functions.
+ */
+#define CIRCLEQ_EMPTY(head) ((head)->cqh_first == (void *)(head))
+
+#define CIRCLEQ_FIRST(head) ((head)->cqh_first)
+
+#define CIRCLEQ_FOREACH(var, head, field)				\
+	for((var) = (head)->cqh_first;					\
+	    (var) != (void *)(head);					\
+	    (var) = (var)->field.cqe_next)
+
+#define CIRCLEQ_FOREACH_REVERSE(var, head, field)			\
+	for((var) = (head)->cqh_last;					\
+	    (var) != (void *)(head);					\
+	    (var) = (var)->field.cqe_prev)
+
+#define	CIRCLEQ_INIT(head) do {						\
+	(head)->cqh_first = (void *)(head);				\
+	(head)->cqh_last = (void *)(head);				\
+} while (0)
+
+#define CIRCLEQ_INSERT_AFTER(head, listelm, elm, field) do {		\
+	(elm)->field.cqe_next = (listelm)->field.cqe_next;		\
+	(elm)->field.cqe_prev = (listelm);				\
+	if ((listelm)->field.cqe_next == (void *)(head))		\
+		(head)->cqh_last = (elm);				\
+	else								\
+		(listelm)->field.cqe_next->field.cqe_prev = (elm);	\
+	(listelm)->field.cqe_next = (elm);				\
+} while (0)
+
+#define CIRCLEQ_INSERT_BEFORE(head, listelm, elm, field) do {		\
+	(elm)->field.cqe_next = (listelm);				\
+	(elm)->field.cqe_prev = (listelm)->field.cqe_prev;		\
+	if ((listelm)->field.cqe_prev == (void *)(head))		\
+		(head)->cqh_first = (elm);				\
+	else								\
+		(listelm)->field.cqe_prev->field.cqe_next = (elm);	\
+	(listelm)->field.cqe_prev = (elm);				\
+} while (0)
+
+#define CIRCLEQ_INSERT_HEAD(head, elm, field) do {			\
+	(elm)->field.cqe_next = (head)->cqh_first;			\
+	(elm)->field.cqe_prev = (void *)(head);				\
+	if ((head)->cqh_last == (void *)(head))				\
+		(head)->cqh_last = (elm);				\
+	else								\
+		(head)->cqh_first->field.cqe_prev = (elm);		\
+	(head)->cqh_first = (elm);					\
+} while (0)
+
+#define CIRCLEQ_INSERT_TAIL(head, elm, field) do {			\
+	(elm)->field.cqe_next = (void *)(head);				\
+	(elm)->field.cqe_prev = (head)->cqh_last;			\
+	if ((head)->cqh_first == (void *)(head))			\
+		(head)->cqh_first = (elm);				\
+	else								\
+		(head)->cqh_last->field.cqe_next = (elm);		\
+	(head)->cqh_last = (elm);					\
+} while (0)
+
+#define CIRCLEQ_LAST(head) ((head)->cqh_last)
+
+#define CIRCLEQ_NEXT(elm,field) ((elm)->field.cqe_next)
+
+#define CIRCLEQ_PREV(elm,field) ((elm)->field.cqe_prev)
+
+#define	CIRCLEQ_REMOVE(head, elm, field) do {				\
+	if ((elm)->field.cqe_next == (void *)(head))			\
+		(head)->cqh_last = (elm)->field.cqe_prev;		\
+	else								\
+		(elm)->field.cqe_next->field.cqe_prev =			\
+		    (elm)->field.cqe_prev;				\
+	if ((elm)->field.cqe_prev == (void *)(head))			\
+		(head)->cqh_first = (elm)->field.cqe_next;		\
+	else								\
+		(elm)->field.cqe_prev->field.cqe_next =			\
+		    (elm)->field.cqe_next;				\
+} while (0)
+
+#ifdef _KERNEL
+
+/*
+ * XXX insque() and remque() are an old way of handling certain queues.
+ * They bogusly assumes that all queue heads look alike.
+ */
+
+struct quehead {
+	struct quehead *qh_link;
+	struct quehead *qh_rlink;
+};
+
+#ifdef	__GNUC__
+
+static __inline void
+insque(void *a, void *b)
+{
+	struct quehead *element = (struct quehead *)a,
+		*head = (struct quehead *)b;
+
+	element->qh_link = head->qh_link;
+	element->qh_rlink = head;
+	head->qh_link = element;
+	element->qh_link->qh_rlink = element;
+}
+
+static __inline void
+remque(void *a)
+{
+	struct quehead *element = (struct quehead *)a;
+
+	element->qh_link->qh_rlink = element->qh_rlink;
+	element->qh_rlink->qh_link = element->qh_link;
+	element->qh_rlink = 0;
+}
+
+#else /* !__GNUC__ */
+
+void	insque __P((void *a, void *b));
+void	remque __P((void *a));
+
+#endif /* __GNUC__ */
+
+#endif /* _KERNEL */
+
+#endif /* !_SYS_QUEUE_H_ */
Index: linux-2.6.32-220.17.1.el5/drivers/misc/tlock/tlock_kern.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.32-220.17.1.el5/drivers/misc/tlock/tlock_kern.c	2012-05-17 02:20:53.408635927 +0800
@@ -0,0 +1,648 @@
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/types.h>
+#include <linux/semaphore.h>
+#include <linux/fdtable.h>
+#include <linux/fs.h>
+#include <linux/uaccess.h>
+#include <linux/file.h>
+#include <linux/miscdevice.h>
+
+#include "queue.h"
+
+#define TLOCK_INIT_FILE_NUM	0
+#define TLOCK_LOCK_NUM		1
+#define TLOCK_UNLOCK_NUM	2
+
+#define TLOCK_TYPE		0xd3
+
+#define TLOCK_INIT_FILE	0xd300
+#define TLOCK_LOCK	0xd301
+#define TLOCK_UNLOCK	0xd302
+#define TLOCK_STAT	0xd303
+
+#define MAX_FILE_PER_TASK	128
+
+#define TLOCK_PID_MASK		0x001fffff
+#define TLOCK_COUNT_MASK	0x7fe00000
+
+#define __wait_event_exclusive_interruptible_timeout(wq, condition, ret)	\
+do {										\
+	DEFINE_WAIT(__wait);							\
+	for(;;) {								\
+		prepare_to_wait_exclusive(					\
+			&wq, &__wait, TASK_INTERRUPTIBLE);			\
+		if (condition)							\
+			break;							\
+		if(!signal_pending(current)) {					\
+			ret = schedule_timeout(ret);				\
+			if (ret)						\
+				continue;					\
+			break;							\
+		}								\
+		ret = -ERESTARTSYS;						\
+		break;								\
+	}									\
+	finish_wait(&wq, &wait);						\
+} while(0)
+
+#define __wait_interruptible(wq, ret)						\
+do {										\
+	DEFINE_WAIT(__wait);							\
+	prepare_to_wait_exclusive(&wq, &__wait, TASK_INTERRUPTIBLE);		\
+	if (!signal_pending(current))						\
+		ret = schedule_timeout(ret);					\
+	finish_wait(&wq, &__wait);						\
+} while(0)
+
+#define wait_event_exclusive_interruptible_timeout(wq, condition, timeout)	\
+({										\
+	long __ret = timeout;							\
+	if (!(condition))							\
+		__wait_event_exclusive_interruptible_timeout(			\
+			wq, condition, __ret);					\
+	ret;									\
+})
+
+#define wait_exclusive_interruptible_timeout(wq, timeout)			\
+({										\
+	long __ret = timeout;							\
+	__wait_interruptible(wq, __ret);					\
+	__ret;									\
+})
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("donghao");
+MODULE_DESCRIPTION("tlock device driver");
+
+typedef struct tlock_args {
+	int fd;
+	int align1;
+	uint64_t offset;
+	uint64_t addr;
+} tlock_args_t;
+
+typedef struct wait_queue {
+	wait_queue_head_t	waitq_outer;
+	spinlock_t		spin_lock;
+	int			sig_flag;
+} tlock_wait_queue;
+
+typedef struct tlock_file {
+	MLIST_ENTRY(tlock_file)	link;
+	MLIST_HEAD(,tlock_task)	task_head;
+	struct inode		*inode;
+	size_t			task_num;
+	size_t			lock_num;
+	tlock_wait_queue	**tlock_waitq;
+	spinlock_t		spin_lock;
+} tlock_file_t;
+
+typedef struct tlock_task {
+	MLIST_ENTRY(tlock_task)	link;
+	pid_t			pid;
+	pid_t			leader_pid;
+	size_t			offset;
+	tlock_file_t		**lock_file_array;
+	size_t			filep_used_num;
+	size_t			filep_num;
+} tlock_task_t;
+
+MLIST_HEAD(,tlock_file) g_file_list = {NULL};
+DECLARE_MUTEX(g_sema_file);
+
+MLIST_HEAD(, tlock_task) g_task_list = {NULL};
+DECLARE_MUTEX(g_sema_task);
+
+#define TASK_BUCKET_SHIFT	10
+#define TASK_BUCKET_NUM		(1 << TASK_BUCKET_SHIFT)
+#define TASK_BUCKET_MARK	(TASK_BUCKET_NUM - 1)
+
+typedef struct tlock_hash_task {
+	MLIST_ENTRY(tlock_hash_task) link;
+	tlock_task_t *ptr_tlock_task;
+} tlock_hash_task_t;
+
+MLIST_HEAD(, tlock_hash_task) g_tlock_task_htable[TASK_BUCKET_NUM] = {{NULL}};
+
+inline void add_file_to_task(tlock_task_t* task, tlock_file_t* file);
+inline tlock_task_t* alloc_tlock_task(tlock_file_t* file, size_t offset);
+inline tlock_hash_task_t* alloc_tlock_hash_task(tlock_task_t * ptr_tlock_task);
+inline void free_tlock_task(tlock_task_t* task);
+inline tlock_task_t* find_tlock_task(struct task_struct* ts);
+inline tlock_file_t* alloc_tlock_file(struct inode* node, size_t lock_num);
+inline void free_tlock_file(tlock_file_t* file);
+inline void update_list(tlock_args_t* args, struct inode *node);
+inline int lock_list(tlock_args_t* args, struct inode* node);
+inline void unlock_list(tlock_args_t* args, struct inode *node);
+inline int lock_in_file(tlock_file_t* file, size_t offset, caddr_t addr);
+inline int unlock_in_file(tlock_file_t* file, size_t offset, caddr_t addr);
+inline tlock_file_t* find_file_in_list(struct inode* node);
+
+/* for tlock stat tool */
+static void traverse_tlock_task(void)
+{
+	int i;
+	tlock_task_t *task;
+	tlock_hash_task_t *htask;
+
+	printk("begin-- print tlock task info...............\n\n");
+	down(&g_sema_task);
+
+	printk("begin-- print tlock task....................\n");
+	task = MLIST_FIRST(&g_task_list);
+	while(task) {
+		printk("task pid = %d, task thread_group leader = %d\n", task->pid, task->leader_pid);
+		task = MLIST_NEXT(task, link);
+	}
+	printk("end--   print tlock task....................\n\n");
+
+	printk("begin-- print tlock hash task...............\n");
+	for(i = 0; i < TASK_BUCKET_NUM; i++) {
+		htask = MLIST_FIRST(&g_tlock_task_htable[i]);
+		while(htask) {
+			printk("bucket number = %d,hash_table task leader pid = %d\n", i, htask->ptr_tlock_task->leader_pid);
+			htask = MLIST_NEXT(htask, link);
+		}
+	}
+
+	printk("end--   print tlock hash task...............\n\n");
+
+	printk("end--   print tlock task info...............\n");
+
+	up(&g_sema_task);
+}
+
+int tlock_ioctl(struct inode *inode, struct file *dev_file, unsigned int cmd, unsigned long data)
+{
+	tlock_args_t args;
+	struct inode *node;
+	struct file *file;
+	int fput_needed;
+	int ret = 0;
+
+	if (cmd == TLOCK_STAT) {
+		traverse_tlock_task();
+		return 0;
+	}
+
+	if (copy_from_user(&args, (void *)data, sizeof(tlock_args_t)))
+		return -EFAULT;
+
+	file = tlock_fget_light(args.fd, &fput_needed);
+	if (!file)
+		return -EFAULT;
+
+	node = file->f_dentry->d_inode;
+	if (!node) {
+		tlock_fput_light(file, fput_needed);
+		return -EFAULT;
+	}
+
+	switch(cmd) {
+	case TLOCK_INIT_FILE:
+		down(&g_sema_task);
+		down(&g_sema_file);
+		update_list(&args, node);
+		up(&g_sema_file);
+		up(&g_sema_task);
+		break;
+	case TLOCK_LOCK:
+		ret = lock_list(&args, node);
+		break;
+	case TLOCK_UNLOCK:
+		unlock_list(&args, node);
+		break;
+	default:
+		printk(KERN_ALERT "unknow cmd %d in %s:%d", cmd, __func__, __LINE__);
+		ret = -EINVAL;
+		break;
+	}
+
+	tlock_fput_light(file, fput_needed);
+	return ret;
+}
+
+int tlock_flush(struct file *dev_file, fl_owner_t lock)
+{
+	tlock_task_t *old_task, *task;
+	tlock_hash_task_t *old_htask, *htask;
+	int ret = -EFAULT;
+
+	down(&g_sema_task);
+	down(&g_sema_file);
+
+	if (MLIST_EMPTY(&g_task_list))
+		goto out;
+
+	task = MLIST_FIRST(&g_task_list);
+	while(task) {
+		if (task->pid == current->pid ||
+		    task->leader_pid == current->tgid) {
+			htask = MLIST_FIRST( &g_tlock_task_htable[
+					TASK_BUCKET_MARK & task->leader_pid]);
+			while(htask) {
+				if (htask->ptr_tlock_task->leader_pid ==
+				    task->leader_pid) {
+					old_htask = htask;
+					htask = MLIST_NEXT(htask, link);
+					MLIST_REMOVE(old_htask, link);
+					kfree(old_htask);
+					if (!htask)
+						break;
+				}
+				htask = MLIST_NEXT(htask, link);
+			}
+
+			old_task = task;
+			free_tlock_task(task);
+
+			task = MLIST_NEXT(task, link);
+			MLIST_REMOVE(old_task, link);
+			kfree(old_task);
+
+			if (!task)
+				break;
+		}
+		task = MLIST_NEXT(task, link);
+	}
+	ret = 0;
+out:
+	up(&g_sema_file);
+	up(&g_sema_task);
+	return ret;
+
+}
+
+tlock_task_t * alloc_tlock_task(tlock_file_t *file, size_t offset)
+{
+	tlock_task_t *task;
+
+	task = kmalloc(sizeof(tlock_task_t), GFP_KERNEL);
+	if (unlikely(!task)) {
+		printk(KERN_ALERT "nomem for task\n");
+		goto out;
+	}
+
+	task->pid = current->pid;
+	task->leader_pid = current->tgid;
+	task->offset = offset;
+	task->lock_file_array = kmalloc(sizeof(tlock_file_t *) * MAX_FILE_PER_TASK, GFP_KERNEL);
+	if (unlikely(!task->lock_file_array)) {
+		printk(KERN_ALERT "nomem for lock_file_array\n");
+		kfree(task);
+		task = NULL;
+		goto out;
+	}
+	task->filep_num = MAX_FILE_PER_TASK;
+	task->filep_used_num = 0;
+out:
+	return task;
+}
+
+tlock_hash_task_t * alloc_tlock_hash_task(tlock_task_t *ptr_tlock_task)
+{
+	tlock_hash_task_t *task;
+
+	task = kmalloc(sizeof(tlock_hash_task_t), GFP_KERNEL);
+	if (task)
+		task->ptr_tlock_task = ptr_tlock_task;
+	else
+		printk(KERN_ALERT "nomem for hash_task\n");
+
+	return task;
+}
+
+tlock_task_t * find_tlock_task(struct task_struct *ts)
+{
+	tlock_task_t *ret_task = NULL;
+	tlock_hash_task_t *htask;
+
+	down(&g_sema_task);
+
+	htask = MLIST_FIRST(&g_tlock_task_htable[TASK_BUCKET_MARK & ts->tgid]);
+	while (htask && (htask->ptr_tlock_task->leader_pid != ts->tgid))
+		htask = MLIST_NEXT(htask, link);
+
+	if (htask)
+		ret_task = htask->ptr_tlock_task;
+
+	up(&g_sema_task);
+	return ret_task;
+}
+
+void free_tlock_task(tlock_task_t *task)
+{
+	int i, lock_pos;
+	tlock_file_t *file;
+	tlock_wait_queue *wait_queue;
+
+	for(i = 0; i < task->filep_used_num; i++) {
+		file = task->lock_file_array[i];
+		if (!file)
+			continue;
+
+		file->task_num--;
+		if (file->task_num <= 0) {
+			MLIST_REMOVE(file, link);
+			free_tlock_file(file);
+			continue;
+		}
+
+		for (lock_pos = 0;
+		     lock_pos < file->lock_num;
+		     lock_pos ++) {
+			wait_queue = file->tlock_waitq[lock_pos];
+			if (wait_queue)
+				wake_up_interruptible_sync(
+					&wait_queue->waitq_outer);
+		}
+	}
+	kfree(task->lock_file_array);
+}
+
+tlock_file_t *alloc_tlock_file(struct inode *node, size_t lock_num)
+{
+	size_t i;
+	tlock_file_t *file;
+
+	file = kmalloc(sizeof(tlock_file_t), GFP_KERNEL);
+	if (unlikely(!file)) {
+		printk(KERN_ALERT "nomem for file\n");
+		goto out;
+	}
+
+	file->task_num = 0;
+	file->inode = node;
+	file->lock_num = lock_num;
+	spin_lock_init(&file->spin_lock);
+
+	file->tlock_waitq = kmalloc(sizeof(tlock_wait_queue *) * lock_num,
+				    GFP_KERNEL);
+	if (unlikely(!file->tlock_waitq)) {
+		printk(KERN_ALERT "nomen for waitq\n");
+		kfree(file);
+		file = NULL;
+		goto out;
+	}
+
+	for (i = 0; i < lock_num; i++)
+		file->tlock_waitq[i] = NULL;
+	MLIST_INSERT_HEAD(&g_file_list, file, link);
+
+out:
+	return file;
+}
+
+void free_tlock_file(tlock_file_t *file)
+{
+	size_t i;
+	tlock_wait_queue *wait_queue;
+
+	for(i = 0; i < file->lock_num; i++) {
+		wait_queue = file->tlock_waitq[i];
+		if (wait_queue)
+			kfree(wait_queue);
+	}
+
+	kfree(file->tlock_waitq);
+	kfree(file);
+}
+
+void update_list(tlock_args_t *args, struct inode *node)
+{
+	size_t i;
+	int fill_node = 0;
+	int error;
+	int *addr;
+	tlock_task_t *task;
+	tlock_hash_task_t *htask;
+	tlock_file_t *file;
+
+	file = find_file_in_list(node);
+	if (!file) {
+		file = alloc_tlock_file(node, args->offset);
+		if (!file) {
+			printk(KERN_ALERT "file struct NULL\n");
+			return;
+		}
+		addr = (int *)args->addr;
+		for (i = 0; i < args->offset; i++) {
+			error = copy_to_user(addr, &fill_node, sizeof(int));
+			addr++;
+		}
+	}
+
+	MLIST_FOREACH(task, &g_task_list, link) {
+		if (task->pid == current->pid) {
+			add_file_to_task(task, file);
+			return;
+		}
+	}
+
+	task = alloc_tlock_task(file, args->offset);
+	if (!task) {
+		printk(KERN_ALERT "task struct NULL\n");
+		return;
+	}
+
+	htask = alloc_tlock_hash_task(task);
+	if (!htask) {
+		printk(KERN_ALERT "hash_task struct NULL\n");
+		free_tlock_task(task);
+		return;
+	}
+
+	add_file_to_task(task, file);
+
+	MLIST_INSERT_HEAD(&g_task_list, task, link);
+	MLIST_INSERT_HEAD(&g_tlock_task_htable[current->tgid & TASK_BUCKET_MARK], htask, link);
+}
+
+void add_file_to_task(tlock_task_t *task, tlock_file_t *file)
+{
+	int i = 0;
+	for (i = 0; i < task->filep_used_num; i++) {
+		if (file == task->lock_file_array[i])
+			return;
+	}
+
+	if (task->filep_used_num >= MAX_FILE_PER_TASK) {
+		printk(KERN_ALERT "lock_file_arry full, add file to task failed.\n");
+		return;
+	}
+	task->lock_file_array[task->filep_used_num] = file;
+	task->filep_used_num ++;
+	file->task_num ++;
+}
+
+int lock_list(tlock_args_t *args, struct inode *node)
+{
+	tlock_file_t *file = find_file_in_list(node);
+	int ret = -EINVAL;
+
+	if (unlikely(!file))
+		goto out;
+
+	down(&g_sema_file);
+	file->task_num++;
+	up(&g_sema_file);
+
+	if (args->offset >= file->lock_num) {
+		printk(KERN_ALERT "offset too large\n");
+		goto out;
+	}
+
+	ret = lock_in_file(file, args->offset, (caddr_t)args->addr);
+out:
+	return ret;
+}
+
+void unlock_list(tlock_args_t *args, struct inode *node)
+{
+	tlock_file_t *file = find_file_in_list(node);
+
+	if (unlikely(!file))
+		return;
+
+	down(&g_sema_file);
+	file->task_num--;
+	up(&g_sema_file);
+
+	if (args->offset >= file->lock_num)
+		return;
+
+	unlock_in_file(file, args->offset, (caddr_t)args->addr);
+}
+
+
+tlock_file_t * find_file_in_list(struct inode *node)
+{
+	tlock_file_t *file = NULL;
+
+	MLIST_FOREACH(file, &g_file_list, link) {
+		if (file && file->inode == node)
+			break;
+	}
+
+	if (MLIST_EMPTY(&g_file_list) || !file || file->inode != node)
+		return NULL;
+
+	return file;
+}
+
+int lock_in_file(tlock_file_t *file, size_t offset, caddr_t addr)
+{
+	tlock_wait_queue *wait_queue;
+	void *tmp;
+	pid_t cur_pid;
+	struct task_struct * pts;
+	tlock_task_t *ptlts = NULL;
+	int mutex;
+	int ret = 0;
+	int should_free = 0;
+
+	wait_queue = file->tlock_waitq[offset];
+	if (likely(wait_queue))
+		goto lock_wait_pid;
+
+	tmp = kmalloc(sizeof(tlock_wait_queue), GFP_KERNEL);
+	if (unlikely(!tmp)) {
+		printk(KERN_ALERT "nomemory for waitq\n");
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	spin_lock(&file->spin_lock);
+	wait_queue = file->tlock_waitq[offset];
+	if (!wait_queue) {
+		wait_queue = tmp;
+		file->tlock_waitq[offset] = wait_queue;
+		init_waitqueue_head(&(wait_queue->waitq_outer));
+		spin_lock_init(&(wait_queue->spin_lock));
+		wait_queue->sig_flag = 0;
+	} else
+		should_free = 1;
+	spin_unlock(&file->spin_lock);
+
+	if (should_free)
+		kfree(tmp);
+
+lock_wait_pid:
+	ret = copy_from_user(&mutex, (void *)addr, sizeof(int));
+	if (unlikely(ret))
+		goto out;
+
+	if ((mutex & (TLOCK_PID_MASK | TLOCK_COUNT_MASK)) == 0)
+		goto out;
+
+	ret = wait_exclusive_interruptible_timeout(
+				wait_queue->waitq_outer, HZ);
+	if (unlikely(ret))
+		goto out;
+
+	ret = copy_from_user(&mutex, (void *)addr, sizeof(int));
+	if (unlikely(ret))
+		goto out;
+
+	cur_pid = (mutex & TLOCK_PID_MASK);
+
+	read_lock_tasklist();
+	pts = find_task_by_pid_ns(cur_pid, &init_pid_ns);
+	read_unlock_tasklist();
+
+	if (pts)
+		ptlts = find_tlock_task(pts);
+	if (!pts || !ptlts)
+		ret = cur_pid;
+out:
+	return ret;
+}
+
+int unlock_in_file(tlock_file_t *file, size_t offset, caddr_t addr)
+{
+	tlock_wait_queue *wait_queue = NULL;
+
+	wait_queue = file->tlock_waitq[offset];
+	if (wait_queue)
+		wake_up_interruptible_sync(&wait_queue->waitq_outer);
+	return 0;
+}
+
+long tlock_compat_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	return tlock_ioctl(NULL, NULL, cmd, arg);
+}
+
+struct file_operations tlock_fops = {
+	.owner = THIS_MODULE,
+	.ioctl = tlock_ioctl,
+	.compat_ioctl = tlock_compat_ioctl,
+	.flush = tlock_flush
+};
+
+static struct miscdevice tlock_dev = {
+	MISC_DYNAMIC_MINOR,
+	"tlock",
+	&tlock_fops
+};
+
+static int tlock_kern_init(void)
+{
+	misc_register(&tlock_dev);
+	printk(KERN_ALERT "tlock init\n");
+	return 0;
+}
+
+
+static void tlock_kern_exit(void)
+{
+	misc_deregister(&tlock_dev);
+	printk(KERN_ALERT "tlock exit\n");
+}
+
+
+
+module_init(tlock_kern_init);
+module_exit(tlock_kern_exit);
Index: linux-2.6.32-220.17.1.el5/drivers/misc/Makefile
===================================================================
--- linux-2.6.32-220.17.1.el5.orig/drivers/misc/Makefile	2012-05-17 02:20:36.736553259 +0800
+++ linux-2.6.32-220.17.1.el5/drivers/misc/Makefile	2012-05-17 02:20:53.408635927 +0800
@@ -23,4 +23,5 @@
 obj-$(CONFIG_C2PORT)		+= c2port/
 obj-y				+= eeprom/
 obj-y				+= cb710/
+obj-y				+= tlock/
 obj-$(CONFIG_VMWARE_BALLOON)	+= vmware_balloon.o
Index: linux-2.6.32-220.17.1.el5/drivers/misc/Kconfig
===================================================================
--- linux-2.6.32-220.17.1.el5.orig/drivers/misc/Kconfig	2012-05-17 02:20:36.752553339 +0800
+++ linux-2.6.32-220.17.1.el5/drivers/misc/Kconfig	2012-05-17 02:20:53.408635927 +0800
@@ -233,6 +233,13 @@
 	  This driver can also be built as a module.  If so, the module
 	  will be called isl29003.
 
+config TLOCK
+	tristate "Taobao Tlock for Tbdm"
+	default m
+	---help---
+	This is for Taobao Tlock, which is needed by Taobao Tbdm. The option
+	is recommended as M.
+
 config EP93XX_PWM
 	tristate "EP93xx PWM support"
 	depends on ARCH_EP93XX
Index: linux-2.6.32-220.17.1.el5/include/linux/file.h
===================================================================
--- linux-2.6.32-220.17.1.el5.orig/include/linux/file.h	2012-05-17 02:20:36.724553200 +0800
+++ linux-2.6.32-220.17.1.el5/include/linux/file.h	2012-05-17 02:20:53.408635927 +0800
@@ -38,5 +38,7 @@
 extern void put_unused_fd(unsigned int fd);
 
 extern void fd_install(unsigned int fd, struct file *file);
-
+/* wrapper routines for Taobao tlock */
+extern struct file *tlock_fget_light(unsigned int fd, int *fput_needed);
+extern void tlock_fput_light(struct file *file, int fput_needed);
 #endif /* __LINUX_FILE_H */
