From: Tao Ma <boyu.mt@taobao.com>
Subject: flashcache port to taobao kernel.
Patch-mainline: top commit a21bb2ef915aedcc31d2b98a751f79b471971651
References: 

Acked-by: 
Signed-off-by: Tao Ma <boyu.mt@taobao.com>

Index: linux-2.6.32-131.0.15.el6/drivers/md/Kconfig
===================================================================
--- linux-2.6.32-131.0.15.el6.orig/drivers/md/Kconfig
+++ linux-2.6.32-131.0.15.el6/drivers/md/Kconfig
@@ -259,6 +259,12 @@ config DM_ZERO
 	  A target that discards writes, and returns all zeroes for
 	  reads.  Useful in some recovery situations.
 
+config DM_FLASHCACHE
+	tristate "Block level disk caching target (EXPERIMENTAL)"
+	depends on BLK_DEV_DM && EXPERIMENTAL
+	---help---
+	  A write back block caching target.
+
 config DM_MULTIPATH
 	tristate "Multipath target"
 	depends on BLK_DEV_DM
Index: linux-2.6.32-131.0.15.el6/drivers/md/Makefile
===================================================================
--- linux-2.6.32-131.0.15.el6.orig/drivers/md/Makefile
+++ linux-2.6.32-131.0.15.el6/drivers/md/Makefile
@@ -11,6 +11,8 @@ dm-mirror-y	+= dm-raid1.o
 dm-replicator-y	+= dm-repl.o
 dm-log-userspace-y \
 		+= dm-log-userspace-base.o dm-log-userspace-transfer.o
+flashcache-y	+= flashcache_main.o flashcache_subr.o \
+		   flashcache_ioctl.o flashcache_conf.o
 md-mod-y	+= md.o bitmap.o
 raid456-y	+= raid5.o
 
@@ -37,6 +39,7 @@ obj-$(CONFIG_DM_SNAPSHOT)	+= dm-snapshot
 obj-$(CONFIG_DM_MIRROR)		+= dm-mirror.o dm-log.o dm-region-hash.o
 obj-$(CONFIG_DM_LOG_USERSPACE)	+= dm-log-userspace.o
 obj-$(CONFIG_DM_ZERO)		+= dm-zero.o
+obj-$(CONFIG_DM_FLASHCACHE)	+= flashcache.o
 obj-$(CONFIG_DM_REPLICATOR)	+= dm-replicator.o \
 				   dm-repl-log-ringbuffer.o \
 				   dm-repl-slink-blockdev.o \
Index: linux-2.6.32-131.0.15.el6/drivers/md/flashcache.h
===================================================================
--- /dev/null
+++ linux-2.6.32-131.0.15.el6/drivers/md/flashcache.h
@@ -0,0 +1,539 @@
+/****************************************************************************
+ *  flashcache.h
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#ifndef FLASHCACHE_H
+#define FLASHCACHE_H
+
+#define FLASHCACHE_VERSION		2
+
+#define DEV_PATHLEN	128
+
+#ifdef __KERNEL__
+
+/* Like ASSERT() but always compiled in */
+
+#define VERIFY(x) do { \
+	if (unlikely(!(x))) { \
+		dump_stack(); \
+		panic("VERIFY: assertion (%s) failed at %s (%d)\n", \
+		      #x,  __FILE__ , __LINE__);		    \
+	} \
+} while(0)
+
+#define DMC_DEBUG 0
+#define DMC_DEBUG_LITE 0
+
+#define DM_MSG_PREFIX "flashcache"
+#define DMC_PREFIX "flashcache: "
+
+#if DMC_DEBUG
+#define DPRINTK( s, arg... ) printk(DMC_PREFIX s "\n", ##arg)
+#else
+#define DPRINTK( s, arg... )
+#endif
+
+/*
+ * Block checksums :
+ * Block checksums seem a good idea (especially for debugging, I found a couple
+ * of bugs with this), but in practice there are a number of issues with this
+ * in production.
+ * 1) If a flash write fails, there is no guarantee that the failure was atomic.
+ * Some sectors may have been written to flash. If so, the checksum we have
+ * is wrong. We could re-read the flash block and recompute the checksum, but
+ * the read could fail too.
+ * 2) On a node crash, we could have crashed between the flash data write and the
+ * flash metadata update (which updates the new checksum to flash metadata). When
+ * we reboot, the checksum we read from metadata is wrong. This is worked around
+ * by having the cache load recompute checksums after an unclean shutdown.
+ * 3) Checksums require 4 or 8 more bytes per block in terms of metadata overhead.
+ * Especially because the metadata is wired into memory.
+ * 4) Checksums force us to do a flash metadata IO on a block re-dirty. If we
+ * didn't maintain checksums, we could avoid the metadata IO on a re-dirty.
+ * Therefore in production we disable block checksums.
+ */
+#if 0
+#define FLASHCACHE_DO_CHECKSUMS
+#endif
+
+#if DMC_DEBUG_LITE
+#define DPRINTK_LITE( s, arg... ) printk(DMC_PREFIX s "\n", ##arg)
+#else
+#define DPRINTK_LITE( s, arg... )
+#endif
+
+/* Number of pages for I/O */
+#define FLASHCACHE_COPY_PAGES (1024)
+
+/* Default cache parameters */
+#define DEFAULT_CACHE_SIZE	65536
+#define DEFAULT_CACHE_ASSOC	512
+#define DEFAULT_BLOCK_SIZE	8	/* 4 KB */
+#define DEFAULT_MD_BLOCK_SIZE	8	/* 4 KB */
+#define FLASHCACHE_MAX_MD_BLOCK_SIZE	128	/* 64 KB */
+
+#define FLASHCACHE_FIFO		0
+#define FLASHCACHE_LRU		1
+
+/*
+ * The LRU pointers are maintained as set-relative offsets, instead of
+ * pointers. This enables us to store the LRU pointers per cacheblock
+ * using 4 bytes instead of 16 bytes. The upshot of this is that we
+ * are required to clamp the associativity at an 8K max.
+ */
+#define FLASHCACHE_MIN_ASSOC	 256
+#define FLASHCACHE_MAX_ASSOC	8192
+#define FLASHCACHE_LRU_NULL	0xFFFF
+
+/* Cache block metadata structure */
+struct cacheblock {
+	u_int16_t	cache_state;
+	int16_t 	nr_queued;	/* jobs in pending queue */
+	u_int16_t	lru_prev, lru_next;
+	sector_t 	dbn;	/* Sector number of the cached block */
+#ifdef FLASHCACHE_DO_CHECKSUMS
+	u_int64_t 	checksum;
+#endif
+};
+
+struct cache_set {
+	u_int32_t		set_fifo_next;
+	u_int32_t		set_clean_next;
+	u_int16_t		clean_inprog;
+	u_int16_t		nr_dirty;
+	u_int16_t		lru_head, lru_tail;
+	u_int16_t		dirty_fallow;
+	unsigned long 		fallow_tstamp;
+	unsigned long 		fallow_next_cleaning;
+};
+
+struct flashcache_errors {
+	int	disk_read_errors;
+	int	disk_write_errors;
+	int	ssd_read_errors;
+	int	ssd_write_errors;
+	int	memory_alloc_errors;
+};
+
+struct flashcache_stats {
+	unsigned long reads;		/* Number of reads */
+	unsigned long writes;		/* Number of writes */
+	unsigned long read_hits;	/* Number of cache hits */
+	unsigned long write_hits;	/* Number of write hits (includes dirty write hits) */
+	unsigned long dirty_write_hits;	/* Number of "dirty" write hits */
+	unsigned long replace;		/* Number of cache replacements */
+	unsigned long wr_replace;
+	unsigned long wr_invalidates;	/* Number of write invalidations */
+	unsigned long rd_invalidates;	/* Number of read invalidations */
+	unsigned long pending_inval;	/* Invalidations due to concurrent ios on same block */
+#ifdef FLASHCACHE_DO_CHECKSUMS
+	unsigned long checksum_store;
+	unsigned long checksum_valid;
+	unsigned long checksum_invalid;
+#endif
+	unsigned long enqueues;		/* enqueues on pending queue */
+	unsigned long cleanings;
+	unsigned long fallow_cleanings;
+	unsigned long noroom;		/* No room in set */
+	unsigned long md_write_dirty;	/* Metadata sector writes dirtying block */
+	unsigned long md_write_clean;	/* Metadata sector writes cleaning block */
+	unsigned long md_write_batch;	/* How many md updates did we batch ? */
+	unsigned long md_ssd_writes;	/* How many md ssd writes did we do ? */
+	unsigned long pid_drops;
+	unsigned long pid_adds;
+	unsigned long pid_dels;
+	unsigned long expiry;
+	unsigned long front_merge, back_merge;	/* Write Merging */
+	unsigned long uncached_reads, uncached_writes;
+	unsigned long disk_reads, disk_writes;
+	unsigned long ssd_reads, ssd_writes;
+	unsigned long uncached_io_requeue;
+	unsigned long skipclean;
+	unsigned long trim_blocks;
+	unsigned long clean_set_ios;
+};
+
+/*
+ * Cache context
+ */
+struct cache_c {
+	struct dm_target	*tgt;
+
+	struct dm_dev 		*disk_dev;   /* Source device */
+	struct dm_dev 		*cache_dev; /* Cache device */
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
+	struct dm_kcopyd_client *kcp_client; /* Kcopyd client for writing back data */
+#else
+	struct kcopyd_client *kcp_client; /* Kcopyd client for writing back data */
+#endif
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+	struct dm_io_client *io_client; /* Client memory pool*/
+#endif
+
+	int 			on_ssd_version;
+
+	spinlock_t		cache_spin_lock;
+
+	struct cacheblock	*cache;	/* Hash table for cache blocks */
+	struct cache_set	*cache_sets;
+	struct cache_md_block_head *md_blocks_buf;
+
+	unsigned int md_block_size;	/* Metadata block size in sectors */
+
+	sector_t size;			/* Cache size */
+	unsigned int assoc;		/* Cache associativity */
+	unsigned int block_size;	/* Cache block size */
+	unsigned int block_shift;	/* Cache block size in bits */
+	unsigned int block_mask;	/* Cache block mask */
+	unsigned int consecutive_shift;	/* Consecutive blocks size in bits */
+
+	wait_queue_head_t destroyq;	/* Wait queue for I/O completion */
+	/* XXX - Updates of nr_jobs should happen inside the lock. But doing it outside
+	   is OK since the filesystem is unmounted at this point */
+	atomic_t nr_jobs;		/* Number of I/O jobs */
+
+#define SLOW_REMOVE    1
+#define FAST_REMOVE    2
+	atomic_t remove_in_prog;
+
+	int	dirty_thresh_set;	/* Per set dirty threshold to start cleaning */
+	int	max_clean_ios_set;	/* Max cleaning IOs per set */
+	int	max_clean_ios_total;	/* Total max cleaning IOs */
+	int	clean_inprog;
+	int	sync_index;
+	int	nr_dirty;
+	unsigned long cached_blocks;	/* Number of cached blocks */
+	unsigned long pending_jobs_count;
+	int	md_blocks;		/* Numbers of metadata blocks, including header */
+
+	/* Stats */
+	struct flashcache_stats flashcache_stats;
+
+	/* Errors */
+	struct flashcache_errors flashcache_errors;
+
+#define IO_LATENCY_GRAN_USECS	250
+#define IO_LATENCY_MAX_US_TRACK	10000	/* 10 ms */
+#define IO_LATENCY_BUCKETS	(IO_LATENCY_MAX_US_TRACK / IO_LATENCY_GRAN_USECS)
+	unsigned long	latency_hist[IO_LATENCY_BUCKETS];
+	unsigned long	latency_hist_10ms;
+
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+	struct work_struct delayed_clean;
+#else
+	struct delayed_work delayed_clean;
+#endif
+
+	unsigned long pid_expire_check;
+
+	struct flashcache_cachectl_pid *blacklist_head, *blacklist_tail;
+	struct flashcache_cachectl_pid *whitelist_head, *whitelist_tail;
+	int num_blacklist_pids, num_whitelist_pids;
+	unsigned long blacklist_expire_check, whitelist_expire_check;
+
+#define PENDING_JOB_HASH_SIZE		32
+	struct pending_job *pending_job_hashbuckets[PENDING_JOB_HASH_SIZE];
+
+	struct cache_c	*next_cache;
+
+	char cache_devname[DEV_PATHLEN];
+	char disk_devname[DEV_PATHLEN];
+};
+
+/* kcached/pending job states */
+#define READCACHE	1
+#define WRITECACHE	2
+#define READDISK	3
+#define WRITEDISK	4
+#define READFILL	5	/* Read Cache Miss Fill */
+#define INVALIDATE	6
+#define WRITEDISK_SYNC	7
+
+struct kcached_job {
+	struct list_head list;
+	struct cache_c *dmc;
+	struct bio *bio;	/* Original bio */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+	struct io_region disk;
+	struct io_region cache;
+#else
+	struct dm_io_region disk;
+	struct dm_io_region cache;
+#endif
+	int    index;
+	int    action;
+	int 	error;
+	struct flash_cacheblock *md_block;
+	struct bio_vec md_io_bvec;
+	struct timeval io_start_time;
+	struct kcached_job *next;
+};
+
+struct pending_job {
+	struct bio *bio;
+	int	action;
+	int	index;
+	struct pending_job *prev, *next;
+};
+#endif /* __KERNEL__ */
+
+/* States of a cache block */
+#define INVALID			0x0001
+#define VALID			0x0002	/* Valid */
+#define DISKREADINPROG		0x0004	/* Read from disk in progress */
+#define DISKWRITEINPROG		0x0008	/* Write to disk in progress */
+#define CACHEREADINPROG		0x0010	/* Read from cache in progress */
+#define CACHEWRITEINPROG	0x0020	/* Write to cache in progress */
+#define DIRTY			0x0040	/* Dirty, needs writeback to disk */
+/*
+ * Old and Dirty blocks are cleaned with a Clock like algorithm. The leading hand
+ * marks DIRTY_FALLOW_1. 60 seconds (default) later, the trailing hand comes along and
+ * marks DIRTY_FALLOW_2 if DIRTY_FALLOW_1 is already set. If the block was used in the
+ * interim, (DIRTY_FALLOW_1|DIRTY_FALLOW_2) is cleared. Any block that has both
+ * DIRTY_FALLOW_1 and DIRTY_FALLOW_2 marked is considered old and is eligible
+ * for cleaning.
+ */
+#define DIRTY_FALLOW_1		0x0080
+#define DIRTY_FALLOW_2		0x0100
+
+#define FALLOW_DOCLEAN		(DIRTY_FALLOW_1 | DIRTY_FALLOW_2)
+#define BLOCK_IO_INPROG	(DISKREADINPROG | DISKWRITEINPROG | CACHEREADINPROG | CACHEWRITEINPROG)
+
+/* Cache metadata is read by Flashcache utilities */
+#ifndef __KERNEL__
+typedef u_int64_t sector_t;
+#endif
+
+/* On Flash (cache metadata) Structures */
+#define CACHE_MD_STATE_DIRTY		0xdeadbeef
+#define CACHE_MD_STATE_CLEAN		0xfacecafe
+#define CACHE_MD_STATE_FASTCLEAN	0xcafefeed
+#define CACHE_MD_STATE_UNSTABLE		0xc8249756
+
+struct flash_superblock {
+	sector_t size;		/* Cache size */
+	u_int32_t block_size;	/* Cache block size */
+	u_int32_t assoc;	/* Cache associativity */
+	u_int32_t cache_sb_state;	/* Clean shutdown ? */
+	char cache_devname[DEV_PATHLEN];
+	sector_t cache_devsize;
+	char disk_devname[DEV_PATHLEN];
+	sector_t disk_devsize;
+	u_int32_t cache_version;
+	u_int32_t md_block_size;
+};
+
+/*
+ * We do metadata updates only when a block trasitions from DIRTY -> CLEAN
+ * or from CLEAN -> DIRTY. Consequently, on an unclean shutdown, we only
+ * pick up blocks that are marked (DIRTY | CLEAN), we clean these and stick
+ * them in the cache.
+ * On a clean shutdown, we will sync the state for every block, and we will
+ * load every block back into cache on a restart.
+ *
+ * Note: When using larger flashcache metadata blocks, it is important to make
+ * sure that a flash_cacheblock does not straddle 2 sectors. This avoids
+ * partial writes of a metadata slot on a powerfail/node crash. Aligning this
+ * a 16b or 32b struct avoids that issue.
+ *
+ * Note: If a on-ssd flash_cacheblock does not fit exactly within a 512b sector,
+ * (ie. if there are any remainder runt bytes), logic in flashcache_conf.c which
+ * reads and writes flashcache metadata on create/load/remove will break.
+ *
+ * If changing these, make sure they remain a ^2 size !
+ */
+#ifdef FLASHCACHE_DO_CHECKSUMS
+struct flash_cacheblock {
+	sector_t 	dbn;	/* Sector number of the cached block */
+	u_int64_t 	checksum;
+	u_int32_t	cache_state; /* INVALID | VALID | DIRTY */
+} __attribute__ ((aligned(32)));
+#else
+struct flash_cacheblock {
+	sector_t 	dbn;	/* Sector number of the cached block */
+	u_int32_t	cache_state; /* INVALID | VALID | DIRTY */
+} __attribute__ ((aligned(16)));
+#endif
+
+#define MD_BLOCK_BYTES(DMC)		((DMC)->md_block_size * 512)
+#define MD_SECTORS_PER_BLOCK(DMC)	((DMC)->md_block_size)
+#define MD_SLOTS_PER_BLOCK(DMC)		(MD_BLOCK_BYTES(DMC) / (sizeof(struct flash_cacheblock)))
+#define INDEX_TO_MD_BLOCK(DMC, INDEX)	((INDEX) / MD_SLOTS_PER_BLOCK(DMC))
+#define INDEX_TO_MD_BLOCK_OFFSET(DMC, INDEX)	((INDEX) % MD_SLOTS_PER_BLOCK(DMC))
+
+#define METADATA_IO_BLOCKSIZE		(256*1024)
+#define METADATA_IO_NUM_BLOCKS(dmc)	(METADATA_IO_BLOCKSIZE / MD_BLOCK_BYTES(dmc))
+
+#define INDEX_TO_CACHE_ADDR(DMC, INDEX)	\
+	(((sector_t)(INDEX) << (DMC)->block_shift) + (DMC)->md_blocks * MD_SECTORS_PER_BLOCK((DMC)))
+
+#ifdef __KERNEL__
+
+/* Cache persistence */
+#define CACHE_RELOAD		1
+#define CACHE_CREATE		2
+#define CACHE_FORCECREATE	3
+
+/*
+ * We have one of these for *every* cache metadata sector, to keep track
+ * of metadata ios in progress for blocks covered in this sector. Only
+ * one metadata IO per sector can be in progress at any given point in
+ * time
+ */
+struct cache_md_block_head {
+	u_int32_t		nr_in_prog;
+	struct kcached_job	*queued_updates, *md_io_inprog;
+};
+
+#define MIN_JOBS 1024
+
+#define DIRTY_THRESH_MIN	10
+#define DIRTY_THRESH_MAX	90
+#define DIRTY_THRESH_DEF	20
+
+/* DM async IO mempool sizing */
+#define FLASHCACHE_ASYNC_SIZE 1024
+
+enum {
+	FLASHCACHE_WHITELIST=0,
+	FLASHCACHE_BLACKLIST=1,
+};
+
+struct flashcache_cachectl_pid {
+	pid_t					pid;
+	struct flashcache_cachectl_pid		*next, *prev;
+	unsigned long				expiry;
+};
+
+struct dbn_index_pair {
+	sector_t	dbn;
+	int		index;
+};
+
+/* Error injection flags */
+#define READDISK_ERROR				0x00000001
+#define READCACHE_ERROR				0x00000002
+#define READFILL_ERROR				0x00000004
+#define WRITECACHE_ERROR			0x00000008
+#define WRITECACHE_MD_ERROR			0x00000010
+#define WRITEDISK_MD_ERROR			0x00000020
+#define KCOPYD_CALLBACK_ERROR			0x00000040
+#define DIRTY_WRITEBACK_JOB_ALLOC_FAIL		0x00000080
+#define READ_MISS_JOB_ALLOC_FAIL		0x00000100
+#define READ_HIT_JOB_ALLOC_FAIL			0x00000200
+#define READ_HIT_PENDING_JOB_ALLOC_FAIL		0x00000400
+#define INVAL_PENDING_JOB_ALLOC_FAIL		0x00000800
+#define WRITE_HIT_JOB_ALLOC_FAIL		0x00001000
+#define WRITE_HIT_PENDING_JOB_ALLOC_FAIL	0x00002000
+#define WRITE_MISS_JOB_ALLOC_FAIL		0x00004000
+#define WRITES_LIST_ALLOC_FAIL			0x00008000
+#define MD_ALLOC_SECTOR_ERROR			0x00010000
+
+/* Inject a 5s delay between syncing blocks and metadata */
+#define FLASHCACHE_SYNC_REMOVE_DELAY		5000
+
+int flashcache_map(struct dm_target *ti, struct bio *bio,
+		   union map_info *map_context);
+int flashcache_ctr(struct dm_target *ti, unsigned int argc,
+		   char **argv);
+void flashcache_dtr(struct dm_target *ti);
+
+int flashcache_status(struct dm_target *ti, status_type_t type,
+		      char *result, unsigned int maxlen);
+
+struct kcached_job *flashcache_alloc_cache_job(void);
+void flashcache_free_cache_job(struct kcached_job *job);
+struct pending_job *flashcache_alloc_pending_job(struct cache_c *dmc);
+void flashcache_free_pending_job(struct pending_job *job);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+u_int64_t flashcache_compute_checksum(struct bio *bio);
+void flashcache_store_checksum(struct kcached_job *job);
+int flashcache_validate_checksum(struct kcached_job *job);
+int flashcache_read_compute_checksum(struct cache_c *dmc, int index, void *block);
+#endif
+struct kcached_job *pop(struct list_head *jobs);
+void push(struct list_head *jobs, struct kcached_job *job);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+void do_work(void *unused);
+#else
+void do_work(struct work_struct *unused);
+#endif
+struct kcached_job *new_kcached_job(struct cache_c *dmc, struct bio* bio,
+				    int index);
+void push_pending(struct kcached_job *job);
+void push_io(struct kcached_job *job);
+void push_md_io(struct kcached_job *job);
+void push_md_complete(struct kcached_job *job);
+void push_uncached_io_complete(struct kcached_job *job);
+int flashcache_pending_empty(void);
+int flashcache_io_empty(void);
+int flashcache_md_io_empty(void);
+int flashcache_md_complete_empty(void);
+void flashcache_md_write_done(struct kcached_job *job);
+void flashcache_do_pending(struct kcached_job *job);
+void flashcache_md_write(struct kcached_job *job);
+void flashcache_md_write_kickoff(struct kcached_job *job);
+void flashcache_do_io(struct kcached_job *job);
+void flashcache_uncached_io_complete(struct kcached_job *job);
+void flashcache_clean_set(struct cache_c *dmc, int set);
+void flashcache_sync_all(struct cache_c *dmc);
+void flashcache_reclaim_lru_movetail(struct cache_c *dmc, int index);
+void flashcache_merge_writes(struct cache_c *dmc,
+			     struct dbn_index_pair *writes_list,
+			     int *nr_writes, int set);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+int flashcache_dm_io_sync_vm(struct cache_c *dmc, struct io_region *where,
+			     int rw, void *data);
+#else
+int flashcache_dm_io_sync_vm(struct cache_c *dmc, struct dm_io_region *where,
+			     int rw, void *data);
+#endif
+void flashcache_update_sync_progress(struct cache_c *dmc);
+void flashcache_unplug_device(struct block_device *bdev);
+void flashcache_enq_pending(struct cache_c *dmc, struct bio* bio,
+			    int index, int action, struct pending_job *job);
+struct pending_job *flashcache_deq_pending(struct cache_c *dmc, int index);
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+int dm_io_async_bvec(unsigned int num_regions,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
+			    struct dm_io_region *where,
+#else
+			    struct io_region *where,
+#endif
+			    int rw,
+			    struct bio_vec *bvec, io_notify_fn fn,
+			    void *context);
+#endif
+
+void flashcache_detect_fallow(struct cache_c *dmc, int index);
+void flashcache_clear_fallow(struct cache_c *dmc, int index);
+
+void flashcache_bio_endio(struct bio *bio, int error,
+			  struct cache_c *dmc, struct timeval *io_start_time);
+
+#endif /* __KERNEL__ */
+
+#endif
Index: linux-2.6.32-131.0.15.el6/drivers/md/flashcache_conf.c
===================================================================
--- /dev/null
+++ linux-2.6.32-131.0.15.el6/drivers/md/flashcache_conf.c
@@ -0,0 +1,2290 @@
+/****************************************************************************
+ *  flashcache_conf.c
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#include <asm/atomic.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/list.h>
+#include <linux/blkdev.h>
+#include <linux/bio.h>
+#include <linux/slab.h>
+#include <linux/hash.h>
+#include <linux/spinlock.h>
+#include <linux/workqueue.h>
+#include <linux/pagemap.h>
+#include <linux/random.h>
+#include <linux/hardirq.h>
+#include <linux/sysctl.h>
+#include <linux/version.h>
+#include <linux/reboot.h>
+#include <linux/delay.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+#include "dm.h"
+#include "dm-io.h"
+#include "dm-bio-list.h"
+#include "kcopyd.h"
+#else
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+#include "dm.h"
+#endif
+#include <linux/device-mapper.h>
+#include <linux/bio.h>
+#include <linux/dm-kcopyd.h>
+#endif
+
+#include "flashcache.h"
+#include "flashcache_ioctl.h"
+
+int sysctl_flashcache_reclaim_policy = FLASHCACHE_FIFO;
+int sysctl_flashcache_write_merge = 1;
+/* XXX - Some access to this are MP unsafe, but harmless. Not worth fixing */
+int sysctl_flashcache_error_inject = 0;
+int sysctl_fallow_clean_speed = 2;
+
+static struct ctl_table_header *flashcache_table_header;
+
+int sysctl_flashcache_sync;
+int sysctl_flashcache_stop_sync = 0;
+int sysctl_flashcache_zerostats;
+int sysctl_flashcache_dirty_thresh = DIRTY_THRESH_DEF;
+int sysctl_flashcache_debug = 0;
+int sysctl_max_clean_ios_total = 4;
+int sysctl_max_clean_ios_set = 2;
+int sysctl_flashcache_max_pids = 100;
+int sysctl_pid_expiry_check = 60;
+int sysctl_pid_do_expiry = 0;
+int sysctl_flashcache_fast_remove = 0;
+int sysctl_cache_all = 1;
+int sysctl_fallow_delay = 60*15;	/* 15 mins default */
+int sysctl_flashcache_lat_hist = 0;
+
+static int fallow_clean_speed_min = 1;
+static int fallow_clean_speed_max = 100;
+
+struct cache_c *cache_list_head = NULL;
+struct work_struct _kcached_wq;
+u_int64_t size_hist[33];
+
+struct kmem_cache *_job_cache;
+mempool_t *_job_pool;
+struct kmem_cache *_pending_job_cache;
+mempool_t *_pending_job_pool;
+
+atomic_t nr_cache_jobs;
+atomic_t nr_pending_jobs;
+
+extern struct list_head *_pending_jobs;
+extern struct list_head *_io_jobs;
+extern struct list_head *_md_io_jobs;
+extern struct list_head *_md_complete_jobs;
+
+static void flashcache_zero_stats(struct cache_c *dmc);
+
+struct flashcache_control_s {
+	unsigned long synch_flags;
+};
+
+struct flashcache_control_s *flashcache_control;
+
+/* Bit offsets for wait_on_bit_lock() */
+#define FLASHCACHE_UPDATE_LIST		0
+
+static int flashcache_notify_reboot(struct notifier_block *this,
+				    unsigned long code, void *x);
+static void flashcache_sync_for_remove(struct cache_c *dmc);
+
+static int
+flashcache_wait_schedule(void *unused)
+{
+	schedule();
+	return 0;
+}
+
+static int
+flashcache_io_latency_init(ctl_table *table, int write,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+			   struct file *file,
+#endif
+			   void __user *buffer,
+			   size_t *length, loff_t *ppos)
+{
+	struct cache_c *dmc;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+	proc_dointvec_minmax(table, write, file, buffer, length, ppos);
+#else
+	proc_dointvec_minmax(table, write, buffer, length, ppos);
+#endif
+	if (write) {
+		if (sysctl_flashcache_lat_hist) {
+			(void)wait_on_bit_lock(&flashcache_control->synch_flags,
+					       FLASHCACHE_UPDATE_LIST,
+					       flashcache_wait_schedule,
+					       TASK_UNINTERRUPTIBLE);
+			for (dmc = cache_list_head ;
+			     dmc != NULL ;
+			     dmc = dmc->next_cache) {
+				int i;
+
+				for (i = 0 ; i < IO_LATENCY_BUCKETS ; i++)
+					dmc->latency_hist[i] = 0;
+				dmc->latency_hist_10ms = 0;
+			}
+			clear_bit(FLASHCACHE_UPDATE_LIST, &flashcache_control->synch_flags);
+			smp_mb__after_clear_bit();
+			wake_up_bit(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST);
+		}
+	}
+	return 0;
+}
+
+static int
+flashcache_sync_sysctl_handler(ctl_table *table, int write,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+			       struct file *file,
+#endif
+			       void __user *buffer,
+			       size_t *length, loff_t *ppos)
+{
+	struct cache_c *dmc;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+	proc_dointvec_minmax(table, write, file, buffer, length, ppos);
+#else
+	proc_dointvec_minmax(table, write, buffer, length, ppos);
+#endif
+	if (write) {
+		if (sysctl_flashcache_sync) {
+			sysctl_flashcache_stop_sync = 0;
+			(void)wait_on_bit_lock(&flashcache_control->synch_flags,
+					       FLASHCACHE_UPDATE_LIST,
+					       flashcache_wait_schedule,
+					       TASK_UNINTERRUPTIBLE);
+			for (dmc = cache_list_head ;
+			     dmc != NULL ;
+			     dmc = dmc->next_cache) {
+				cancel_delayed_work(&dmc->delayed_clean);
+				flush_scheduled_work();
+				flashcache_sync_all(dmc);
+			}
+			clear_bit(FLASHCACHE_UPDATE_LIST, &flashcache_control->synch_flags);
+			smp_mb__after_clear_bit();
+			wake_up_bit(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST);
+		}
+	}
+	return 0;
+}
+
+static int
+flashcache_zerostats_sysctl_handler(ctl_table *table, int write,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+				    struct file *file,
+#endif
+				    void __user *buffer,
+				    size_t *length, loff_t *ppos)
+{
+	struct cache_c *dmc;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+	proc_dointvec_minmax(table, write, file, buffer, length, ppos);
+#else
+	proc_dointvec_minmax(table, write, buffer, length, ppos);
+#endif
+	if (write) {
+		if (sysctl_flashcache_zerostats) {
+			(void)wait_on_bit_lock(&flashcache_control->synch_flags,
+					       FLASHCACHE_UPDATE_LIST,
+					       flashcache_wait_schedule,
+					       TASK_UNINTERRUPTIBLE);
+			for (dmc = cache_list_head ;
+			     dmc != NULL ;
+			     dmc = dmc->next_cache)
+				flashcache_zero_stats(dmc);
+			clear_bit(FLASHCACHE_UPDATE_LIST, &flashcache_control->synch_flags);
+			smp_mb__after_clear_bit();
+			wake_up_bit(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST);
+		}
+	}
+	return 0;
+}
+
+static int
+flashcache_dirty_thresh_sysctl_handler(ctl_table *table, int write,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+				       struct file *file,
+#endif
+				       void __user *buffer,
+				       size_t *length, loff_t *ppos)
+{
+	struct cache_c *dmc;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+        proc_dointvec_minmax(table, write, file, buffer, length, ppos);
+#else
+        proc_dointvec_minmax(table, write, buffer, length, ppos);
+#endif
+	if (write) {
+		if (sysctl_flashcache_dirty_thresh > DIRTY_THRESH_MAX ||
+		    sysctl_flashcache_dirty_thresh < DIRTY_THRESH_MIN)
+			sysctl_flashcache_dirty_thresh = DIRTY_THRESH_DEF;
+		(void)wait_on_bit_lock(&flashcache_control->synch_flags,
+				       FLASHCACHE_UPDATE_LIST,
+				       flashcache_wait_schedule,
+				       TASK_UNINTERRUPTIBLE);
+		for (dmc = cache_list_head ;
+		     dmc != NULL ;
+		     dmc = dmc->next_cache)
+			dmc->dirty_thresh_set =
+				(dmc->assoc * sysctl_flashcache_dirty_thresh) / 100;
+		clear_bit(FLASHCACHE_UPDATE_LIST, &flashcache_control->synch_flags);
+		smp_mb__after_clear_bit();
+		wake_up_bit(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST);
+	}
+	return 0;
+}
+
+static int
+flashcache_max_clean_ios_total_sysctl_handler(ctl_table *table, int write,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+					      struct file *file,
+#endif
+					      void __user *buffer,
+					      size_t *length, loff_t *ppos)
+{
+	struct cache_c *dmc;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+        proc_dointvec_minmax(table, write, file, buffer, length, ppos);
+#else
+        proc_dointvec_minmax(table, write, buffer, length, ppos);
+#endif
+	if (write) {
+		(void)wait_on_bit_lock(&flashcache_control->synch_flags,
+				       FLASHCACHE_UPDATE_LIST,
+				       flashcache_wait_schedule,
+				       TASK_UNINTERRUPTIBLE);
+		for (dmc = cache_list_head ;
+		     dmc != NULL ;
+		     dmc = dmc->next_cache)
+			dmc->max_clean_ios_total = sysctl_max_clean_ios_total;
+		clear_bit(FLASHCACHE_UPDATE_LIST, &flashcache_control->synch_flags);
+		smp_mb__after_clear_bit();
+		wake_up_bit(&flashcache_control->synch_flags,
+			    FLASHCACHE_UPDATE_LIST);
+	}
+	return 0;
+}
+
+static int
+flashcache_max_clean_ios_set_sysctl_handler(ctl_table *table, int write,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+					    struct file *file,
+#endif
+					    void __user *buffer,
+					    size_t *length, loff_t *ppos)
+{
+	struct cache_c *dmc;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
+        proc_dointvec_minmax(table, write, file, buffer, length, ppos);
+#else
+        proc_dointvec_minmax(table, write, buffer, length, ppos);
+#endif
+	if (write) {
+		(void)wait_on_bit_lock(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST,
+				       flashcache_wait_schedule, TASK_UNINTERRUPTIBLE);
+		for (dmc = cache_list_head ;
+		     dmc != NULL ;
+		     dmc = dmc->next_cache)
+			dmc->max_clean_ios_set = sysctl_max_clean_ios_set;
+		clear_bit(FLASHCACHE_UPDATE_LIST, &flashcache_control->synch_flags);
+		smp_mb__after_clear_bit();
+		wake_up_bit(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST);
+	}
+	return 0;
+}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+#define CTL_UNNUMBERED			-2
+#endif
+
+static ctl_table flashcache_table[] = {
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_UNNUMBERED,
+#endif
+		.procname	= "io_latency_hist",
+		.data		= &sysctl_flashcache_lat_hist,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &flashcache_io_latency_init,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.strategy	= &sysctl_intvec,
+#endif
+	},
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_UNNUMBERED,
+#endif
+		.procname	= "do_sync",
+		.data		= &sysctl_flashcache_sync,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &flashcache_sync_sysctl_handler,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.strategy	= &sysctl_intvec,
+#endif
+	},
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_UNNUMBERED,
+#endif
+		.procname	= "stop_sync",
+		.data		= &sysctl_flashcache_stop_sync,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_UNNUMBERED,
+#endif
+		.procname	= "dirty_thresh_pct",
+		.data		= &sysctl_flashcache_dirty_thresh,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &flashcache_dirty_thresh_sysctl_handler,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.strategy	= &sysctl_intvec,
+#endif
+	},
+#ifdef notdef
+	/* Devel only */
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_UNNUMBERED,
+#endif
+		.procname	= "debug",
+		.data		= &sysctl_flashcache_debug,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+#endif /* notdef */
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_UNNUMBERED,
+#endif
+		.procname	= "max_clean_ios_total",
+		.data		= &sysctl_max_clean_ios_total,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &flashcache_max_clean_ios_total_sysctl_handler,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.strategy	= &sysctl_intvec,
+#endif
+	},
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_UNNUMBERED,
+#endif
+		.procname	= "max_clean_ios_set",
+		.data		= &sysctl_max_clean_ios_set,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &flashcache_max_clean_ios_set_sysctl_handler,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.strategy	= &sysctl_intvec,
+#endif
+	},
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_UNNUMBERED,
+#endif
+		.procname	= "do_pid_expiry",
+		.data		= &sysctl_pid_do_expiry,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_UNNUMBERED,
+#endif
+		.procname	= "max_pids",
+		.data		= &sysctl_flashcache_max_pids,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_UNNUMBERED,
+#endif
+		.procname	= "pid_expiry_secs",
+		.data		= &sysctl_pid_expiry_check,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_UNNUMBERED,
+#endif
+		.procname	= "reclaim_policy",
+		.data		= &sysctl_flashcache_reclaim_policy,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+#ifdef notdef
+	/* Write merging is always enabled */
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_UNNUMBERED,
+#endif
+		.procname	= "write_merge",
+		.data		= &sysctl_flashcache_write_merge,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+#endif /* notdef */
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_UNNUMBERED,
+#endif
+		.procname	= "zero_stats",
+		.data		= &sysctl_flashcache_zerostats,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &flashcache_zerostats_sysctl_handler,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.strategy	= &sysctl_intvec,
+#endif
+	},
+#ifdef notdef
+	/* Disable this for all except devel builds */
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_UNNUMBERED,
+#endif
+		.procname	= "error_inject",
+		.data		= &sysctl_flashcache_error_inject,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+#endif
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_UNNUMBERED,
+#endif
+		.procname	= "fast_remove",
+		.data		= &sysctl_flashcache_fast_remove,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_UNNUMBERED,
+#endif
+		.procname	= "cache_all",
+		.data		= &sysctl_cache_all,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_UNNUMBERED,
+#endif
+		.procname	= "fallow_clean_speed",
+		.data		= &sysctl_fallow_clean_speed,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec_minmax,
+		.extra1		= &fallow_clean_speed_min,
+		.extra2		= &fallow_clean_speed_max,
+	},
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_UNNUMBERED,
+#endif
+		.procname	= "fallow_delay",
+		.data		= &sysctl_fallow_delay,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+  {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+	.ctl_name = 0
+#endif
+  }
+};
+
+static ctl_table flashcache_dir_table[] = {
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_UNNUMBERED,
+#endif
+		.procname	= "flashcache",
+		.maxlen		= 0,
+		.mode		= S_IRUGO|S_IXUGO,
+		.child		= flashcache_table,
+	},
+  {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+	.ctl_name = 0
+#endif
+  }
+};
+
+static ctl_table flashcache_root_table[] = {
+	{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+		.ctl_name	= CTL_DEV,
+#endif
+		.procname	= "dev",
+		.maxlen		= 0,
+		.mode		= 0555,
+		.child		= flashcache_dir_table,
+	},
+  {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+	.ctl_name = 0
+#endif
+  }
+};
+
+static int
+flashcache_jobs_init(void)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,23)
+	_job_cache = kmem_cache_create("kcached-jobs",
+	                               sizeof(struct kcached_job),
+	                               __alignof__(struct kcached_job),
+	                               0, NULL, NULL);
+#else
+	_job_cache = kmem_cache_create("kcached-jobs",
+	                               sizeof(struct kcached_job),
+	                               __alignof__(struct kcached_job),
+	                               0, NULL);
+#endif
+	if (!_job_cache)
+		return -ENOMEM;
+
+	_job_pool = mempool_create(MIN_JOBS, mempool_alloc_slab,
+	                           mempool_free_slab, _job_cache);
+	if (!_job_pool) {
+		kmem_cache_destroy(_job_cache);
+		return -ENOMEM;
+	}
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,23)
+	_pending_job_cache = kmem_cache_create("pending-jobs",
+					       sizeof(struct pending_job),
+					       __alignof__(struct pending_job),
+					       0, NULL, NULL);
+#else
+	_pending_job_cache = kmem_cache_create("pending-jobs",
+					       sizeof(struct pending_job),
+					       __alignof__(struct pending_job),
+					       0, NULL);
+#endif
+	if (!_pending_job_cache) {
+		mempool_destroy(_job_pool);
+		kmem_cache_destroy(_job_cache);
+		return -ENOMEM;
+	}
+
+	_pending_job_pool = mempool_create(MIN_JOBS, mempool_alloc_slab,
+					   mempool_free_slab, _pending_job_cache);
+	if (!_pending_job_pool) {
+		kmem_cache_destroy(_pending_job_cache);
+		mempool_destroy(_job_pool);
+		kmem_cache_destroy(_job_cache);
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static void
+flashcache_jobs_exit(void)
+{
+	VERIFY(flashcache_pending_empty());
+	VERIFY(flashcache_io_empty());
+	VERIFY(flashcache_md_io_empty());
+	VERIFY(flashcache_md_complete_empty());
+
+	mempool_destroy(_job_pool);
+	kmem_cache_destroy(_job_cache);
+	_job_pool = NULL;
+	_job_cache = NULL;
+	mempool_destroy(_pending_job_pool);
+	kmem_cache_destroy(_pending_job_cache);
+	_pending_job_pool = NULL;
+	_pending_job_cache = NULL;
+}
+
+static int
+flashcache_kcached_init(struct cache_c *dmc)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
+	int r;
+
+	r = dm_io_get(FLASHCACHE_ASYNC_SIZE);
+	if (r) {
+		DMERR("flashcache_kcached_init: Could not resize dm io pool");
+		return r;
+	}
+#endif
+	init_waitqueue_head(&dmc->destroyq);
+	atomic_set(&dmc->nr_jobs, 0);
+	atomic_set(&dmc->remove_in_prog, 0);
+	return 0;
+}
+
+static void
+flashcache_kcached_client_destroy(struct cache_c *dmc)
+{
+	/* Wait for all IOs */
+	wait_event(dmc->destroyq, !atomic_read(&dmc->nr_jobs));
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
+	dm_io_put(FLASHCACHE_ASYNC_SIZE);
+#endif
+}
+
+/*
+ * Write out the metadata one sector at a time.
+ * Then dump out the superblock.
+ */
+static int
+flashcache_md_store(struct cache_c *dmc)
+{
+	struct flash_cacheblock *meta_data_cacheblock, *next_ptr;
+	struct flash_superblock *header;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+	struct io_region where;
+#else
+	struct dm_io_region where;
+#endif
+	int i, j;
+	int num_valid = 0, num_dirty = 0;
+	int error;
+	int write_errors = 0;
+	int sectors_written = 0, sectors_expected = 0; /* debug */
+	int slots_written = 0; /* How many cache slots did we fill in this MD io block ? */
+
+	meta_data_cacheblock = (struct flash_cacheblock *)vmalloc(METADATA_IO_BLOCKSIZE);
+	if (!meta_data_cacheblock) {
+		DMERR("flashcache_md_store: Unable to allocate memory");
+		DMERR("flashcache_md_store: Could not write out cache metadata !");
+		return 1;
+	}
+
+	where.bdev = dmc->cache_dev->bdev;
+	where.sector = MD_SECTORS_PER_BLOCK(dmc);
+	slots_written = 0;
+	next_ptr = meta_data_cacheblock;
+	j = MD_SLOTS_PER_BLOCK(dmc);
+	for (i = 0 ; i < dmc->size ; i++) {
+		if (dmc->cache[i].cache_state & VALID)
+			num_valid++;
+		if (dmc->cache[i].cache_state & DIRTY)
+			num_dirty++;
+		next_ptr->dbn = dmc->cache[i].dbn;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		next_ptr->checksum = dmc->cache[i].checksum;
+#endif
+		next_ptr->cache_state = dmc->cache[i].cache_state &
+			(INVALID | VALID | DIRTY);
+		next_ptr++;
+		slots_written++;
+		j--;
+		if (j == 0) {
+			/*
+			 * Filled the block, write and goto the next metadata block.
+			 */
+			if (slots_written == MD_SLOTS_PER_BLOCK(dmc) * METADATA_IO_NUM_BLOCKS(dmc)) {
+				/*
+				 * Wrote out an entire metadata IO block, write the block to the ssd.
+				 */
+				where.count = (slots_written / MD_SLOTS_PER_BLOCK(dmc)) *
+					MD_SECTORS_PER_BLOCK(dmc);
+				slots_written = 0;
+				sectors_written += where.count;	/* debug */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+				error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, meta_data_cacheblock);
+#else
+				error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, meta_data_cacheblock);
+#endif
+				if (error) {
+					write_errors++;
+					DMERR("flashcache_md_store: Could not write out cache metadata block %lu error %d !",
+					      where.sector, error);
+				}
+				where.sector += where.count;	/* Advance offset */
+			}
+			/* Move next slot pointer into next block */
+			next_ptr = (struct flash_cacheblock *)
+				((caddr_t)meta_data_cacheblock + ((slots_written / MD_SLOTS_PER_BLOCK(dmc)) * MD_BLOCK_BYTES(dmc)));
+			j = MD_SLOTS_PER_BLOCK(dmc);
+		}
+	}
+	if (next_ptr != meta_data_cacheblock) {
+		/* Write the remaining last blocks out */
+		VERIFY(slots_written > 0);
+		where.count = (slots_written / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+		if (slots_written % MD_SLOTS_PER_BLOCK(dmc))
+			where.count += MD_SECTORS_PER_BLOCK(dmc);
+		sectors_written += where.count;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+		error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, meta_data_cacheblock);
+#else
+		error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, meta_data_cacheblock);
+#endif
+		if (error) {
+			write_errors++;
+				DMERR("flashcache_md_store: Could not write out cache metadata block %lu error %d !",
+				      where.sector, error);
+		}
+	}
+	/* Debug Tests */
+	sectors_expected = (dmc->size / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+	if (dmc->size % MD_SLOTS_PER_BLOCK(dmc))
+		sectors_expected += MD_SECTORS_PER_BLOCK(dmc);
+	if (sectors_expected != sectors_written) {
+		printk("flashcache_md_store" "Sector Mismatch ! sectors_expected=%d, sectors_written=%d\n",
+		       sectors_expected, sectors_written);
+		panic("flashcache_md_store: sector mismatch\n");
+	}
+
+	vfree((void *)meta_data_cacheblock);
+
+	header = (struct flash_superblock *)vmalloc(MD_BLOCK_BYTES(dmc));
+	if (!header) {
+		DMERR("flashcache_md_store: Unable to allocate memory");
+		DMERR("flashcache_md_store: Could not write out cache metadata !");
+		return 1;
+	}
+	memset(header, 0, MD_BLOCK_BYTES(dmc));
+
+	/* Write the header out last */
+	if (write_errors == 0) {
+		if (num_dirty == 0)
+			header->cache_sb_state = CACHE_MD_STATE_CLEAN;
+		else
+			header->cache_sb_state = CACHE_MD_STATE_FASTCLEAN;
+	} else
+		header->cache_sb_state = CACHE_MD_STATE_UNSTABLE;
+	header->block_size = dmc->block_size;
+	header->md_block_size = dmc->md_block_size;
+	header->size = dmc->size;
+	header->assoc = dmc->assoc;
+	strncpy(header->disk_devname, dmc->disk_devname, DEV_PATHLEN);
+	strncpy(header->cache_devname, dmc->cache_devname, DEV_PATHLEN);
+	header->cache_devsize = to_sector(dmc->cache_dev->bdev->bd_inode->i_size);
+	header->disk_devsize = to_sector(dmc->disk_dev->bdev->bd_inode->i_size);
+	header->cache_version = dmc->on_ssd_version;
+
+	DPRINTK("Store metadata to disk: block size(%u), md block size(%u), cache size(%llu)" \
+	        "associativity(%u)",
+	        header->block_size, header->md_block_size, header->size,
+	        header->assoc);
+
+	where.sector = 0;
+	where.count = dmc->md_block_size;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+	error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, header);
+#else
+	error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, header);
+#endif
+	if (error) {
+		write_errors++;
+		DMERR("flashcache_md_store: Could not write out cache metadata superblock %lu error %d !",
+		      where.sector, error);
+	}
+
+	vfree((void *)header);
+
+	if (write_errors == 0)
+		DMINFO("Cache metadata saved to disk");
+	else {
+		DMINFO("CRITICAL : There were %d errors in saving cache metadata saved to disk",
+		       write_errors);
+		if (num_dirty)
+			DMINFO("CRITICAL : You have likely lost %d dirty blocks", num_dirty);
+	}
+
+	DMINFO("flashcache_md_store: valid blocks = %d dirty blocks = %d md_sectors = %d\n",
+	       num_valid, num_dirty, dmc->md_blocks * MD_SECTORS_PER_BLOCK(dmc));
+
+	return 0;
+}
+
+static int
+flashcache_md_create(struct cache_c *dmc, int force)
+{
+	struct flash_cacheblock *meta_data_cacheblock, *next_ptr;
+	struct flash_superblock *header;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+	struct io_region where;
+#else
+	struct dm_io_region where;
+#endif
+	int i, j, error;
+	sector_t cache_size, dev_size;
+	sector_t order;
+	int sectors_written = 0, sectors_expected = 0; /* debug */
+	int slots_written = 0; /* How many cache slots did we fill in this MD io block ? */
+
+	header = (struct flash_superblock *)vmalloc(MD_BLOCK_BYTES(dmc));
+	if (!header) {
+		DMERR("flashcache_md_create: Unable to allocate sector");
+		return 1;
+	}
+	where.bdev = dmc->cache_dev->bdev;
+	where.sector = 0;
+	where.count = dmc->md_block_size;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+	error = flashcache_dm_io_sync_vm(dmc, &where, READ, header);
+#else
+	error = flashcache_dm_io_sync_vm(dmc, &where, READ, header);
+#endif
+	if (error) {
+		vfree((void *)header);
+		DMERR("flashcache_md_create: Could not read cache superblock %lu error %d !",
+		      where.sector, error);
+		return 1;
+	}
+	if (!force &&
+	    ((header->cache_sb_state == CACHE_MD_STATE_DIRTY) ||
+	     (header->cache_sb_state == CACHE_MD_STATE_CLEAN) ||
+	     (header->cache_sb_state == CACHE_MD_STATE_FASTCLEAN))) {
+		vfree((void *)header);
+		DMERR("flashcache_md_create: Existing Cache Detected, use force to re-create");
+		return 1;
+	}
+	/* Compute the size of the metadata, including header.
+	   Note dmc->size is in raw sectors */
+	dmc->md_blocks = INDEX_TO_MD_BLOCK(dmc, dmc->size / dmc->block_size) + 1 + 1;
+	dmc->size -= dmc->md_blocks * MD_SECTORS_PER_BLOCK(dmc);	/* total sectors available for cache */
+	dmc->size /= dmc->block_size;
+	dmc->size = (dmc->size / dmc->assoc) * dmc->assoc;
+	/* Recompute since dmc->size was possibly trunc'ed down */
+	dmc->md_blocks = INDEX_TO_MD_BLOCK(dmc, dmc->size) + 1 + 1;
+	DMINFO("flashcache_md_create: md_blocks = %d, md_sectors = %d\n",
+	       dmc->md_blocks, dmc->md_blocks * MD_SECTORS_PER_BLOCK(dmc));
+	dev_size = to_sector(dmc->cache_dev->bdev->bd_inode->i_size);
+	cache_size = dmc->md_blocks * MD_SECTORS_PER_BLOCK(dmc) + (dmc->size * dmc->block_size);
+	if (cache_size > dev_size) {
+		DMERR("Requested cache size exceeds the cache device's capacity" \
+		      "(%lu>%lu)",
+  		      cache_size, dev_size);
+		vfree((void *)header);
+		return 1;
+	}
+	order = dmc->size * sizeof(struct cacheblock);
+	DMINFO("Allocate %luKB (%luB per) mem for %lu-entry cache" \
+	       "(capacity:%luMB, associativity:%u, block size:%u " \
+	       "sectors(%uKB))",
+	       order >> 10, sizeof(struct cacheblock), dmc->size,
+	       cache_size >> (20-SECTOR_SHIFT), dmc->assoc, dmc->block_size,
+	       dmc->block_size >> (10-SECTOR_SHIFT));
+	dmc->cache = (struct cacheblock *)vmalloc(order);
+	if (!dmc->cache) {
+		vfree((void *)header);
+		DMERR("flashcache_md_create: Unable to allocate cache md");
+		return 1;
+	}
+	/* Initialize the cache structs */
+	for (i = 0; i < dmc->size ; i++) {
+		dmc->cache[i].dbn = 0;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		dmc->cache[i].checksum = 0;
+#endif
+		dmc->cache[i].cache_state = INVALID;
+		dmc->cache[i].nr_queued = 0;
+	}
+	meta_data_cacheblock = (struct flash_cacheblock *)vmalloc(METADATA_IO_BLOCKSIZE);
+	if (!meta_data_cacheblock) {
+		DMERR("flashcache_md_store: Unable to allocate memory");
+		DMERR("flashcache_md_store: Could not write out cache metadata !");
+		return 1;
+	}
+	where.sector = MD_SECTORS_PER_BLOCK(dmc);
+	slots_written = 0;
+	next_ptr = meta_data_cacheblock;
+	j = MD_SLOTS_PER_BLOCK(dmc);
+	for (i = 0 ; i < dmc->size ; i++) {
+		next_ptr->dbn = dmc->cache[i].dbn;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		next_ptr->checksum = dmc->cache[i].checksum;
+#endif
+		next_ptr->cache_state = dmc->cache[i].cache_state &
+			(INVALID | VALID | DIRTY);
+		next_ptr++;
+		slots_written++;
+		j--;
+		if (j == 0) {
+			/*
+			 * Filled the block, write and goto the next metadata block.
+			 */
+			if (slots_written == MD_SLOTS_PER_BLOCK(dmc) * METADATA_IO_NUM_BLOCKS(dmc)) {
+				/*
+				 * Wrote out an entire metadata IO block, write the block to the ssd.
+				 */
+				where.count = (slots_written / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+				slots_written = 0;
+				sectors_written += where.count;	/* debug */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+				error = flashcache_dm_io_sync_vm(dmc, &where, WRITE,
+								 meta_data_cacheblock);
+#else
+				error = flashcache_dm_io_sync_vm(dmc, &where, WRITE,
+								 meta_data_cacheblock);
+#endif
+				if (error) {
+					vfree((void *)header);
+					vfree((void *)meta_data_cacheblock);
+					vfree(dmc->cache);
+					DMERR("flashcache_md_create: Could not write cache metadata block %lu error %d !",
+					      where.sector, error);
+					return 1;
+				}
+				where.sector += where.count;	/* Advance offset */
+			}
+			/* Move next slot pointer into next metadata block */
+			next_ptr = (struct flash_cacheblock *)
+				((caddr_t)meta_data_cacheblock + ((slots_written / MD_SLOTS_PER_BLOCK(dmc)) * MD_BLOCK_BYTES(dmc)));
+			j = MD_SLOTS_PER_BLOCK(dmc);
+		}
+	}
+	if (next_ptr != meta_data_cacheblock) {
+		/* Write the remaining last blocks out */
+		VERIFY(slots_written > 0);
+		where.count = (slots_written / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+		if (slots_written % MD_SLOTS_PER_BLOCK(dmc))
+			where.count += MD_SECTORS_PER_BLOCK(dmc);
+		sectors_written += where.count;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+		error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, meta_data_cacheblock);
+#else
+		error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, meta_data_cacheblock);
+#endif
+		if (error) {
+			vfree((void *)header);
+			vfree((void *)meta_data_cacheblock);
+			vfree(dmc->cache);
+			DMERR("flashcache_md_create: Could not write cache metadata block %lu error %d !",
+			      where.sector, error);
+			return 1;
+		}
+	}
+	/* Debug Tests */
+	sectors_expected = (dmc->size / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+	if (dmc->size % MD_SLOTS_PER_BLOCK(dmc))
+		sectors_expected += MD_SECTORS_PER_BLOCK(dmc);
+	if (sectors_expected != sectors_written) {
+		printk("flashcache_md_create" "Sector Mismatch ! sectors_expected=%d, sectors_written=%d\n",
+		       sectors_expected, sectors_written);
+		panic("flashcache_md_create: sector mismatch\n");
+	}
+	vfree((void *)meta_data_cacheblock);
+	/* Write the header */
+	header->cache_sb_state = CACHE_MD_STATE_DIRTY;
+	header->block_size = dmc->block_size;
+	header->md_block_size = dmc->md_block_size;
+	header->size = dmc->size;
+	header->assoc = dmc->assoc;
+	strncpy(header->disk_devname, dmc->disk_devname, DEV_PATHLEN);
+	strncpy(header->cache_devname, dmc->cache_devname, DEV_PATHLEN);
+	header->cache_devsize = to_sector(dmc->cache_dev->bdev->bd_inode->i_size);
+	header->disk_devsize = to_sector(dmc->disk_dev->bdev->bd_inode->i_size);
+	dmc->on_ssd_version = header->cache_version = FLASHCACHE_VERSION;
+	where.sector = 0;
+	where.count = dmc->md_block_size;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+	error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, header);
+#else
+	error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, header);
+#endif
+	if (error) {
+		vfree((void *)header);
+		vfree(dmc->cache);
+		DMERR("flashcache_md_create: Could not write cache superblock %lu error %d !",
+		      where.sector, error);
+		return 1;
+	}
+	vfree((void *)header);
+	return 0;
+}
+
+static int
+flashcache_md_load(struct cache_c *dmc)
+{
+	struct flash_cacheblock *meta_data_cacheblock, *next_ptr;
+	struct flash_superblock *header;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+	struct io_region where;
+#else
+	struct dm_io_region where;
+#endif
+	int i, j;
+	u_int64_t size, slots_read;
+	int clean_shutdown;
+	int dirty_loaded = 0;
+	sector_t order, data_size;
+	int num_valid = 0;
+	int error;
+	int sectors_read = 0, sectors_expected = 0;	/* Debug */
+
+	/*
+	 * We don't know what the preferred block size is, just read off
+	 * the default md blocksize.
+	 */
+	header = (struct flash_superblock *)vmalloc(DEFAULT_MD_BLOCK_SIZE);
+	if (!header) {
+		DMERR("flashcache_md_load: Unable to allocate memory");
+		return 1;
+	}
+	where.bdev = dmc->cache_dev->bdev;
+	where.sector = 0;
+	where.count = DEFAULT_MD_BLOCK_SIZE;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+	error = flashcache_dm_io_sync_vm(dmc, &where, READ, header);
+#else
+	error = flashcache_dm_io_sync_vm(dmc, &where, READ, header);
+#endif
+	if (error) {
+		vfree((void *)header);
+		DMERR("flashcache_md_load: Could not read cache superblock %lu error %d!",
+		      where.sector, error);
+		return 1;
+	}
+
+	if (header->cache_version == 1) {
+		/* Backwards compatibility, md was 512 bytes always in V1.0 */
+		header->md_block_size = 1;
+	} else if (header->cache_version > FLASHCACHE_VERSION) {
+		vfree((void *)header);
+		DMERR("flashcache_md_load: Unknown version %d found in superblock!", header->cache_version);
+		return 1;
+	}
+	dmc->on_ssd_version = header->cache_version;
+
+	DPRINTK("Loaded cache conf: version(%d), block size(%u), md block size(%u), cache size(%llu), " \
+	        "associativity(%u)",
+	        header->cache_version, header->block_size, header->md_block_size, header->size,
+	        header->assoc);
+	if (!((header->cache_sb_state == CACHE_MD_STATE_DIRTY) ||
+	      (header->cache_sb_state == CACHE_MD_STATE_CLEAN) ||
+	      (header->cache_sb_state == CACHE_MD_STATE_FASTCLEAN))) {
+		vfree((void *)header);
+		DMERR("flashcache_md_load: Corrupt Cache Superblock");
+		return 1;
+	}
+	if (header->cache_sb_state == CACHE_MD_STATE_DIRTY) {
+		DMINFO("Unclean Shutdown Detected");
+		printk(KERN_ALERT "Only DIRTY blocks exist in cache");
+		clean_shutdown = 0;
+	} else if (header->cache_sb_state == CACHE_MD_STATE_CLEAN) {
+		DMINFO("Slow (clean) Shutdown Detected");
+		printk(KERN_ALERT "Only CLEAN blocks exist in cache");
+		clean_shutdown = 1;
+	} else {
+		DMINFO("Fast (clean) Shutdown Detected");
+		printk(KERN_ALERT "Both CLEAN and DIRTY blocks exist in cache");
+		clean_shutdown = 1;
+	}
+	dmc->block_size = header->block_size;
+	dmc->md_block_size = header->md_block_size;
+	dmc->block_shift = ffs(dmc->block_size) - 1;
+	dmc->block_mask = dmc->block_size - 1;
+	dmc->size = header->size;
+	dmc->assoc = header->assoc;
+	dmc->consecutive_shift = ffs(dmc->assoc) - 1;
+	dmc->md_blocks = INDEX_TO_MD_BLOCK(dmc, dmc->size) + 1 + 1;
+	DMINFO("flashcache_md_load: md_blocks = %d, md_sectors = %d, md_block_size = %d\n",
+	       dmc->md_blocks, dmc->md_blocks * MD_SECTORS_PER_BLOCK(dmc), dmc->md_block_size);
+	data_size = dmc->size * dmc->block_size;
+	order = dmc->size * sizeof(struct cacheblock);
+	DMINFO("Allocate %luKB (%ldB per) mem for %lu-entry cache" \
+	       "(capacity:%luMB, associativity:%u, block size:%u " \
+	       "sectors(%uKB))",
+	       order >> 10, sizeof(struct cacheblock), dmc->size,
+	       (dmc->md_blocks * MD_SECTORS_PER_BLOCK(dmc) + data_size) >> (20-SECTOR_SHIFT),
+	       dmc->assoc, dmc->block_size,
+	       dmc->block_size >> (10-SECTOR_SHIFT));
+	dmc->cache = (struct cacheblock *)vmalloc(order);
+	if (!dmc->cache) {
+		DMERR("load_metadata: Unable to allocate memory");
+		vfree((void *)header);
+		return 1;
+	}
+	/* Read the metadata in large blocks and populate incore state */
+	meta_data_cacheblock = (struct flash_cacheblock *)vmalloc(METADATA_IO_BLOCKSIZE);
+	if (!meta_data_cacheblock) {
+		vfree((void *)header);
+		vfree(dmc->cache);
+		DMERR("flashcache_md_load: Unable to allocate memory");
+		return 1;
+	}
+	where.sector = MD_SECTORS_PER_BLOCK(dmc);
+	size = dmc->size;
+	i = 0;
+	while (size > 0) {
+		slots_read = min(size, (u_int64_t)(MD_SLOTS_PER_BLOCK(dmc) * METADATA_IO_NUM_BLOCKS(dmc)));
+		if (slots_read % MD_SLOTS_PER_BLOCK(dmc))
+			where.count = (1 + (slots_read / MD_SLOTS_PER_BLOCK(dmc))) * MD_SECTORS_PER_BLOCK(dmc);
+		else
+			where.count = (slots_read / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+		sectors_read += where.count;	/* Debug */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+		error = flashcache_dm_io_sync_vm(dmc, &where, READ, meta_data_cacheblock);
+#else
+		error = flashcache_dm_io_sync_vm(dmc, &where, READ, meta_data_cacheblock);
+#endif
+		if (error) {
+			vfree((void *)header);
+			vfree(dmc->cache);
+			vfree((void *)meta_data_cacheblock);
+			DMERR("flashcache_md_load: Could not read cache metadata block %lu error %d !",
+			      where.sector, error);
+			return 1;
+		}
+		where.sector += where.count;
+		next_ptr = meta_data_cacheblock;
+		for (j = 0 ; j < slots_read ; j++) {
+			/*
+			 * XXX - Now that we force each on-ssd metadata cache slot to be a ^2, where
+			 * we are guaranteed that the slots will exactly fit within a sector (and
+			 * a metadata block), we can simplify this logic. We don't need this next test.
+			 */
+			if ((j % MD_SLOTS_PER_BLOCK(dmc)) == 0) {
+				/* Move onto next block */
+				next_ptr = (struct flash_cacheblock *)
+					((caddr_t)meta_data_cacheblock + MD_BLOCK_BYTES(dmc) * (j / MD_SLOTS_PER_BLOCK(dmc)));
+			}
+			dmc->cache[i].nr_queued = 0;
+			/*
+			 * If unclean shutdown, only the DIRTY blocks are loaded.
+			 */
+			if (clean_shutdown || (next_ptr->cache_state & DIRTY)) {
+				if (next_ptr->cache_state & DIRTY)
+					dirty_loaded++;
+				dmc->cache[i].cache_state = next_ptr->cache_state;
+				VERIFY((dmc->cache[i].cache_state & (VALID | INVALID))
+				       != (VALID | INVALID));
+				if (dmc->cache[i].cache_state & VALID)
+					num_valid++;
+				dmc->cache[i].dbn = next_ptr->dbn;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+				if (clean_shutdown)
+					dmc->cache[i].checksum = next_ptr->checksum;
+				else {
+					error = flashcache_read_compute_checksum(dmc, i, block);
+					if (error) {
+						vfree((void *)header);
+						vfree(dmc->cache);
+						vfree((void *)meta_data_cacheblock);
+						DMERR("flashcache_md_load: Could not read cache metadata block %lu error %d !",
+						      dmc->cache[i].dbn, error);
+						return 1;
+					}
+				}
+#endif
+			} else {
+				dmc->cache[i].cache_state = INVALID;
+				dmc->cache[i].dbn = 0;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+				dmc->cache[i].checksum = 0;
+#endif
+			}
+			next_ptr++;
+			i++;
+		}
+		size -= slots_read;
+	}
+	/* Debug Tests */
+	sectors_expected = (dmc->size / MD_SLOTS_PER_BLOCK(dmc)) * MD_SECTORS_PER_BLOCK(dmc);
+	if (dmc->size % MD_SLOTS_PER_BLOCK(dmc))
+		sectors_expected += MD_SECTORS_PER_BLOCK(dmc);
+	if (sectors_expected != sectors_read) {
+		printk("flashcache_md_load" "Sector Mismatch ! sectors_expected=%d, sectors_read=%d\n",
+		       sectors_expected, sectors_read);
+		panic("flashcache_md_load: sector mismatch\n");
+	}
+	vfree((void *)meta_data_cacheblock);
+	/*
+	 * For writing the superblock out, use the preferred blocksize that
+	 * we read from the superblock above.
+	 */
+	if (DEFAULT_MD_BLOCK_SIZE != dmc->md_block_size) {
+		vfree((void *)header);
+		header = (struct flash_superblock *)vmalloc(MD_BLOCK_BYTES(dmc));
+		if (!header) {
+			DMERR("flashcache_md_load: Unable to allocate memory");
+			return 1;
+		}
+	}
+	/* Before we finish loading, we need to dirty the superblock and
+	   write it out */
+	header->size = dmc->size;
+	header->block_size = dmc->block_size;
+	header->md_block_size = dmc->md_block_size;
+	header->assoc = dmc->assoc;
+	header->cache_sb_state = CACHE_MD_STATE_DIRTY;
+	strncpy(header->disk_devname, dmc->disk_devname, DEV_PATHLEN);
+	strncpy(header->cache_devname, dmc->cache_devname, DEV_PATHLEN);
+	header->cache_devsize = to_sector(dmc->cache_dev->bdev->bd_inode->i_size);
+	header->disk_devsize = to_sector(dmc->disk_dev->bdev->bd_inode->i_size);
+	header->cache_version = dmc->on_ssd_version;
+	where.sector = 0;
+	where.count = dmc->md_block_size;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+	error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, header);
+#else
+	error = flashcache_dm_io_sync_vm(dmc, &where, WRITE, header);
+#endif
+	if (error) {
+		vfree((void *)header);
+		vfree(dmc->cache);
+		DMERR("flashcache_md_load: Could not write cache superblock %lu error %d !",
+		      where.sector, error);
+		return 1;
+	}
+	vfree((void *)header);
+	DMINFO("flashcache_md_load: Cache metadata loaded from disk with %d valid %d DIRTY blocks",
+	       num_valid, dirty_loaded);
+	return 0;
+}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+static void
+flashcache_clean_all_sets(void *data)
+{
+	struct cache_c *dmc = (struct cache_c *)data;
+#else
+static void
+flashcache_clean_all_sets(struct work_struct *work)
+{
+	struct cache_c *dmc = container_of(work, struct cache_c,
+					   delayed_clean.work);
+#endif
+	int i;
+
+	for (i = 0 ; i < (dmc->size >> dmc->consecutive_shift) ; i++)
+		flashcache_clean_set(dmc, i);
+}
+
+static int inline
+flashcache_get_dev(struct dm_target *ti, char *pth, struct dm_dev **dmd,
+		   char *dmc_dname, sector_t tilen)
+{
+	int rc;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,34)
+	rc = dm_get_device(ti, pth,
+			   dm_table_get_mode(ti->table), dmd);
+#else
+#if defined(RHEL_MAJOR) && RHEL_MAJOR == 6
+	rc = dm_get_device(ti, pth,
+			   dm_table_get_mode(ti->table), dmd);
+#else
+	rc = dm_get_device(ti, pth, 0, tilen,
+			   dm_table_get_mode(ti->table), dmd);
+#endif
+#endif
+	if (!rc)
+		strncpy(dmc_dname, pth, DEV_PATHLEN);
+	return rc;
+}
+
+/*
+ * Construct a cache mapping.
+ *  arg[0]: path to source device
+ *  arg[1]: path to cache device
+ *  arg[2]: cache persistence (if set, cache conf is loaded from disk)
+ * Cache configuration parameters (if not set, default values are used.
+ *  arg[3]: cache block size (in sectors)
+ *  arg[4]: cache size (in blocks)
+ *  arg[5]: cache associativity
+ */
+int
+flashcache_ctr(struct dm_target *ti, unsigned int argc, char **argv)
+{
+	struct cache_c *dmc;
+	unsigned int consecutive_blocks;
+	sector_t i, order;
+	int r = -EINVAL;
+	int persistence = 0;
+
+	if (argc < 2) {
+		ti->error = "flashcache: Need at least 2 arguments";
+		goto bad;
+	}
+
+	dmc = kzalloc(sizeof(*dmc), GFP_KERNEL);
+	if (dmc == NULL) {
+		ti->error = "flashcache: Failed to allocate cache context";
+		r = ENOMEM;
+		goto bad;
+	}
+
+	dmc->tgt = ti;
+	if (flashcache_get_dev(ti, argv[0], &dmc->disk_dev,
+			       dmc->disk_devname, ti->len)) {
+		ti->error = "flashcache: Disk device lookup failed";
+		goto bad1;
+	}
+	if (flashcache_get_dev(ti, argv[1], &dmc->cache_dev,
+			       dmc->cache_devname, 0)) {
+		ti->error = "flashcache: Cache device lookup failed";
+		goto bad2;
+	}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+	dmc->io_client = dm_io_client_create(FLASHCACHE_COPY_PAGES);
+	if (IS_ERR(dmc->io_client)) {
+		r = PTR_ERR(dmc->io_client);
+		ti->error = "Failed to create io client\n";
+		goto bad3;
+	}
+#endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
+	r = kcopyd_client_create(FLASHCACHE_COPY_PAGES, &dmc->kcp_client);
+	if (r) {
+		ti->error = "Failed to initialize kcopyd client\n";
+		goto bad3;
+	}
+#else
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
+	r = dm_kcopyd_client_create(FLASHCACHE_COPY_PAGES, &dmc->kcp_client);
+#else
+	r = kcopyd_client_create(FLASHCACHE_COPY_PAGES, &dmc->kcp_client);
+#endif
+	if (r) {
+		ti->error = "Failed to initialize kcopyd client\n";
+		dm_io_client_destroy(dmc->io_client);
+		goto bad3;
+	}
+#endif
+
+	r = flashcache_kcached_init(dmc);
+	if (r) {
+		ti->error = "Failed to initialize kcached";
+		goto bad4;
+	}
+
+	if (argc >= 3) {
+		if (sscanf(argv[2], "%u", &persistence) != 1) {
+			ti->error = "flashcache: sscanf failed, invalid cache persistence";
+			r = -EINVAL;
+			goto bad5;
+		}
+		if (persistence < CACHE_RELOAD || persistence > CACHE_FORCECREATE) {
+			DMERR("persistence = %d", persistence);
+			ti->error = "flashcache: Invalid cache persistence";
+			r = -EINVAL;
+			goto bad5;
+		}
+	}
+	if (persistence == CACHE_RELOAD) {
+		if (flashcache_md_load(dmc)) {
+			ti->error = "flashcache: Cache reload failed";
+			r = -EINVAL;
+			goto bad5;
+		}
+		goto init; /* Skip reading cache parameters from command line */
+	}
+
+	if (argc >= 4) {
+		if (sscanf(argv[3], "%u", &dmc->block_size) != 1) {
+			ti->error = "flashcache: Invalid block size";
+			r = -EINVAL;
+			goto bad5;
+		}
+		if (!dmc->block_size || (dmc->block_size & (dmc->block_size - 1))) {
+			ti->error = "flashcache: Invalid block size";
+			r = -EINVAL;
+			goto bad5;
+		}
+	}
+
+	if (!dmc->block_size)
+		dmc->block_size = DEFAULT_BLOCK_SIZE;
+	dmc->block_shift = ffs(dmc->block_size) - 1;
+	dmc->block_mask = dmc->block_size - 1;
+
+	/* dmc->size is specified in sectors here, and converted to blocks later */
+	if (argc >= 5) {
+		if (sscanf(argv[4], "%lu", &dmc->size) != 1) {
+			ti->error = "flashcache: Invalid cache size";
+			r = -EINVAL;
+			goto bad5;
+		}
+	}
+
+	if (!dmc->size)
+		dmc->size = to_sector(dmc->cache_dev->bdev->bd_inode->i_size);
+
+	if (argc >= 6) {
+		if (sscanf(argv[5], "%u", &dmc->assoc) != 1) {
+			ti->error = "flashcache: Invalid cache associativity";
+			r = -EINVAL;
+			goto bad5;
+		}
+		if (!dmc->assoc || (dmc->assoc & (dmc->assoc - 1)) ||
+		    dmc->assoc > FLASHCACHE_MAX_ASSOC ||
+		    dmc->assoc < FLASHCACHE_MIN_ASSOC ||
+		    dmc->size < dmc->assoc) {
+			ti->error = "flashcache: Invalid cache associativity";
+			r = -EINVAL;
+			goto bad5;
+		}
+	}
+
+	if (!dmc->assoc)
+		dmc->assoc = DEFAULT_CACHE_ASSOC;
+
+	consecutive_blocks = dmc->assoc;
+	dmc->consecutive_shift = ffs(consecutive_blocks) - 1;
+
+	if (argc >= 7) {
+		if (sscanf(argv[6], "%u", &dmc->md_block_size) != 1) {
+			ti->error = "flashcache: Invalid metadata block size";
+			r = -EINVAL;
+			goto bad5;
+		}
+		if (!dmc->md_block_size || (dmc->md_block_size & (dmc->md_block_size - 1)) ||
+		    dmc->md_block_size > FLASHCACHE_MAX_MD_BLOCK_SIZE) {
+			ti->error = "flashcache: Invalid metadata block size";
+			r = -EINVAL;
+			goto bad5;
+		}
+		if (dmc->assoc <
+		    (dmc->md_block_size * 512 / sizeof(struct flash_cacheblock))) {
+			ti->error = "flashcache: Please choose a smaller metadata block size or larger assoc";
+			r = -EINVAL;
+			goto bad5;
+		}
+	}
+
+	if (!dmc->md_block_size)
+		dmc->md_block_size = DEFAULT_MD_BLOCK_SIZE;
+
+	if (dmc->md_block_size * 512 < dmc->cache_dev->bdev->bd_block_size) {
+		ti->error = "flashcache: Metadata block size must be >= cache device sector size";
+		r = -EINVAL;
+		goto bad5;
+	}
+
+	if (persistence == CACHE_CREATE) {
+		if (flashcache_md_create(dmc, 0)) {
+			ti->error = "flashcache: Cache Create Failed";
+			r = -EINVAL;
+			goto bad5;
+		}
+	} else {
+		if (flashcache_md_create(dmc, 1)) {
+			ti->error = "flashcache: Cache Force Create Failed";
+			r = -EINVAL;
+			goto bad5;
+		}
+	}
+
+init:
+	order = (dmc->size >> dmc->consecutive_shift) * sizeof(struct cache_set);
+	dmc->cache_sets = (struct cache_set *)vmalloc(order);
+	if (!dmc->cache_sets) {
+		ti->error = "Unable to allocate memory";
+		r = -ENOMEM;
+		vfree((void *)dmc->cache);
+		goto bad5;
+	}
+
+	for (i = 0 ; i < (dmc->size >> dmc->consecutive_shift) ; i++) {
+		dmc->cache_sets[i].set_fifo_next = i * dmc->assoc;
+		dmc->cache_sets[i].set_clean_next = i * dmc->assoc;
+		dmc->cache_sets[i].nr_dirty = 0;
+		dmc->cache_sets[i].clean_inprog = 0;
+		dmc->cache_sets[i].dirty_fallow = 0;
+		dmc->cache_sets[i].fallow_tstamp = jiffies;
+		dmc->cache_sets[i].fallow_next_cleaning = jiffies;
+		dmc->cache_sets[i].lru_tail = FLASHCACHE_LRU_NULL;
+		dmc->cache_sets[i].lru_head = FLASHCACHE_LRU_NULL;
+	}
+
+	/* Push all blocks into the set specific LRUs */
+	for (i = 0 ; i < dmc->size ; i++) {
+		dmc->cache[i].lru_prev = FLASHCACHE_LRU_NULL;
+		dmc->cache[i].lru_next = FLASHCACHE_LRU_NULL;
+		flashcache_reclaim_lru_movetail(dmc, i);
+	}
+
+	order = (dmc->md_blocks - 1) * sizeof(struct cache_md_block_head);
+	dmc->md_blocks_buf = (struct cache_md_block_head *)vmalloc(order);
+	if (!dmc->md_blocks_buf) {
+		ti->error = "Unable to allocate memory";
+		r = -ENOMEM;
+		vfree((void *)dmc->cache);
+		vfree((void *)dmc->cache_sets);
+		goto bad5;
+	}
+
+	for (i = 0 ; i < dmc->md_blocks - 1 ; i++) {
+		dmc->md_blocks_buf[i].nr_in_prog = 0;
+		dmc->md_blocks_buf[i].queued_updates = NULL;
+	}
+
+	spin_lock_init(&dmc->cache_spin_lock);
+
+	dmc->sync_index = 0;
+	dmc->clean_inprog = 0;
+
+	ti->split_io = dmc->block_size;
+	ti->private = dmc;
+
+	/* Cleaning Thresholds */
+	dmc->dirty_thresh_set = (dmc->assoc * sysctl_flashcache_dirty_thresh) / 100;
+	dmc->max_clean_ios_total = sysctl_max_clean_ios_total;
+	dmc->max_clean_ios_set = sysctl_max_clean_ios_set;
+
+	(void)wait_on_bit_lock(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST,
+			       flashcache_wait_schedule, TASK_UNINTERRUPTIBLE);
+	dmc->next_cache = cache_list_head;
+	cache_list_head = dmc;
+	clear_bit(FLASHCACHE_UPDATE_LIST, &flashcache_control->synch_flags);
+	smp_mb__after_clear_bit();
+	wake_up_bit(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST);
+
+	for (i = 0 ; i < dmc->size ; i++) {
+		if (dmc->cache[i].cache_state & VALID)
+			dmc->cached_blocks++;
+		if (dmc->cache[i].cache_state & DIRTY) {
+			dmc->cache_sets[i / dmc->assoc].nr_dirty++;
+			dmc->nr_dirty++;
+		}
+	}
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+	INIT_WORK(&dmc->delayed_clean, flashcache_clean_all_sets, dmc);
+#else
+	INIT_DELAYED_WORK(&dmc->delayed_clean, flashcache_clean_all_sets);
+#endif
+
+	dmc->whitelist_head = NULL;
+	dmc->whitelist_tail = NULL;
+	dmc->blacklist_head = NULL;
+	dmc->blacklist_tail = NULL;
+	dmc->num_whitelist_pids = 0;
+	dmc->num_blacklist_pids = 0;
+
+	return 0;
+
+bad5:
+	flashcache_kcached_client_destroy(dmc);
+bad4:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
+	dm_kcopyd_client_destroy(dmc->kcp_client);
+#else
+	kcopyd_client_destroy(dmc->kcp_client);
+#endif
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+	dm_io_client_destroy(dmc->io_client);
+#endif
+bad3:
+	dm_put_device(ti, dmc->cache_dev);
+bad2:
+	dm_put_device(ti, dmc->disk_dev);
+bad1:
+	kfree(dmc);
+bad:
+	return r;
+}
+
+static void
+flashcache_zero_stats(struct cache_c *dmc)
+{
+	int i;
+
+	memset(&dmc->flashcache_stats, 0, sizeof(struct flashcache_stats));
+	for (i = 0 ; i < IO_LATENCY_BUCKETS ; i++)
+		dmc->latency_hist[i] = 0;
+	dmc->latency_hist_10ms = 0;
+}
+
+/*
+ * Destroy the cache mapping.
+ */
+void
+flashcache_dtr(struct dm_target *ti)
+{
+	struct cache_c *dmc = (struct cache_c *) ti->private;
+	struct cache_c **nodepp;
+	int i;
+	int nr_queued = 0;
+	struct flashcache_stats *stats = &dmc->flashcache_stats;
+
+	flashcache_sync_for_remove(dmc);
+	flashcache_md_store(dmc);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
+	dm_io_put(FLASHCACHE_ASYNC_SIZE); /* Must be done after md_store() */
+#endif
+	if (!sysctl_flashcache_fast_remove && dmc->nr_dirty > 0)
+		DMERR("Could not sync %d blocks to disk, cache still dirty",
+		      dmc->nr_dirty);
+	DMINFO("cache jobs %d, pending jobs %d", atomic_read(&nr_cache_jobs),
+	       atomic_read(&nr_pending_jobs));
+	for (i = 0 ; i < dmc->size ; i++)
+		nr_queued += dmc->cache[i].nr_queued;
+	DMINFO("cache queued jobs %d", nr_queued);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+	kcopyd_client_destroy(dmc->kcp_client);
+#else
+	dm_kcopyd_client_destroy(dmc->kcp_client);
+#endif
+	if ((stats->reads > 0) && (stats->writes > 0)) {
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		DMINFO("stats: reads(%lu), writes(%lu), read hits(%lu), write hits(%lu), " \
+		       "read hit percent(%ld), replacement(%lu), write invalidates(%lu), " \
+		       "read invalidates(%lu), write replacement(%lu), pending enqueues(%lu), " \
+		       "pending inval(%lu) cleanings(%lu), fallow cleanings(%lu) " \
+		       "checksum invalid(%ld), checksum store(%ld), checksum valid(%ld)" \
+		       "front merge(%ld) back merge(%ld)",
+		       stats->reads, stats->writes,
+		       stats->read_hits, stats->write_hits,
+		       stats->read_hits*100/stats->reads,
+		       stats->replace, stats->wr_invalidates,
+		       stats->rd_invalidates,
+		       stats->wr_replace, stats->enqueues,
+		       stats->pending_inval, stats->cleanings,
+		       stats->fallow_cleanings,
+		       stats->checksum_store, stats->checksum_valid,
+		       stats->checksum_invalid,
+		       stats->front_merge, stats->back_merge);
+#else
+		DMINFO("stats: reads(%lu), writes(%lu), read hits(%lu), write hits(%lu), " \
+		       "read hit percent(%ld), replacement(%lu), write invalidates(%lu), " \
+		       "read invalidates(%lu), write replacement(%lu), pending enqueues(%lu), " \
+		       "pending inval(%lu) cleanings(%lu) fallow cleanings(%lu)" \
+		       "front merge(%ld) back merge(%ld)",
+		       stats->reads, stats->writes,
+		       stats->read_hits, stats->write_hits,
+		       stats->read_hits*100/stats->reads,
+		       stats->replace, stats->wr_invalidates,
+		       stats->rd_invalidates,
+		       stats->wr_replace, stats->enqueues,
+		       stats->pending_inval, stats->cleanings,
+		       stats->fallow_cleanings,
+		       stats->front_merge, stats->back_merge);
+#endif
+
+	}
+	if (dmc->size > 0) {
+		DMINFO("conf: capacity(%luM), associativity(%u), block size(%uK), " \
+		       "total blocks(%lu), cached blocks(%lu), cache percent(%ld), dirty blocks(%d)",
+		       dmc->size*dmc->block_size>>11, dmc->assoc,
+		       dmc->block_size>>(10-SECTOR_SHIFT),
+		       dmc->size, dmc->cached_blocks,
+		       (dmc->cached_blocks*100)/dmc->size, dmc->nr_dirty);
+	}
+	vfree((void *)dmc->cache);
+	vfree((void *)dmc->cache_sets);
+	vfree((void *)dmc->md_blocks_buf);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+	dm_io_client_destroy(dmc->io_client);
+#endif
+	flashcache_del_all_pids(dmc, FLASHCACHE_WHITELIST, 1);
+	flashcache_del_all_pids(dmc, FLASHCACHE_BLACKLIST, 1);
+	VERIFY(dmc->num_whitelist_pids == 0);
+	VERIFY(dmc->num_blacklist_pids == 0);
+	dm_put_device(ti, dmc->disk_dev);
+	dm_put_device(ti, dmc->cache_dev);
+	(void)wait_on_bit_lock(&flashcache_control->synch_flags,
+			       FLASHCACHE_UPDATE_LIST,
+			       flashcache_wait_schedule,
+			       TASK_UNINTERRUPTIBLE);
+	nodepp = &cache_list_head;
+	while (*nodepp != NULL) {
+		if (*nodepp == dmc) {
+			*nodepp = dmc->next_cache;
+			break;
+		}
+		nodepp = &((*nodepp)->next_cache);
+	}
+	clear_bit(FLASHCACHE_UPDATE_LIST, &flashcache_control->synch_flags);
+	smp_mb__after_clear_bit();
+	wake_up_bit(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST);
+	kfree(dmc);
+}
+
+extern int sysctl_flashcache_lat_hist;
+
+void
+flashcache_status_info(struct cache_c *dmc, status_type_t type,
+		       char *result, unsigned int maxlen)
+{
+	int read_hit_pct, write_hit_pct, dirty_write_hit_pct;
+	int sz = 0; /* DMEMIT */
+	struct flashcache_stats *stats = &dmc->flashcache_stats;
+
+	if (stats->reads > 0)
+		read_hit_pct = stats->read_hits * 100 / stats->reads;
+	else
+		read_hit_pct = 0;
+	if (stats->writes > 0) {
+		write_hit_pct = stats->write_hits * 100 / stats->writes;
+		dirty_write_hit_pct = stats->dirty_write_hits * 100 / stats->writes;
+	} else {
+		write_hit_pct = 0;
+		dirty_write_hit_pct = 0;
+	}
+	DMEMIT("stats: \n\treads(%lu), writes(%lu)\n",
+	       stats->reads, stats->writes);
+#ifdef FLASHCACHE_DO_CHECKSUMS
+	DMEMIT("\tread hits(%lu), read hit percent(%d)\n"		\
+	       "\twrite hits(%lu) write hit percent(%d)\n" 		\
+	       "\tdirty write hits(%lu) dirty write hit percent(%d)\n" 	\
+	       "\treplacement(%lu), write replacement(%lu)\n"		\
+	       "\twrite invalidates(%lu), read invalidates(%lu)\n"	\
+	       "\tchecksum store(%ld), checksum valid(%ld), checksum invalid(%ld)\n" \
+	       "\tpending enqueues(%lu), pending inval(%lu)\n"		\
+	       "\tmetadata dirties(%lu), metadata cleans(%lu)\n" \
+	       "\tmetadata batch(%lu) metadata ssd writes(%lu)\n" \
+	       "\tcleanings(%lu) fallow cleanings(%lu)\n"	   \
+	       "\tno room(%lu) front merge(%lu) back merge(%lu)\n" \
+	       "\tdisk reads(%lu), disk writes(%lu) ssd reads(%lu) ssd writes(%lu)\n" \
+	       "\tuncached reads(%lu), uncached writes(%lu), uncached IO requeue(%lu)\n" \
+	       "\tpid_adds(%lu), pid_dels(%lu), pid_drops(%lu) pid_expiry(%lu)",
+	       stats->read_hits, read_hit_pct,
+	       stats->write_hits, write_hit_pct,
+	       stats->dirty_write_hits, dirty_write_hit_pct,
+	       stats->replace, stats->wr_replace,
+	       stats->wr_invalidates, stats->rd_invalidates,
+	       stats->checksum_store, stats->checksum_valid, stats->checksum_invalid,
+	       stats->enqueues, stats->pending_inval,
+	       stats->md_write_dirty, stats->md_write_clean,
+	       stats->md_write_batch, stats->md_ssd_writes,
+	       stats->cleanings, stats->fallow_cleanings,
+	       stats->noroom, stats->front_merge, stats->back_merge,
+	       stats->disk_reads, stats->disk_writes, stats->ssd_reads, stats->ssd_writes,
+	       stats->uncached_reads, stats->uncached_writes, stats->uncached_io_requeue,
+	       stats->pid_adds, stats->pid_dels, stats->pid_drops, stats->expiry);
+#else
+	DMEMIT("\tread hits(%lu), read hit percent(%d)\n"		\
+	       "\twrite hits(%lu) write hit percent(%d)\n" 		\
+	       "\tdirty write hits(%lu) dirty write hit percent(%d)\n" 	\
+	       "\treplacement(%lu) write replacement(%lu)\n"		\
+	       "\twrite invalidates(%lu) read invalidates(%lu)\n"	\
+	       "\tpending enqueues(%lu) pending inval(%lu)\n"		\
+	       "\tmetadata dirties(%lu) metadata cleans(%lu)\n" \
+	       "\tmetadata batch(%lu) metadata ssd writes(%lu)\n" \
+	       "\tcleanings(%lu) fallow cleanings(%lu)\n" \
+	       "\tno room(%lu) front merge(%lu) back merge(%lu)\n" \
+	       "\tdisk reads(%lu) disk writes(%lu) ssd reads(%lu) ssd writes(%lu)\n" \
+	       "\tuncached reads(%lu) uncached writes(%lu), uncached IO requeue(%lu)\n" \
+	       "\tpid_adds(%lu) pid_dels(%lu) pid_drops(%lu) pid_expiry(%lu)",
+	       stats->read_hits, read_hit_pct,
+	       stats->write_hits, write_hit_pct,
+	       stats->dirty_write_hits, dirty_write_hit_pct,
+	       stats->replace, stats->wr_replace,
+	       stats->wr_invalidates, stats->rd_invalidates,
+	       stats->enqueues, stats->pending_inval,
+	       stats->md_write_dirty, stats->md_write_clean,
+	       stats->md_write_batch, stats->md_ssd_writes,
+	       stats->cleanings, stats->fallow_cleanings,
+	       stats->noroom, stats->front_merge, stats->back_merge,
+	       stats->disk_reads, stats->disk_writes, stats->ssd_reads, stats->ssd_writes,
+	       stats->uncached_reads, stats->uncached_writes, stats->uncached_io_requeue,
+	       stats->pid_adds, stats->pid_dels, stats->pid_drops, stats->expiry);
+#endif
+	if (sysctl_flashcache_lat_hist) {
+		int i;
+
+		DMEMIT("\nIO Latency Histogram: \n");
+		for (i = 1 ; i <= IO_LATENCY_BUCKETS ; i++) {
+			DMEMIT("< %d\tusecs : %lu\n", i * IO_LATENCY_GRAN_USECS, dmc->latency_hist[i - 1]);
+		}
+		DMEMIT("> 10\tmsecs : %lu", dmc->latency_hist_10ms);
+	}
+}
+
+static void
+flashcache_status_table(struct cache_c *dmc, status_type_t type,
+			     char *result, unsigned int maxlen)
+{
+	u_int64_t  cache_pct, dirty_pct;
+	int i;
+	int sz = 0; /* DMEMIT */
+
+	if (dmc->size > 0) {
+		dirty_pct = ((u_int64_t)dmc->nr_dirty * 100) / dmc->size;
+		cache_pct = ((u_int64_t)dmc->cached_blocks * 100) / dmc->size;
+	} else {
+		cache_pct = 0;
+		dirty_pct = 0;
+	}
+	DMEMIT("conf:\n"						\
+	       "\tssd dev (%s), disk dev (%s)\n"                        \
+	       "\tcapacity(%luM), associativity(%u), data block size(%uK) metadata block size(%ub)\n" \
+	       "\ttotal blocks(%lu), cached blocks(%lu), cache percent(%d)\n" \
+	       "\tdirty blocks(%d), dirty percent(%d)\n",
+	       dmc->cache_devname, dmc->disk_devname,
+	       dmc->size*dmc->block_size>>11, dmc->assoc,
+	       dmc->block_size>>(10-SECTOR_SHIFT),
+	       dmc->md_block_size * 512,
+	       dmc->size, dmc->cached_blocks,
+	       (int)cache_pct, dmc->nr_dirty, (int)dirty_pct);
+	DMEMIT("\tnr_queued(%lu)\n", dmc->pending_jobs_count);
+	DMEMIT("Size Hist: ");
+	for (i = 1 ; i <= 32 ; i++) {
+		if (size_hist[i] > 0)
+			DMEMIT("%d:%llu ", i*512, size_hist[i]);
+	}
+}
+
+/*
+ * Report cache status:
+ *  Output cache stats upon request of device status;
+ *  Output cache configuration upon request of table status.
+ */
+int
+flashcache_status(struct dm_target *ti, status_type_t type,
+	     char *result, unsigned int maxlen)
+{
+	struct cache_c *dmc = (struct cache_c *) ti->private;
+
+	switch (type) {
+	case STATUSTYPE_INFO:
+		flashcache_status_info(dmc, type, result, maxlen);
+		break;
+	case STATUSTYPE_TABLE:
+		flashcache_status_table(dmc, type, result, maxlen);
+		break;
+	}
+	return 0;
+}
+
+static struct target_type flashcache_target = {
+	.name   = "flashcache",
+	.version= {1, 0, 1},
+	.module = THIS_MODULE,
+	.ctr    = flashcache_ctr,
+	.dtr    = flashcache_dtr,
+	.map    = flashcache_map,
+	.status = flashcache_status,
+	.ioctl 	= flashcache_ioctl,
+};
+
+static void
+flashcache_sync_for_remove(struct cache_c *dmc)
+{
+	do {
+		atomic_set(&dmc->remove_in_prog, SLOW_REMOVE); /* Stop cleaning of sets */
+		if (!sysctl_flashcache_fast_remove) {
+			/*
+			 * Kick off cache cleaning. client_destroy will wait for cleanings
+			 * to finish.
+			 */
+			printk(KERN_ALERT "Cleaning %d blocks please WAIT", dmc->nr_dirty);
+			/* Tune up the cleaning parameters to clean very aggressively */
+			dmc->max_clean_ios_total = 20;
+			dmc->max_clean_ios_set = 10;
+			flashcache_sync_all(dmc);
+		} else {
+			/* Needed to abort any in-progress cleanings, leave blocks DIRTY */
+			atomic_set(&dmc->remove_in_prog, 1);
+			printk(KERN_ALERT "Fast flashcache remove Skipping cleaning of %d blocks",
+			       dmc->nr_dirty);
+		}
+		/*
+		 * We've prevented new cleanings from starting (for the fast remove case)
+		 * and we will wait for all in progress cleanings to exit.
+		 * Wait a few seconds for everything to quiesce before writing out the
+		 * cache metadata.
+		 */
+		msleep(FLASHCACHE_SYNC_REMOVE_DELAY);
+		/* Wait for all the dirty blocks to get written out, and any other IOs */
+		wait_event(dmc->destroyq, !atomic_read(&dmc->nr_jobs));
+		cancel_delayed_work(&dmc->delayed_clean);
+		flush_scheduled_work();
+	} while (!sysctl_flashcache_fast_remove && dmc->nr_dirty > 0);
+}
+
+static int
+flashcache_notify_reboot(struct notifier_block *this,
+			 unsigned long code, void *x)
+{
+	struct cache_c *dmc;
+
+	(void)wait_on_bit_lock(&flashcache_control->synch_flags,
+			       FLASHCACHE_UPDATE_LIST,
+			       flashcache_wait_schedule,
+			       TASK_UNINTERRUPTIBLE);
+	for (dmc = cache_list_head ;
+	     dmc != NULL ;
+	     dmc = dmc->next_cache) {
+		flashcache_sync_for_remove(dmc);
+		flashcache_md_store(dmc);
+	}
+	clear_bit(FLASHCACHE_UPDATE_LIST, &flashcache_control->synch_flags);
+	smp_mb__after_clear_bit();
+	wake_up_bit(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST);
+	return NOTIFY_DONE;
+}
+
+/*
+ * The notifiers are registered in descending order of priority and
+ * executed in descending order or priority. We should be run before
+ * any notifiers of ssd's or other block devices. Typically, devices
+ * use a priority of 0.
+ * XXX - If in the future we happen to use a md device as the cache
+ * block device, we have a problem because md uses a priority of
+ * INT_MAX as well. But we want to run before the md's reboot notifier !
+ */
+static struct notifier_block flashcache_notifier = {
+	.notifier_call	= flashcache_notify_reboot,
+	.next		= NULL,
+	.priority	= INT_MAX, /* should be > ssd pri's and disk dev pri's */
+};
+
+static int
+flashcache_stats_show(struct seq_file *seq, void *v)
+{
+	struct cache_c *dmc;
+	struct flashcache_stats *stats;
+
+	(void)wait_on_bit_lock(&flashcache_control->synch_flags,
+			       FLASHCACHE_UPDATE_LIST,
+			       flashcache_wait_schedule,
+			       TASK_UNINTERRUPTIBLE);
+	for (dmc = cache_list_head ;
+	     dmc != NULL ;
+	     dmc = dmc->next_cache) {
+		int read_hit_pct, write_hit_pct, dirty_write_hit_pct;
+
+		stats = &dmc->flashcache_stats;
+		if (stats->reads > 0)
+			read_hit_pct = stats->read_hits * 100 / stats->reads;
+		else
+			read_hit_pct = 0;
+		if (stats->writes > 0) {
+			write_hit_pct = stats->write_hits * 100 / stats->writes;
+			dirty_write_hit_pct = stats->dirty_write_hits * 100 / stats->writes;
+		} else {
+			write_hit_pct = 0;
+			dirty_write_hit_pct = 0;
+		}
+		seq_printf(seq, "reads=%lu writes=%lu ", stats->reads,
+			   stats->writes);
+		seq_printf(seq, "read_hits=%lu read_hit_percent=%d write_hits=%lu write_hit_percent=%d ",
+			   stats->read_hits, read_hit_pct,
+			   stats->write_hits, write_hit_pct);
+		seq_printf(seq, "dirty_write_hits=%lu dirty_write_hit_percent=%d ",
+			   stats->dirty_write_hits, dirty_write_hit_pct);
+		seq_printf(seq, "replacement=%lu write_replacement=%lu ",
+			   stats->replace, stats->wr_replace);
+		seq_printf(seq, "write_invalidates=%lu read_invalidates=%lu ",
+			   stats->wr_invalidates, stats->rd_invalidates);
+		seq_printf(seq, "pending_enqueues=%lu pending_inval=%lu ",
+			   stats->enqueues, stats->pending_inval);
+		seq_printf(seq, "metadata_dirties=%lu metadata_cleans=%lu ",
+			   stats->md_write_dirty, stats->md_write_clean);
+		seq_printf(seq, "cleanings=%lu no_room=%lu front_merge=%lu back_merge=%lu ",
+			   stats->cleanings, stats->noroom, stats->front_merge, stats->back_merge);
+		seq_printf(seq, "pid_adds=%lu pid_dels=%lu pid_drops=%lu pid_expiry=%lu ",
+			   stats->pid_adds, stats->pid_dels, stats->pid_drops, stats->expiry);
+		seq_printf(seq, "disk_reads=%lu disk_writes=%lu ssd_reads=%lu ssd_writes=%lu ",
+			   stats->disk_reads, stats->disk_writes, stats->ssd_reads, stats->ssd_writes);
+		seq_printf(seq, "uncached_reads=%lu uncached_writes=%lu\n",
+			   stats->uncached_reads, stats->uncached_writes);
+
+	}
+	clear_bit(FLASHCACHE_UPDATE_LIST, &flashcache_control->synch_flags);
+	smp_mb__after_clear_bit();
+	wake_up_bit(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST);
+	return 0;
+}
+
+static int
+flashcache_stats_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &flashcache_stats_show, NULL);
+}
+
+static struct file_operations flashcache_stats_operations = {
+	.open		= flashcache_stats_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int
+flashcache_errors_show(struct seq_file *seq, void *v)
+{
+	struct cache_c *dmc;
+
+	(void)wait_on_bit_lock(&flashcache_control->synch_flags,
+			       FLASHCACHE_UPDATE_LIST,
+			       flashcache_wait_schedule,
+			       TASK_UNINTERRUPTIBLE);
+	for (dmc = cache_list_head ;
+	     dmc != NULL ;
+	     dmc = dmc->next_cache) {
+		seq_printf(seq, "disk_read_errors=%d disk_write_errors=%d ",
+			   dmc->flashcache_errors.disk_read_errors,
+			   dmc->flashcache_errors.disk_write_errors);
+		seq_printf(seq, "ssd_read_errors=%d ssd_write_errors=%d ",
+			   dmc->flashcache_errors.ssd_read_errors,
+			   dmc->flashcache_errors.ssd_write_errors);
+		seq_printf(seq, "memory_alloc_errors=%d\n",
+			   dmc->flashcache_errors.memory_alloc_errors);
+		memset(&dmc->flashcache_errors, 0, sizeof(struct flashcache_errors));
+	}
+	clear_bit(FLASHCACHE_UPDATE_LIST, &flashcache_control->synch_flags);
+	smp_mb__after_clear_bit();
+	wake_up_bit(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST);
+	return 0;
+}
+
+static int
+flashcache_errors_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &flashcache_errors_show, NULL);
+}
+
+static struct file_operations flashcache_errors_operations = {
+	.open		= flashcache_errors_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int
+flashcache_iosize_hist_show(struct seq_file *seq, void *v)
+{
+	int i;
+
+	for (i = 1 ; i <= 32 ; i++) {
+		seq_printf(seq, "%d:%llu ", i*512, size_hist[i]);
+	}
+	seq_printf(seq, "\n");
+	return 0;
+}
+
+static int
+flashcache_iosize_hist_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &flashcache_iosize_hist_show, NULL);
+}
+
+static struct file_operations flashcache_iosize_hist_operations = {
+	.open		= flashcache_iosize_hist_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int
+flashcache_pidlists_show(struct seq_file *seq, void *v)
+{
+	struct cache_c *dmc;
+	struct flashcache_cachectl_pid *pid_list;
+ 	unsigned long flags;
+
+	(void)wait_on_bit_lock(&flashcache_control->synch_flags,
+			       FLASHCACHE_UPDATE_LIST,
+			       flashcache_wait_schedule,
+			       TASK_UNINTERRUPTIBLE);
+	for (dmc = cache_list_head ;
+	     dmc != NULL ;
+	     dmc = dmc->next_cache) {
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		seq_printf(seq, "Blacklist: ");
+		pid_list = dmc->blacklist_head;
+		while (pid_list != NULL) {
+			seq_printf(seq, "%u ", pid_list->pid);
+			pid_list = pid_list->next;
+		}
+		seq_printf(seq, "\n");
+		seq_printf(seq, "Whitelist: ");
+		pid_list = dmc->whitelist_head;
+		while (pid_list != NULL) {
+			seq_printf(seq, "%u ", pid_list->pid);
+			pid_list = pid_list->next;
+		}
+		seq_printf(seq, "\n");
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	}
+	clear_bit(FLASHCACHE_UPDATE_LIST, &flashcache_control->synch_flags);
+	smp_mb__after_clear_bit();
+	wake_up_bit(&flashcache_control->synch_flags, FLASHCACHE_UPDATE_LIST);
+	return 0;
+}
+
+static int
+flashcache_pidlists_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &flashcache_pidlists_show, NULL);
+}
+
+static struct file_operations flashcache_pidlists_operations = {
+	.open		= flashcache_pidlists_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+extern char *flashcache_sw_version;
+
+static int
+flashcache_version_show(struct seq_file *seq, void *v)
+{
+	seq_printf(seq, "Flashcache Version : %s\n", flashcache_sw_version);
+#ifdef COMMIT_REV
+	seq_printf(seq, "git commit: %s\n", COMMIT_REV);
+#endif
+	return 0;
+}
+
+static int
+flashcache_version_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &flashcache_version_show, NULL);
+}
+
+static struct file_operations flashcache_version_operations = {
+	.open		= flashcache_version_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+/*
+ * Initiate a cache target.
+ */
+int __init
+flashcache_init(void)
+{
+	int r;
+
+	r = flashcache_jobs_init();
+	if (r)
+		return r;
+	atomic_set(&nr_cache_jobs, 0);
+	atomic_set(&nr_pending_jobs, 0);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+	INIT_WORK(&_kcached_wq, do_work, NULL);
+#else
+	INIT_WORK(&_kcached_wq, do_work);
+#endif
+	for (r = 0 ; r < 33 ; r++)
+		size_hist[r] = 0;
+	r = dm_register_target(&flashcache_target);
+	if (r < 0) {
+		DMERR("cache: register failed %d", r);
+	}
+
+        printk("flashcache: %s initialized\n", flashcache_sw_version);
+
+#ifdef CONFIG_PROC_FS
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+	flashcache_table_header =
+		register_sysctl_table(flashcache_root_table, 1);
+#else
+	flashcache_table_header =
+		register_sysctl_table(flashcache_root_table);
+#endif
+	{
+		struct proc_dir_entry *entry;
+
+		entry = create_proc_entry("flashcache_stats", 0, NULL);
+		if (entry)
+			entry->proc_fops =  &flashcache_stats_operations;
+		entry = create_proc_entry("flashcache_errors", 0, NULL);
+		if (entry)
+			entry->proc_fops =  &flashcache_errors_operations;
+		entry = create_proc_entry("flashcache_iosize_hist", 0, NULL);
+		if (entry)
+			entry->proc_fops =  &flashcache_iosize_hist_operations;
+		entry = create_proc_entry("flashcache_pidlists", 0, NULL);
+		if (entry)
+			entry->proc_fops =  &flashcache_pidlists_operations;
+		entry = create_proc_entry("flashcache_version", 0, NULL);
+		if (entry)
+			entry->proc_fops =  &flashcache_version_operations;
+	}
+#endif
+	flashcache_control = (struct flashcache_control_s *)
+		kmalloc(sizeof(struct flashcache_control_s), GFP_KERNEL);
+	flashcache_control->synch_flags = 0;
+	register_reboot_notifier(&flashcache_notifier);
+	return r;
+}
+
+/*
+ * Destroy a cache target.
+ */
+void
+flashcache_exit(void)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+	int r = dm_unregister_target(&flashcache_target);
+
+	if (r < 0)
+		DMERR("cache: unregister failed %d", r);
+#else
+	dm_unregister_target(&flashcache_target);
+#endif
+	unregister_reboot_notifier(&flashcache_notifier);
+	flashcache_jobs_exit();
+#ifdef CONFIG_PROC_FS
+	unregister_sysctl_table(flashcache_table_header);
+	remove_proc_entry("flashcache_stats", NULL);
+	remove_proc_entry("flashcache_errors", NULL);
+	remove_proc_entry("flashcache_iosize_hist", NULL);
+	remove_proc_entry("flashcache_pidlists", NULL);
+	remove_proc_entry("flashcache_version", NULL);
+#endif
+	kfree(flashcache_control);
+}
+
+module_init(flashcache_init);
+module_exit(flashcache_exit);
+
+EXPORT_SYMBOL(flashcache_md_load);
+EXPORT_SYMBOL(flashcache_md_create);
+EXPORT_SYMBOL(flashcache_md_store);
+
+MODULE_DESCRIPTION(DM_NAME " Facebook flash cache target");
+MODULE_AUTHOR("Mohan - based on code by Ming");
+MODULE_LICENSE("GPL");
Index: linux-2.6.32-131.0.15.el6/drivers/md/flashcache_ioctl.c
===================================================================
--- /dev/null
+++ linux-2.6.32-131.0.15.el6/drivers/md/flashcache_ioctl.c
@@ -0,0 +1,428 @@
+/****************************************************************************
+ *  flashcache_ioctl.c
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#include <asm/atomic.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/list.h>
+#include <linux/blkdev.h>
+#include <linux/bio.h>
+#include <linux/slab.h>
+#include <linux/hash.h>
+#include <linux/spinlock.h>
+#include <linux/workqueue.h>
+#include <linux/pagemap.h>
+#include <linux/random.h>
+#include <linux/hardirq.h>
+#include <linux/sysctl.h>
+#include <linux/version.h>
+#include <linux/pid.h>
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+#include "dm.h"
+#include "dm-io.h"
+#include "dm-bio-list.h"
+#include "kcopyd.h"
+#else
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+#include "dm.h"
+#endif
+#include <linux/device-mapper.h>
+#include <linux/bio.h>
+#include <linux/dm-kcopyd.h>
+#endif
+#include "flashcache.h"
+#include "flashcache_ioctl.h"
+
+extern int sysctl_flashcache_max_pids;
+extern int sysctl_pid_expiry_check;
+extern int sysctl_cache_all;
+
+static int flashcache_find_pid_locked(struct cache_c *dmc, pid_t pid,
+				      int which_list);
+static void flashcache_del_pid_locked(struct cache_c *dmc, pid_t pid,
+				      int which_list);
+
+static int
+flashcache_find_pid_locked(struct cache_c *dmc, pid_t pid,
+			   int which_list)
+{
+	struct flashcache_cachectl_pid *pid_list;
+
+	pid_list = ((which_list == FLASHCACHE_WHITELIST) ?
+		    dmc->whitelist_head : dmc->blacklist_head);
+	for ( ; pid_list != NULL ; pid_list = pid_list->next) {
+		if (pid_list->pid == pid)
+			return 1;
+	}
+	return 0;
+}
+
+static void
+flashcache_drop_pids(struct cache_c *dmc, int which_list)
+{
+	if (which_list == FLASHCACHE_WHITELIST) {
+		while (dmc->num_whitelist_pids >= sysctl_flashcache_max_pids) {
+			VERIFY(dmc->whitelist_head != NULL);
+			flashcache_del_pid_locked(dmc, dmc->whitelist_tail->pid,
+						  which_list);
+			dmc->flashcache_stats.pid_drops++;
+		}
+	} else {
+		while (dmc->num_blacklist_pids >= sysctl_flashcache_max_pids) {
+			VERIFY(dmc->blacklist_head != NULL);
+			flashcache_del_pid_locked(dmc, dmc->blacklist_tail->pid,
+						  which_list);
+			dmc->flashcache_stats.pid_drops++;
+		}
+	}
+}
+
+static void
+flashcache_add_pid(struct cache_c *dmc, pid_t pid, int which_list)
+{
+	struct flashcache_cachectl_pid *new;
+ 	unsigned long flags;
+
+	new = kmalloc(sizeof(struct flashcache_cachectl_pid), GFP_KERNEL);
+	new->pid = pid;
+	new->next = NULL;
+	new->expiry = jiffies + sysctl_pid_expiry_check * HZ;
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	if (which_list == FLASHCACHE_WHITELIST) {
+		if (dmc->num_whitelist_pids > sysctl_flashcache_max_pids)
+			flashcache_drop_pids(dmc, which_list);
+	} else {
+		if (dmc->num_blacklist_pids > sysctl_flashcache_max_pids)
+			flashcache_drop_pids(dmc, which_list);
+	}
+	if (flashcache_find_pid_locked(dmc, pid, which_list) == 0) {
+		struct flashcache_cachectl_pid **head, **tail;
+
+		if (which_list == FLASHCACHE_WHITELIST) {
+			head = &dmc->whitelist_head;
+			tail = &dmc->whitelist_tail;
+		} else {
+			head = &dmc->blacklist_head;
+			tail = &dmc->blacklist_tail;
+		}
+		/* Add the new pid to the tail */
+		new->prev = *tail;
+		if (*head == NULL) {
+			VERIFY(*tail == NULL);
+			*head = new;
+		} else {
+			VERIFY(*tail != NULL);
+			(*tail)->next = new;
+		}
+		*tail = new;
+		if (which_list == FLASHCACHE_WHITELIST)
+			dmc->num_whitelist_pids++;
+		else
+			dmc->num_blacklist_pids++;
+		dmc->flashcache_stats.pid_adds++;
+		/* When adding the first entry to list, set expiry check timeout */
+		if (*head == new)
+			dmc->pid_expire_check =
+				jiffies + ((sysctl_pid_expiry_check + 1) * HZ);
+	} else
+		kfree(new);
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	return;
+}
+
+static void
+flashcache_del_pid_locked(struct cache_c *dmc, pid_t pid, int which_list)
+{
+	struct flashcache_cachectl_pid *node;
+	struct flashcache_cachectl_pid **head, **tail;
+
+	if (which_list == FLASHCACHE_WHITELIST) {
+		head = &dmc->whitelist_head;
+		tail = &dmc->whitelist_tail;
+	} else {
+		head = &dmc->blacklist_head;
+		tail = &dmc->blacklist_tail;
+	}
+	for (node = *tail ; node != NULL ; node = node->prev) {
+		if (which_list == FLASHCACHE_WHITELIST)
+			VERIFY(dmc->num_whitelist_pids > 0);
+		else
+			VERIFY(dmc->num_blacklist_pids > 0);
+		if (node->pid == pid) {
+			if (node->prev == NULL) {
+				*head = node->next;
+				if (node->next)
+					node->next->prev = NULL;
+			} else
+				node->prev->next = node->next;
+			if (node->next == NULL) {
+				*tail = node->prev;
+				if (node->prev)
+					node->prev->next = NULL;
+			} else
+				node->next->prev = node->prev;
+			kfree(node);
+			dmc->flashcache_stats.pid_dels++;
+			if (which_list == FLASHCACHE_WHITELIST)
+				dmc->num_whitelist_pids--;
+			else
+				dmc->num_blacklist_pids--;
+			return;
+		}
+	}
+}
+
+static void
+flashcache_del_pid(struct cache_c *dmc, pid_t pid, int which_list)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	flashcache_del_pid_locked(dmc, pid, which_list);
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+}
+
+/*
+ * This removes all "dead" pids. Pids that may have not cleaned up.
+ */
+void
+flashcache_del_all_pids(struct cache_c *dmc, int which_list, int force)
+{
+	struct flashcache_cachectl_pid *node, **tail;
+	unsigned long flags;
+
+	if (which_list == FLASHCACHE_WHITELIST)
+		tail = &dmc->whitelist_tail;
+	else
+		tail = &dmc->blacklist_tail;
+	rcu_read_lock();
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	node = *tail;
+	while (node != NULL) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,31)
+		if (force == 0) {
+			struct task_struct *task;
+
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,23)
+			task = find_task_by_pid_type(PIDTYPE_PID, node->pid);
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(2,6,31)
+			task = find_task_by_vpid(node->pid);
+#else
+			ask = pid_task(find_vpid(node->pid), PIDTYPE_PID);
+#endif
+			/*
+			 * If that task was found, don't remove it !
+			 * This prevents a rogue "delete all" from removing
+			 * every thread from the list.
+			 */
+			if (task) {
+				node = node->prev;
+				continue;
+			}
+		}
+#endif
+		flashcache_del_pid_locked(dmc, node->pid, which_list);
+		node = *tail;
+	}
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	rcu_read_unlock();
+}
+
+static void
+flashcache_pid_expiry_list_locked(struct cache_c *dmc, int which_list)
+{
+	struct flashcache_cachectl_pid **head, **tail, *node;
+
+	if (which_list == FLASHCACHE_WHITELIST) {
+		head = &dmc->whitelist_head;
+		tail = &dmc->whitelist_tail;
+	} else {
+		head = &dmc->blacklist_head;
+		tail = &dmc->blacklist_tail;
+	}
+	for (node = *head ; node != NULL ; node = node->next) {
+		if (which_list == FLASHCACHE_WHITELIST)
+			VERIFY(dmc->num_whitelist_pids > 0);
+		else
+			VERIFY(dmc->num_blacklist_pids > 0);
+		if (time_after(node->expiry, jiffies))
+			continue;
+		if (node->prev == NULL) {
+			*head = node->next;
+			if (node->next)
+				node->next->prev = NULL;
+		} else
+			node->prev->next = node->next;
+		if (node->next == NULL) {
+			*tail = node->prev;
+			if (node->prev)
+				node->prev->next = NULL;
+		} else
+			node->next->prev = node->prev;
+		kfree(node);
+		if (which_list == FLASHCACHE_WHITELIST)
+			dmc->num_whitelist_pids--;
+		else
+			dmc->num_blacklist_pids--;
+		dmc->flashcache_stats.expiry++;
+	}
+}
+
+void
+flashcache_pid_expiry_all_locked(struct cache_c *dmc)
+{
+	if (likely(time_before(jiffies, dmc->pid_expire_check)))
+		return;
+	flashcache_pid_expiry_list_locked(dmc, FLASHCACHE_WHITELIST);
+	flashcache_pid_expiry_list_locked(dmc, FLASHCACHE_BLACKLIST);
+	dmc->pid_expire_check = jiffies + (sysctl_pid_expiry_check + 1) * HZ;
+}
+
+/*
+ * Is the IO cacheable, depending on global cacheability and the white/black
+ * lists ? This function is a bit confusing because we want to support inheritance
+ * of cacheability across pthreads (so we use the tgid). But when an entire thread
+ * group is added to the white/black list, we want to provide for exceptions for
+ * individual threads as well.
+ * The Rules (in decreasing order of priority) :
+ * 1) Check the pid (thread id) against the list.
+ * 2) Check the tgid against the list, then check for exceptions within the tgid.
+ */
+int
+flashcache_uncacheable(struct cache_c *dmc)
+{
+	int dontcache;
+
+	if (sysctl_cache_all) {
+		/* If the tid has been blacklisted, we don't cache at all.
+		   This overrides everything else */
+		dontcache = flashcache_find_pid_locked(dmc, current->pid,
+						       FLASHCACHE_BLACKLIST);
+		if (dontcache)
+			goto out;
+		/* Is the tgid in the blacklist ? */
+		dontcache = flashcache_find_pid_locked(dmc, current->tgid,
+						       FLASHCACHE_BLACKLIST);
+		/*
+		 * If we found the tgid in the blacklist, is there a whitelist
+		 * exception entered for this thread ?
+		 */
+		if (dontcache) {
+			if (flashcache_find_pid_locked(dmc, current->pid,
+						       FLASHCACHE_WHITELIST))
+				dontcache = 0;
+		}
+	} else { /* cache nothing */
+		/* If the tid has been whitelisted, we cache
+		   This overrides everything else */
+		dontcache = !flashcache_find_pid_locked(dmc, current->pid,
+							FLASHCACHE_WHITELIST);
+		if (!dontcache)
+			goto out;
+		/* Is the tgid in the whitelist ? */
+		dontcache = !flashcache_find_pid_locked(dmc, current->tgid,
+							FLASHCACHE_WHITELIST);
+		/*
+		 * If we found the tgid in the whitelist, is there a black list
+		 * exception entered for this thread ?
+		 */
+		if (!dontcache) {
+			if (flashcache_find_pid_locked(dmc, current->pid,
+						       FLASHCACHE_BLACKLIST))
+				dontcache = 1;
+		}
+	}
+out:
+	return dontcache;
+}
+
+/*
+ * Add/del pids whose IOs should be non-cacheable.
+ * We limit this number to 100 (arbitrary and sysctl'able).
+ * We also add an expiry to each entry (defaluts at 60 sec,
+ * arbitrary and sysctlable).
+ * This is needed because Linux lacks an "at_exit()" hook
+ * that modules can supply to do any cleanup on process
+ * exit, for cases where the process dies after marking itself
+ * non-cacheable.
+ */
+int
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+flashcache_ioctl(struct dm_target *ti, struct inode *inode,
+		 struct file *filp, unsigned int cmd,
+		 unsigned long arg)
+#else
+flashcache_ioctl(struct dm_target *ti, unsigned int cmd, unsigned long arg)
+#endif
+{
+	struct cache_c *dmc = (struct cache_c *) ti->private;
+	struct block_device *bdev = dmc->disk_dev->bdev;
+	struct file fake_file = {};
+	struct dentry fake_dentry = {};
+	pid_t pid;
+
+	switch(cmd) {
+	case FLASHCACHEADDBLACKLIST:
+		if (copy_from_user(&pid, (pid_t *)arg, sizeof(pid_t)))
+			return -EFAULT;
+		flashcache_add_pid(dmc, pid, FLASHCACHE_BLACKLIST);
+		return 0;
+	case FLASHCACHEDELBLACKLIST:
+		if (copy_from_user(&pid, (pid_t *)arg, sizeof(pid_t)))
+			return -EFAULT;
+		flashcache_del_pid(dmc, pid, FLASHCACHE_BLACKLIST);
+		return 0;
+	case FLASHCACHEDELALLBLACKLIST:
+		flashcache_del_all_pids(dmc, FLASHCACHE_BLACKLIST, 0);
+		return 0;
+	case FLASHCACHEADDWHITELIST:
+		if (copy_from_user(&pid, (pid_t *)arg, sizeof(pid_t)))
+			return -EFAULT;
+		flashcache_add_pid(dmc, pid, FLASHCACHE_WHITELIST);
+		return 0;
+	case FLASHCACHEDELWHITELIST:
+		if (copy_from_user(&pid, (pid_t *)arg, sizeof(pid_t)))
+			return -EFAULT;
+		flashcache_del_pid(dmc, pid, FLASHCACHE_WHITELIST);
+		return 0;
+	case FLASHCACHEDELALLWHITELIST:
+		flashcache_del_all_pids(dmc, FLASHCACHE_WHITELIST, 0);
+		return 0;
+	default:
+		fake_file.f_mode = dmc->disk_dev->mode;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+		fake_file.f_dentry = &fake_dentry;
+#else
+		fake_file.f_path.dentry = &fake_dentry;
+#endif
+		fake_dentry.d_inode = bdev->bd_inode;
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+		return blkdev_driver_ioctl(bdev->bd_inode, &fake_file, bdev->bd_disk, cmd, arg);
+#else
+		return __blkdev_driver_ioctl(dmc->disk_dev->bdev, dmc->disk_dev->mode, cmd, arg);
+#endif
+	}
+
+}
Index: linux-2.6.32-131.0.15.el6/drivers/md/flashcache_ioctl.h
===================================================================
--- /dev/null
+++ linux-2.6.32-131.0.15.el6/drivers/md/flashcache_ioctl.h
@@ -0,0 +1,67 @@
+/****************************************************************************
+ *  flashcache_ioctl.h
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#ifndef FLASHCACHE_IOCTL_H
+#define FLASHCACHE_IOCTL_H
+
+#include <linux/types.h>
+
+#define FLASHCACHE_IOCTL 0xfe
+
+enum {
+	FLASHCACHEADDNCPID_CMD=200,
+	FLASHCACHEDELNCPID_CMD,
+	FLASHCACHEDELNCALL_CMD,
+	FLASHCACHEADDWHITELIST_CMD,
+	FLASHCACHEDELWHITELIST_CMD,
+	FLASHCACHEDELWHITELISTALL_CMD,
+};
+
+#define FLASHCACHEADDNCPID	_IOW(FLASHCACHE_IOCTL, FLASHCACHEADDNCPID_CMD, pid_t)
+#define FLASHCACHEDELNCPID	_IOW(FLASHCACHE_IOCTL, FLASHCACHEDELNCPID_CMD, pid_t)
+#define FLASHCACHEDELNCALL	_IOW(FLASHCACHE_IOCTL, FLASHCACHEDELNCALL_CMD, pid_t)
+
+#define FLASHCACHEADDBLACKLIST		FLASHCACHEADDNCPID
+#define FLASHCACHEDELBLACKLIST		FLASHCACHEDELNCPID
+#define FLASHCACHEDELALLBLACKLIST	FLASHCACHEDELNCALL
+
+#define FLASHCACHEADDWHITELIST		_IOW(FLASHCACHE_IOCTL, FLASHCACHEADDWHITELIST_CMD, pid_t)
+#define FLASHCACHEDELWHITELIST		_IOW(FLASHCACHE_IOCTL, FLASHCACHEDELWHITELIST_CMD, pid_t)
+#define FLASHCACHEDELALLWHITELIST	_IOW(FLASHCACHE_IOCTL, FLASHCACHEDELWHITELISTALL_CMD, pid_t)
+
+#ifdef __KERNEL__
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+int flashcache_ioctl(struct dm_target *ti, struct inode *inode,
+		     struct file *filp, unsigned int cmd,
+		     unsigned long arg);
+#else
+int flashcache_ioctl(struct dm_target *ti, unsigned int cmd,
+ 		     unsigned long arg);
+#endif
+void flashcache_pid_expiry_all_locked(struct cache_c *dmc);
+int flashcache_uncacheable(struct cache_c *dmc);
+void flashcache_del_all_pids(struct cache_c *dmc, int which_list, int force);
+#endif /* __KERNEL__ */
+
+#endif
Index: linux-2.6.32-131.0.15.el6/drivers/md/flashcache_main.c
===================================================================
--- /dev/null
+++ linux-2.6.32-131.0.15.el6/drivers/md/flashcache_main.c
@@ -0,0 +1,2018 @@
+/****************************************************************************
+ *  flashcache_main.c
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#include <asm/atomic.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/list.h>
+#include <linux/blkdev.h>
+#include <linux/bio.h>
+#include <linux/slab.h>
+#include <linux/hash.h>
+#include <linux/spinlock.h>
+#include <linux/workqueue.h>
+#include <linux/pagemap.h>
+#include <linux/random.h>
+#include <linux/hardirq.h>
+#include <linux/sysctl.h>
+#include <linux/version.h>
+#include <linux/pid.h>
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,21)
+#include <linux/device-mapper.h>
+#include <linux/bio.h>
+#endif
+#include "dm.h"
+#include "dm-io.h"
+#include "dm-bio-list.h"
+#include "kcopyd.h"
+#else
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+#include "dm.h"
+#endif
+#include <linux/device-mapper.h>
+#include <linux/bio.h>
+#include <linux/dm-kcopyd.h>
+#endif
+#include "flashcache.h"
+#include "flashcache_ioctl.h"
+
+#ifndef DM_MAPIO_SUBMITTED
+#define DM_MAPIO_SUBMITTED	0
+#endif
+
+/*
+ * TODO List :
+ * 1) sysctls : Create per-cache device sysctls instead of global sysctls.
+ * 2) Management of non cache pids : Needs improvement. Remove registration
+ * on process exits (with  a pseudo filesstem'ish approach perhaps) ?
+ * 3) Breaking up the cache spinlock : Right now contention on the spinlock
+ * is not a problem. Might need change in future.
+ * 4) Use the standard linked list manipulation macros instead rolling our own.
+ * 5) Fix a security hole : A malicious process with 'ro' access to a file can
+ * potentially corrupt file data. This can be fixed by copying the data on a
+ * cache read miss.
+ */
+
+#define FLASHCACHE_SW_VERSION "flashcache-1.0"
+char *flashcache_sw_version = FLASHCACHE_SW_VERSION;
+
+static void flashcache_read_miss(struct cache_c *dmc, struct bio* bio,
+				 int index);
+static void flashcache_write(struct cache_c *dmc, struct bio* bio);
+static int flashcache_inval_blocks(struct cache_c *dmc, struct bio *bio);
+static void flashcache_dirty_writeback(struct cache_c *dmc, int index);
+void flashcache_sync_blocks(struct cache_c *dmc);
+static void flashcache_start_uncached_io(struct cache_c *dmc, struct bio *bio);
+
+extern struct work_struct _kcached_wq;
+extern u_int64_t size_hist[];
+
+extern int sysctl_flashcache_error_inject;
+extern int sysctl_flashcache_stop_sync;
+extern int sysctl_flashcache_reclaim_policy;
+extern int sysctl_pid_do_expiry;
+extern int sysctl_fallow_clean_speed;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+int dm_io_async_bvec(unsigned int num_regions,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
+			    struct dm_io_region *where,
+#else
+			    struct io_region *where,
+#endif
+			    int rw,
+			    struct bio_vec *bvec, io_notify_fn fn,
+			    void *context)
+{
+	struct kcached_job *job = (struct kcached_job *)context;
+	struct cache_c *dmc = job->dmc;
+	struct dm_io_request iorq;
+
+	iorq.bi_rw = rw;
+	iorq.mem.type = DM_IO_BVEC;
+	iorq.mem.ptr.bvec = bvec;
+	iorq.notify.fn = fn;
+	iorq.notify.context = context;
+	iorq.client = dmc->io_client;
+	return dm_io(&iorq, num_regions, where, NULL);
+}
+#endif
+
+/*
+ * A simple 2-hand clock like algorithm is used to identify dirty blocks
+ * that lie fallow in the cache and thus are candidates for cleaning.
+ * Note that we could have such fallow blocks in sets where the dirty blocks
+ * is under the configured threshold.
+ * The hands are spaced 60 seconds apart (one sweep runs every 60 seconds).
+ * The interval is configurable via a sysctl.
+ * Blocks are moved to DIRTY_FALLOW_1, if they are found to be in DIRTY_FALLOW_1
+ * for 60 seconds or more, they are moved to DIRTY_FALLOW_1 | DIRTY_FALLOW_2, at
+ * which point they are eligible for cleaning. Of course any intervening use
+ * of the block within the interval turns off these 2 bits.
+ *
+ * Cleaning of these blocks happens from the flashcache_clean_set() function.
+ */
+void
+flashcache_detect_fallow(struct cache_c *dmc, int index)
+{
+	struct cacheblock *cacheblk = &dmc->cache[index];
+
+	if ((cacheblk->cache_state & DIRTY) &&
+	    ((cacheblk->cache_state & BLOCK_IO_INPROG) == 0)) {
+		if ((cacheblk->cache_state & DIRTY_FALLOW_1) == 0)
+			cacheblk->cache_state |= DIRTY_FALLOW_1;
+		else if ((cacheblk->cache_state & DIRTY_FALLOW_2) == 0) {
+			dmc->cache_sets[index / dmc->assoc].dirty_fallow++;
+			cacheblk->cache_state |= DIRTY_FALLOW_2;
+		}
+	}
+}
+
+void
+flashcache_clear_fallow(struct cache_c *dmc, int index)
+{
+	struct cacheblock *cacheblk = &dmc->cache[index];
+	int set = index / dmc->assoc;
+
+	if (cacheblk->cache_state & FALLOW_DOCLEAN) {
+		if (cacheblk->cache_state & DIRTY_FALLOW_2) {
+			VERIFY(dmc->cache_sets[set].dirty_fallow > 0);
+			dmc->cache_sets[set].dirty_fallow--;
+		}
+		cacheblk->cache_state &= ~FALLOW_DOCLEAN;
+	}
+}
+
+void
+flashcache_io_callback(unsigned long error, void *context)
+{
+	struct kcached_job *job = (struct kcached_job *) context;
+	struct cache_c *dmc = job->dmc;
+	struct bio *bio;
+	unsigned long flags;
+	int index = job->index;
+	struct cacheblock *cacheblk = &dmc->cache[index];
+
+	VERIFY(index != -1);
+	bio = job->bio;
+	VERIFY(bio != NULL);
+	if (unlikely(error)) {
+		error = -EIO;
+		DMERR("flashcache_io_callback: io error %ld block %lu action %d",
+		      error, job->disk.sector, job->action);
+	}
+	job->error = error;
+	switch (job->action) {
+	case READDISK:
+		DPRINTK("flashcache_io_callback: READDISK  %d",
+			index);
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		if (unlikely(sysctl_flashcache_error_inject & READDISK_ERROR)) {
+			job->error = error = -EIO;
+			sysctl_flashcache_error_inject &= ~READDISK_ERROR;
+		}
+		VERIFY(cacheblk->cache_state & DISKREADINPROG);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		if (likely(error == 0)) {
+			/* Kick off the write to the cache */
+			job->action = READFILL;
+			push_io(job);
+			schedule_work(&_kcached_wq);
+			return;
+		} else {
+			dmc->flashcache_errors.disk_read_errors++;
+			flashcache_bio_endio(bio, error, dmc, &job->io_start_time);
+		}
+		break;
+	case READCACHE:
+		DPRINTK("flashcache_io_callback: READCACHE %d",
+			index);
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		if (unlikely(sysctl_flashcache_error_inject & READCACHE_ERROR)) {
+			job->error = error = -EIO;
+			sysctl_flashcache_error_inject &= ~READCACHE_ERROR;
+		}
+		VERIFY(cacheblk->cache_state & CACHEREADINPROG);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		if (unlikely(error))
+			dmc->flashcache_errors.ssd_read_errors++;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		if (likely(error == 0)) {
+			if (flashcache_validate_checksum(job)) {
+				DMERR("flashcache_io_callback: Checksum mismatch at disk offset %lu",
+				      job->disk.sector);
+				error = -EIO;
+			}
+		}
+#endif
+		flashcache_bio_endio(bio, error, dmc, &job->io_start_time);
+		break;
+	case READFILL:
+		DPRINTK("flashcache_io_callback: READFILL %d",
+			index);
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		if (unlikely(sysctl_flashcache_error_inject & READFILL_ERROR)) {
+			job->error = error = -EIO;
+			sysctl_flashcache_error_inject &= ~READFILL_ERROR;
+		}
+		if (unlikely(error))
+			dmc->flashcache_errors.ssd_write_errors++;
+		VERIFY(cacheblk->cache_state & DISKREADINPROG);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		flashcache_bio_endio(bio, error, dmc, &job->io_start_time);
+		break;
+	case WRITECACHE:
+		DPRINTK("flashcache_io_callback: WRITECACHE %d",
+			index);
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		if (unlikely(sysctl_flashcache_error_inject & WRITECACHE_ERROR)) {
+			job->error = error = -EIO;
+			sysctl_flashcache_error_inject &= ~WRITECACHE_ERROR;
+		}
+		VERIFY(cacheblk->cache_state & CACHEWRITEINPROG);
+		if (likely(error == 0)) {
+#ifdef FLASHCACHE_DO_CHECKSUMS
+			dmc->flashcache_stats.checksum_store++;
+			spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+			flashcache_store_checksum(job);
+			/*
+			 * We need to update the metadata on a DIRTY->DIRTY as well
+			 * since we save the checksums.
+			 */
+			flashcache_md_write(job);
+			return;
+#else
+			spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+			/* Only do cache metadata update on a non-DIRTY->DIRTY transition */
+			if ((cacheblk->cache_state & DIRTY) == 0) {
+				flashcache_md_write(job);
+				return;
+			}
+#endif
+		} else {
+			dmc->flashcache_errors.ssd_write_errors++;
+			spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		}
+		flashcache_bio_endio(bio, error, dmc, &job->io_start_time);
+		break;
+	}
+	/*
+	 * The INPROG flag is still set. We cannot turn that off until all the pending requests
+	 * processed. We need to loop the pending requests back to a workqueue. We have the job,
+	 * add it to the pending req queue.
+	 */
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	if (unlikely(error || cacheblk->nr_queued > 0)) {
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		push_pending(job);
+		schedule_work(&_kcached_wq);
+	} else {
+		cacheblk->cache_state &= ~BLOCK_IO_INPROG;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		flashcache_free_cache_job(job);
+		if (atomic_dec_and_test(&dmc->nr_jobs))
+			wake_up(&dmc->destroyq);
+	}
+}
+
+static void
+flashcache_free_pending_jobs(struct cache_c *dmc, struct cacheblock *cacheblk,
+			     int error)
+{
+	struct pending_job *pending_job, *freelist = NULL;
+
+	VERIFY(spin_is_locked(&dmc->cache_spin_lock));
+	freelist = flashcache_deq_pending(dmc, cacheblk - &dmc->cache[0]);
+	while (freelist != NULL) {
+		pending_job = freelist;
+		freelist = pending_job->next;
+		VERIFY(cacheblk->nr_queued > 0);
+		cacheblk->nr_queued--;
+		flashcache_bio_endio(pending_job->bio, error, dmc, NULL);
+		flashcache_free_pending_job(pending_job);
+	}
+	VERIFY(cacheblk->nr_queued == 0);
+}
+
+/*
+ * Common error handling for everything.
+ * 1) If the block isn't dirty, invalidate it.
+ * 2) Error all pending IOs that totally or partly overlap this block.
+ * 3) Free the job.
+ */
+static void
+flashcache_do_pending_error(struct kcached_job *job)
+{
+	struct cache_c *dmc = job->dmc;
+	unsigned long flags;
+	struct cacheblock *cacheblk = &dmc->cache[job->index];
+
+	DMERR("flashcache_do_pending_error: error %d block %lu action %d",
+	      job->error, job->disk.sector, job->action);
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	VERIFY(cacheblk->cache_state & VALID);
+	/* Invalidate block if possible */
+	if ((cacheblk->cache_state & DIRTY) == 0) {
+		dmc->cached_blocks--;
+		dmc->flashcache_stats.pending_inval++;
+		cacheblk->cache_state &= ~VALID;
+		cacheblk->cache_state |= INVALID;
+	}
+	flashcache_free_pending_jobs(dmc, cacheblk, job->error);
+	cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	flashcache_free_cache_job(job);
+	if (atomic_dec_and_test(&dmc->nr_jobs))
+		wake_up(&dmc->destroyq);
+}
+
+static void
+flashcache_do_pending_noerror(struct kcached_job *job)
+{
+	struct cache_c *dmc = job->dmc;
+	int index = job->index;
+	unsigned long flags;
+	struct pending_job *pending_job, *freelist;
+	int queued;
+	struct cacheblock *cacheblk = &dmc->cache[index];
+
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	if (cacheblk->cache_state & DIRTY) {
+		cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+		cacheblk->cache_state |= DISKWRITEINPROG;
+		flashcache_clear_fallow(dmc, index);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		flashcache_dirty_writeback(dmc, index);
+		goto out;
+	}
+	DPRINTK("flashcache_do_pending: Index %d %lx",
+		index, cacheblk->cache_state);
+	VERIFY(cacheblk->cache_state & VALID);
+	dmc->cached_blocks--;
+	dmc->flashcache_stats.pending_inval++;
+	cacheblk->cache_state &= ~VALID;
+	cacheblk->cache_state |= INVALID;
+	while ((freelist = flashcache_deq_pending(dmc, index)) != NULL) {
+		while (freelist != NULL) {
+			VERIFY(!(cacheblk->cache_state & DIRTY));
+			pending_job = freelist;
+			freelist = pending_job->next;
+			VERIFY(cacheblk->nr_queued > 0);
+			cacheblk->nr_queued--;
+			if (pending_job->action == INVALIDATE) {
+				DPRINTK("flashcache_do_pending: INVALIDATE  %llu",
+					next_job->bio->bi_sector);
+				VERIFY(pending_job->bio != NULL);
+				queued = flashcache_inval_blocks(dmc, pending_job->bio);
+				if (queued) {
+					if (unlikely(queued < 0)) {
+						/*
+						 * Memory allocation failure inside inval_blocks.
+						 * Fail this io.
+						 */
+						flashcache_bio_endio(pending_job->bio, -EIO, dmc, NULL);
+					}
+					flashcache_free_pending_job(pending_job);
+					continue;
+				}
+			}
+			spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+			DPRINTK("flashcache_do_pending: Sending down IO %llu",
+				pending_job->bio->bi_sector);
+			/* Start uncached IO */
+			flashcache_start_uncached_io(dmc, pending_job->bio);
+			flashcache_free_pending_job(pending_job);
+			spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		}
+	}
+	VERIFY(cacheblk->nr_queued == 0);
+	cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+out:
+	flashcache_free_cache_job(job);
+	if (atomic_dec_and_test(&dmc->nr_jobs))
+		wake_up(&dmc->destroyq);
+}
+
+void
+flashcache_do_pending(struct kcached_job *job)
+{
+	if (job->error)
+		flashcache_do_pending_error(job);
+	else
+		flashcache_do_pending_noerror(job);
+}
+
+void
+flashcache_do_io(struct kcached_job *job)
+{
+	struct bio *bio = job->bio;
+	int r = 0;
+
+	VERIFY(job->action == READFILL);
+	/* Write to cache device */
+#ifdef FLASHCACHE_DO_CHECKSUMS
+	flashcache_store_checksum(job);
+	job->dmc->flashcache_stats.checksum_store++;
+#endif
+	job->dmc->flashcache_stats.ssd_writes++;
+	r = dm_io_async_bvec(1, &job->cache, WRITE, bio->bi_io_vec + bio->bi_idx,
+			     flashcache_io_callback, job);
+	VERIFY(r == 0);
+	/* In our case, dm_io_async_bvec() must always return 0 */
+}
+
+/*
+ * Map a block from the source device to a block in the cache device.
+ */
+static unsigned long
+hash_block(struct cache_c *dmc, sector_t dbn)
+{
+	unsigned long set_number, value;
+
+	value = (unsigned long)
+		(dbn >> (dmc->block_shift + dmc->consecutive_shift));
+	set_number = value % (dmc->size >> dmc->consecutive_shift);
+	DPRINTK("Hash: %llu(%lu)->%lu", dbn, value, set_number);
+	return set_number;
+}
+
+static void
+find_valid_dbn(struct cache_c *dmc, sector_t dbn,
+	       int start_index, int *index)
+{
+	int i;
+	int end_index = start_index + dmc->assoc;
+
+	for (i = start_index ; i < end_index ; i++) {
+		if (dbn == dmc->cache[i].dbn &&
+		    (dmc->cache[i].cache_state & VALID)) {
+			*index = i;
+			if (sysctl_flashcache_reclaim_policy == FLASHCACHE_LRU &&
+			    ((dmc->cache[i].cache_state & BLOCK_IO_INPROG) == 0))
+				flashcache_reclaim_lru_movetail(dmc, i);
+			/*
+			 * If the block was DIRTY and earmarked for cleaning because it was old, make
+			 * the block young again.
+			 */
+			flashcache_clear_fallow(dmc, i);
+			return;
+		}
+	}
+	*index = -1;
+}
+
+static int
+find_invalid_dbn(struct cache_c *dmc, int start_index)
+{
+	int i;
+	int end_index = start_index + dmc->assoc;
+
+	/* Find INVALID slot that we can reuse */
+	for (i = start_index ; i < end_index ; i++) {
+		if (dmc->cache[i].cache_state == INVALID) {
+			if (sysctl_flashcache_reclaim_policy == FLASHCACHE_LRU)
+				flashcache_reclaim_lru_movetail(dmc, i);
+			VERIFY((dmc->cache[i].cache_state & FALLOW_DOCLEAN) == 0);
+			return i;
+		}
+	}
+	return -1;
+}
+
+/* Search for a slot that we can reclaim */
+static void
+find_reclaim_dbn(struct cache_c *dmc, int start_index, int *index)
+{
+	int set = start_index / dmc->assoc;
+	struct cache_set *cache_set = &dmc->cache_sets[set];
+	struct cacheblock *cacheblk;
+
+	if (sysctl_flashcache_reclaim_policy == FLASHCACHE_FIFO) {
+		int end_index = start_index + dmc->assoc;
+		int slots_searched = 0;
+		int i;
+
+		i = cache_set->set_fifo_next;
+		while (slots_searched < dmc->assoc) {
+			VERIFY(i >= start_index);
+			VERIFY(i < end_index);
+			if (dmc->cache[i].cache_state == VALID) {
+				*index = i;
+				VERIFY((dmc->cache[*index].cache_state & FALLOW_DOCLEAN) == 0);
+				break;
+			}
+			slots_searched++;
+			i++;
+			if (i == end_index)
+				i = start_index;
+		}
+		i++;
+		if (i == end_index)
+			i = start_index;
+		cache_set->set_fifo_next = i;
+	} else { /* flashcache_reclaim_policy == FLASHCACHE_LRU */
+		int lru_rel_index;
+
+		lru_rel_index = cache_set->lru_head;
+		while (lru_rel_index != FLASHCACHE_LRU_NULL) {
+			cacheblk = &dmc->cache[lru_rel_index + start_index];
+			if (cacheblk->cache_state == VALID) {
+				VERIFY((cacheblk - &dmc->cache[0]) ==
+				       (lru_rel_index + start_index));
+				*index = cacheblk - &dmc->cache[0];
+				VERIFY((dmc->cache[*index].cache_state & FALLOW_DOCLEAN) == 0);
+				flashcache_reclaim_lru_movetail(dmc, *index);
+				break;
+			}
+			lru_rel_index = cacheblk->lru_next;
+		}
+	}
+}
+
+/*
+ * dbn is the starting sector, io_size is the number of sectors.
+ */
+static int
+flashcache_lookup(struct cache_c *dmc, struct bio *bio, int *index)
+{
+	sector_t dbn = bio->bi_sector;
+#if DMC_DEBUG
+	int io_size = to_sector(bio->bi_size);
+#endif
+	unsigned long set_number = hash_block(dmc, dbn);
+	int invalid, oldest_clean = -1;
+	int start_index;
+
+	start_index = dmc->assoc * set_number;
+	DPRINTK("Cache lookup : dbn %llu(%lu), set = %d",
+		dbn, io_size, set_number);
+	find_valid_dbn(dmc, dbn, start_index, index);
+	if (*index > 0) {
+		DPRINTK("Cache lookup HIT: Block %llu(%lu): VALID index %d",
+			     dbn, io_size, *index);
+		/* We found the exact range of blocks we are looking for */
+		return VALID;
+	}
+	invalid = find_invalid_dbn(dmc, start_index);
+	if (invalid == -1) {
+		/* We didn't find an invalid entry, search for oldest valid entry */
+		find_reclaim_dbn(dmc, start_index, &oldest_clean);
+	}
+	/*
+	 * Cache miss :
+	 * We can't choose an entry marked INPROG, but choose the oldest
+	 * INVALID or the oldest VALID entry.
+	 */
+	*index = start_index + dmc->assoc;
+	if (invalid != -1) {
+		DPRINTK("Cache lookup MISS (INVALID): dbn %llu(%lu), set = %d, index = %d, start_index = %d",
+			     dbn, io_size, set_number, invalid, start_index);
+		*index = invalid;
+	} else if (oldest_clean != -1) {
+		DPRINTK("Cache lookup MISS (VALID): dbn %llu(%lu), set = %d, index = %d, start_index = %d",
+			     dbn, io_size, set_number, oldest_clean, start_index);
+		*index = oldest_clean;
+	} else {
+		DPRINTK_LITE("Cache read lookup MISS (NOROOM): dbn %llu(%lu), set = %d",
+			dbn, io_size, set_number);
+	}
+	if (*index < (start_index + dmc->assoc))
+		return INVALID;
+	else {
+		dmc->flashcache_stats.noroom++;
+		return -1;
+	}
+}
+
+/*
+ * Cache Metadata Update functions
+ */
+void
+flashcache_md_write_callback(unsigned long error, void *context)
+{
+	struct kcached_job *job = (struct kcached_job *)context;
+
+	if (unlikely(error))
+		job->error = -EIO;
+	else
+		job->error = 0;
+	push_md_complete(job);
+	schedule_work(&_kcached_wq);
+}
+
+static int
+flashcache_alloc_md_sector(struct kcached_job *job)
+{
+	struct page *page = NULL;
+
+	if (likely((sysctl_flashcache_error_inject & MD_ALLOC_SECTOR_ERROR) == 0)) {
+		unsigned long addr;
+
+		/* Get physically consecutive pages */
+		addr = __get_free_pages(GFP_NOIO, get_order(MD_BLOCK_BYTES(job->dmc)));
+		if (addr)
+			page = virt_to_page(addr);
+	} else
+		sysctl_flashcache_error_inject &= ~MD_ALLOC_SECTOR_ERROR;
+	job->md_io_bvec.bv_page = page;
+	if (unlikely(page == NULL)) {
+		job->dmc->flashcache_errors.memory_alloc_errors++;
+		return -ENOMEM;
+	}
+	job->md_io_bvec.bv_len = MD_BLOCK_BYTES(job->dmc);
+	job->md_io_bvec.bv_offset = 0;
+	job->md_block = (struct flash_cacheblock *)page_address(page);
+	return 0;
+}
+
+static void
+flashcache_free_md_sector(struct kcached_job *job)
+{
+	if (job->md_io_bvec.bv_page != NULL)
+		__free_pages(job->md_io_bvec.bv_page, get_order(MD_BLOCK_BYTES(job->dmc)));
+}
+
+void
+flashcache_md_write_kickoff(struct kcached_job *job)
+{
+	struct cache_c *dmc = job->dmc;
+	struct flash_cacheblock *md_block;
+	int md_block_ix;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+	struct io_region where;
+#else
+	struct dm_io_region where;
+#endif
+	int i;
+	struct cache_md_block_head *md_block_head;
+	struct kcached_job *orig_job = job;
+	unsigned long flags;
+
+	if (flashcache_alloc_md_sector(job)) {
+		DMERR("flashcache: %d: Cache metadata write failed, cannot alloc page ! block %lu",
+		      job->action, job->disk.sector);
+		flashcache_md_write_callback(-EIO, job);
+		return;
+	}
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	/*
+	 * Transfer whatever is on the pending queue to the md_io_inprog queue.
+	 */
+	md_block_head = &dmc->md_blocks_buf[INDEX_TO_MD_BLOCK(dmc, job->index)];
+	md_block_head->md_io_inprog = md_block_head->queued_updates;
+	md_block_head->queued_updates = NULL;
+	md_block = job->md_block;
+	md_block_ix = INDEX_TO_MD_BLOCK(dmc, job->index) * MD_SLOTS_PER_BLOCK(dmc);
+	/* First copy out the entire md block */
+	for (i = 0 ;
+	     i < MD_SLOTS_PER_BLOCK(dmc) && md_block_ix < dmc->size ;
+	     i++, md_block_ix++) {
+		md_block[i].dbn = dmc->cache[md_block_ix].dbn;
+#ifdef FLASHCACHE_DO_CHECKSUMS
+		md_block[i].checksum = dmc->cache[md_block_ix].checksum;
+#endif
+		md_block[i].cache_state =
+			dmc->cache[md_block_ix].cache_state & (VALID | INVALID | DIRTY);
+	}
+	/* Then set/clear the DIRTY bit for the "current" index */
+	if (job->action == WRITECACHE) {
+		/* DIRTY the cache block */
+		md_block[INDEX_TO_MD_BLOCK_OFFSET(dmc, job->index)].cache_state =
+			(VALID | DIRTY);
+	} else { /* job->action == WRITEDISK* */
+		/* un-DIRTY the cache block */
+		md_block[INDEX_TO_MD_BLOCK_OFFSET(dmc, job->index)].cache_state = VALID;
+	}
+
+	for (job = md_block_head->md_io_inprog ;
+	     job != NULL ;
+	     job = job->next) {
+		dmc->flashcache_stats.md_write_batch++;
+		if (job->action == WRITECACHE) {
+			/* DIRTY the cache block */
+			md_block[INDEX_TO_MD_BLOCK_OFFSET(dmc, job->index)].cache_state =
+				(VALID | DIRTY);
+		} else { /* job->action == WRITEDISK* */
+			/* un-DIRTY the cache block */
+			md_block[INDEX_TO_MD_BLOCK_OFFSET(dmc, job->index)].cache_state = VALID;
+		}
+	}
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	where.bdev = dmc->cache_dev->bdev;
+	where.count = MD_SECTORS_PER_BLOCK(dmc);
+	where.sector = (1 + INDEX_TO_MD_BLOCK(dmc, orig_job->index)) * MD_SECTORS_PER_BLOCK(dmc);
+	dmc->flashcache_stats.ssd_writes++;
+	dmc->flashcache_stats.md_ssd_writes++;
+	dm_io_async_bvec(1, &where, WRITE,
+			 &orig_job->md_io_bvec,
+			 flashcache_md_write_callback, orig_job);
+}
+
+void
+flashcache_md_write_done(struct kcached_job *job)
+{
+	struct cache_c *dmc = job->dmc;
+	struct cache_md_block_head *md_block_head;
+	int index;
+	unsigned long flags;
+	struct kcached_job *job_list;
+	int error = job->error;
+	struct kcached_job *next;
+	struct cacheblock *cacheblk;
+
+	VERIFY(!in_interrupt());
+	VERIFY(job->action == WRITEDISK || job->action == WRITECACHE ||
+	       job->action == WRITEDISK_SYNC);
+	flashcache_free_md_sector(job);
+	job->md_block = NULL;
+	md_block_head = &dmc->md_blocks_buf[INDEX_TO_MD_BLOCK(dmc, job->index)];
+	job_list = job;
+	job->next = md_block_head->md_io_inprog;
+	md_block_head->md_io_inprog = NULL;
+	for (job = job_list ; job != NULL ; job = next) {
+		next = job->next;
+		job->error = error;
+		index = job->index;
+		cacheblk = &dmc->cache[index];
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		if (job->action == WRITECACHE) {
+			if (unlikely(sysctl_flashcache_error_inject & WRITECACHE_MD_ERROR)) {
+				job->error = -EIO;
+				sysctl_flashcache_error_inject &= ~WRITECACHE_MD_ERROR;
+			}
+			if (likely(job->error == 0)) {
+				if ((cacheblk->cache_state & DIRTY) == 0) {
+					dmc->cache_sets[index / dmc->assoc].nr_dirty++;
+					dmc->nr_dirty++;
+				}
+				dmc->flashcache_stats.md_write_dirty++;
+				cacheblk->cache_state |= DIRTY;
+			} else
+				dmc->flashcache_errors.ssd_write_errors++;
+			flashcache_bio_endio(job->bio, job->error, dmc, &job->io_start_time);
+			if (job->error || cacheblk->nr_queued > 0) {
+				if (job->error) {
+					DMERR("flashcache: WRITE: Cache metadata write failed ! error %d block %lu",
+					      job->error, cacheblk->dbn);
+				}
+				spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+				flashcache_do_pending(job);
+			} else {
+				cacheblk->cache_state &= ~BLOCK_IO_INPROG;
+				spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+				flashcache_free_cache_job(job);
+				if (atomic_dec_and_test(&dmc->nr_jobs))
+					wake_up(&dmc->destroyq);
+			}
+		} else {
+			int action = job->action;
+
+			if (unlikely(sysctl_flashcache_error_inject & WRITEDISK_MD_ERROR)) {
+				job->error = -EIO;
+				sysctl_flashcache_error_inject &= ~WRITEDISK_MD_ERROR;
+			}
+			/*
+			 * If we have an error on a WRITEDISK*, no choice but to preserve the
+			 * dirty block in cache. Fail any IOs for this block that occurred while
+			 * the block was being cleaned.
+			 */
+			if (likely(job->error == 0)) {
+				dmc->flashcache_stats.md_write_clean++;
+				cacheblk->cache_state &= ~DIRTY;
+				VERIFY(dmc->cache_sets[index / dmc->assoc].nr_dirty > 0);
+				VERIFY(dmc->nr_dirty > 0);
+				dmc->cache_sets[index / dmc->assoc].nr_dirty--;
+				dmc->nr_dirty--;
+			} else
+				dmc->flashcache_errors.ssd_write_errors++;
+			VERIFY(dmc->cache_sets[index / dmc->assoc].clean_inprog > 0);
+			VERIFY(dmc->clean_inprog > 0);
+			dmc->cache_sets[index / dmc->assoc].clean_inprog--;
+			dmc->clean_inprog--;
+			if (job->error || cacheblk->nr_queued > 0) {
+				if (job->error) {
+					DMERR("flashcache: CLEAN: Cache metadata write failed ! error %d block %lu",
+					      job->error, cacheblk->dbn);
+				}
+				spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+				flashcache_do_pending(job);
+				/* Kick off more cleanings */
+				if (action == WRITEDISK)
+					flashcache_clean_set(dmc, index / dmc->assoc);
+				else
+					flashcache_sync_blocks(dmc);
+			} else {
+				cacheblk->cache_state &= ~BLOCK_IO_INPROG;
+				spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+				flashcache_free_cache_job(job);
+				if (atomic_dec_and_test(&dmc->nr_jobs))
+					wake_up(&dmc->destroyq);
+				/* Kick off more cleanings */
+				if (action == WRITEDISK)
+					flashcache_clean_set(dmc, index / dmc->assoc);
+				else
+					flashcache_sync_blocks(dmc);
+			}
+			dmc->flashcache_stats.cleanings++;
+			if (action == WRITEDISK_SYNC)
+				flashcache_update_sync_progress(dmc);
+		}
+	}
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	if (md_block_head->queued_updates != NULL) {
+		/* peel off the first job from the pending queue and kick that off */
+		job = md_block_head->queued_updates;
+		md_block_head->queued_updates = job->next;
+		job->next = NULL;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		VERIFY(job->action == WRITEDISK || job->action == WRITECACHE ||
+		       job->action == WRITEDISK_SYNC);
+		flashcache_md_write_kickoff(job);
+	} else {
+		md_block_head->nr_in_prog = 0;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	}
+}
+
+/*
+ * Kick off a cache metadata update (called from workqueue).
+ * Cache metadata update IOs to a given metadata sector are serialized using the
+ * nr_in_prog bit in the md sector bufhead.
+ * If a metadata IO is already in progress, we queue up incoming metadata updates
+ * on the pending_jobs list of the md sector bufhead. When kicking off an IO, we
+ * cluster all these pending updates and do all of them as 1 flash write (that
+ * logic is in md_write_kickoff), where it switches out the entire pending_jobs
+ * list and does all of those updates as 1 ssd write.
+ */
+void
+flashcache_md_write(struct kcached_job *job)
+{
+	struct cache_c *dmc = job->dmc;
+	struct cache_md_block_head *md_block_head;
+	unsigned long flags;
+
+	VERIFY(job->action == WRITEDISK || job->action == WRITECACHE ||
+	       job->action == WRITEDISK_SYNC);
+	md_block_head = &dmc->md_blocks_buf[INDEX_TO_MD_BLOCK(dmc, job->index)];
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	/* If a write is in progress for this metadata sector, queue this update up */
+	if (md_block_head->nr_in_prog != 0) {
+		struct kcached_job **nodepp;
+
+		/* A MD update is already in progress, queue this one up for later */
+		nodepp = &md_block_head->queued_updates;
+		while (*nodepp != NULL)
+			nodepp = &((*nodepp)->next);
+		job->next = NULL;
+		*nodepp = job;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	} else {
+		md_block_head->nr_in_prog = 1;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		/*
+		 * Always push to a worker thread. If the driver has
+		 * a completion thread, we could end up deadlocking even
+		 * if the context would be safe enough to write from.
+		 * This could be executed from the context of an IO
+		 * completion thread. Kicking off the write from that
+		 * context could result in the IO completion thread
+		 * blocking (eg on memory allocation). That can easily
+		 * deadlock.
+		 */
+		push_md_io(job);
+		schedule_work(&_kcached_wq);
+	}
+}
+
+static void
+flashcache_kcopyd_callback(int read_err, unsigned int write_err, void *context)
+{
+	struct kcached_job *job = (struct kcached_job *)context;
+	struct cache_c *dmc = job->dmc;
+	int index = job->index;
+	unsigned long flags;
+
+	VERIFY(!in_interrupt());
+	DPRINTK("kcopyd_callback: Index %d", index);
+	VERIFY(job->bio == NULL);
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	VERIFY(dmc->cache[index].cache_state & (DISKWRITEINPROG | VALID | DIRTY));
+	if (unlikely(sysctl_flashcache_error_inject & KCOPYD_CALLBACK_ERROR)) {
+		read_err = -EIO;
+		sysctl_flashcache_error_inject &= ~KCOPYD_CALLBACK_ERROR;
+	}
+	if (likely(read_err == 0 && write_err == 0)) {
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		flashcache_md_write(job);
+	} else {
+		if (read_err)
+			read_err = -EIO;
+		if (write_err)
+			write_err = -EIO;
+		/* Disk write failed. We can not purge this block from flash */
+		DMERR("flashcache: Disk writeback failed ! read error %d write error %d block %lu",
+		      -read_err, -write_err, job->disk.sector);
+		VERIFY(dmc->cache_sets[index / dmc->assoc].clean_inprog > 0);
+		VERIFY(dmc->clean_inprog > 0);
+		dmc->cache_sets[index / dmc->assoc].clean_inprog--;
+		dmc->clean_inprog--;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		/* Set the error in the job and let do_pending() handle the error */
+		if (read_err) {
+			dmc->flashcache_errors.ssd_read_errors++;
+			job->error = read_err;
+		} else {
+			dmc->flashcache_errors.disk_write_errors++;
+			job->error = write_err;
+		}
+		flashcache_do_pending(job);
+		flashcache_clean_set(dmc, index / dmc->assoc); /* Kick off more cleanings */
+		dmc->flashcache_stats.cleanings++;
+	}
+}
+
+static void
+flashcache_dirty_writeback(struct cache_c *dmc, int index)
+{
+	struct kcached_job *job;
+	unsigned long flags;
+	struct cacheblock *cacheblk = &dmc->cache[index];
+	int device_removal = 0;
+
+	DPRINTK("flashcache_dirty_writeback: Index %d", index);
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	VERIFY((cacheblk->cache_state & BLOCK_IO_INPROG) == DISKWRITEINPROG);
+	VERIFY(cacheblk->cache_state & DIRTY);
+	dmc->cache_sets[index / dmc->assoc].clean_inprog++;
+	dmc->clean_inprog++;
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	job = new_kcached_job(dmc, NULL, index);
+	if (unlikely(sysctl_flashcache_error_inject & DIRTY_WRITEBACK_JOB_ALLOC_FAIL)) {
+		if (job)
+			flashcache_free_cache_job(job);
+		job = NULL;
+		sysctl_flashcache_error_inject &= ~DIRTY_WRITEBACK_JOB_ALLOC_FAIL;
+	}
+	/*
+	 * If the device is being removed, do not kick off any more cleanings.
+	 */
+	if (unlikely(atomic_read(&dmc->remove_in_prog))) {
+		DMERR("flashcache: Dirty Writeback (for set cleaning) aborted for device removal, block %lu",
+		      cacheblk->dbn);
+		if (job)
+			flashcache_free_cache_job(job);
+		job = NULL;
+		device_removal = 1;
+	}
+	if (unlikely(job == NULL)) {
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		dmc->cache_sets[index / dmc->assoc].clean_inprog--;
+		dmc->clean_inprog--;
+		flashcache_free_pending_jobs(dmc, cacheblk, -EIO);
+		cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		if (device_removal == 0)
+			DMERR("flashcache: Dirty Writeback (for set cleaning) failed ! Can't allocate memory, block %lu",
+			      cacheblk->dbn);
+	} else {
+		job->bio = NULL;
+		job->action = WRITEDISK;
+		atomic_inc(&dmc->nr_jobs);
+		dmc->flashcache_stats.ssd_reads++;
+		dmc->flashcache_stats.disk_writes++;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+		kcopyd_copy(dmc->kcp_client, &job->cache, 1, &job->disk, 0,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,25)
+			    flashcache_kcopyd_callback,
+#else
+			    (kcopyd_notify_fn) flashcache_kcopyd_callback,
+#endif
+			    job);
+#else
+		dm_kcopyd_copy(dmc->kcp_client, &job->cache, 1, &job->disk, 0,
+			       (dm_kcopyd_notify_fn) flashcache_kcopyd_callback,
+			       (void *)job);
+#endif
+	}
+}
+
+/*
+ * This function encodes the background disk cleaning logic.
+ * Background disk cleaning is triggered for 2 reasons.
+ A) Dirty blocks are lying fallow in the set, making them good
+    candidates for being cleaned.
+ B) This set has dirty blocks over the configured threshold
+    for a set.
+ * (A) takes precedence over (B). Fallow dirty blocks are cleaned
+ * first.
+ * The cleaning of disk blocks is subject to the write limits per
+ * set and across the cache, which this function enforces.
+ *
+ * 1) Select the n blocks that we want to clean (choosing whatever policy),
+ *    sort them.
+ * 2) Then sweep the entire set looking for other DIRTY blocks that can be
+ *    tacked onto any of these blocks to form larger contigous writes.
+ *    The idea here is that if you are going to do a write anyway, then we
+ *    might as well opportunistically write out any contigous blocks for
+ *    free.
+ */
+
+/* Are we under the limits for disk cleaning ? */
+static inline int
+flashcache_can_clean(struct cache_c *dmc,
+		     struct cache_set *cache_set,
+		     int nr_writes)
+{
+	return ((cache_set->clean_inprog + nr_writes) < dmc->max_clean_ios_set &&
+		(nr_writes + dmc->clean_inprog) < dmc->max_clean_ios_total);
+}
+
+extern int sysctl_fallow_delay;
+
+void
+flashcache_clean_set(struct cache_c *dmc, int set)
+{
+	unsigned long flags;
+	int threshold_clean = 0;
+	struct dbn_index_pair *writes_list;
+	int nr_writes = 0, i;
+	int start_index = set * dmc->assoc;
+	int end_index = start_index + dmc->assoc;
+	struct cache_set *cache_set = &dmc->cache_sets[set];
+	struct cacheblock *cacheblk;
+	int do_delayed_clean = 0;
+
+	/*
+	 * If a removal of this device is in progress, don't kick off
+	 * any more cleanings. This isn't sufficient though. We still need to
+	 * stop cleanings inside flashcache_dirty_writeback() because we could
+	 * have started a device remove after tested this here.
+	 */
+	if (atomic_read(&dmc->remove_in_prog))
+		return;
+	writes_list = kmalloc(dmc->assoc * sizeof(struct dbn_index_pair), GFP_NOIO);
+	if (unlikely(sysctl_flashcache_error_inject & WRITES_LIST_ALLOC_FAIL)) {
+		if (writes_list)
+			kfree(writes_list);
+		writes_list = NULL;
+		sysctl_flashcache_error_inject &= ~WRITES_LIST_ALLOC_FAIL;
+	}
+	if (writes_list == NULL) {
+		dmc->flashcache_errors.memory_alloc_errors++;
+		return;
+	}
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	/*
+	 * Before we try to clean any blocks, check the last time the fallow block
+	 * detection was done. If it has been more than "fallow_delay" seconds, make
+	 * a sweep through the set to detect (mark) fallow blocks.
+	 */
+	if (sysctl_fallow_delay && time_after(jiffies, cache_set->fallow_tstamp)) {
+		for (i = start_index ; i < end_index ; i++)
+			flashcache_detect_fallow(dmc, i);
+		cache_set->fallow_tstamp = jiffies + sysctl_fallow_delay * HZ;
+	}
+	/* If there are any dirty fallow blocks, clean them first */
+	for (i = start_index ;
+	     (sysctl_fallow_delay > 0 &&
+	      cache_set->dirty_fallow > 0 &&
+	      time_after(jiffies, cache_set->fallow_next_cleaning) &&
+	      i < end_index) ;
+	     i++) {
+		cacheblk = &dmc->cache[i];
+		if (!(cacheblk->cache_state & DIRTY_FALLOW_2))
+			continue;
+		if (!flashcache_can_clean(dmc, cache_set, nr_writes)) {
+			/*
+			 * There are fallow blocks that need cleaning, but we
+			 * can't clean them this pass, schedule delayed cleaning
+			 * later.
+			 */
+			do_delayed_clean = 1;
+			goto out;
+		}
+		VERIFY(cacheblk->cache_state & DIRTY);
+		VERIFY((cacheblk->cache_state & BLOCK_IO_INPROG) == 0);
+		cacheblk->cache_state |= DISKWRITEINPROG;
+		flashcache_clear_fallow(dmc, i);
+		writes_list[nr_writes].dbn = cacheblk->dbn;
+		writes_list[nr_writes].index = i;
+		dmc->flashcache_stats.fallow_cleanings++;
+		nr_writes++;
+	}
+	if (nr_writes > 0)
+		cache_set->fallow_next_cleaning = jiffies + HZ / sysctl_fallow_clean_speed;
+	if (cache_set->nr_dirty < dmc->dirty_thresh_set ||
+	    !flashcache_can_clean(dmc, cache_set, nr_writes))
+		goto out;
+	/*
+	 * We picked up all the dirty fallow blocks we can. We can still clean more to
+	 * remain under the dirty threshold. Clean some more blocks.
+	 */
+	threshold_clean = cache_set->nr_dirty - dmc->dirty_thresh_set;
+	if (sysctl_flashcache_reclaim_policy == FLASHCACHE_FIFO) {
+		int scanned;
+
+		scanned = 0;
+		i = cache_set->set_clean_next;
+		DPRINTK("flashcache_clean_set: Set %d", set);
+		while (scanned < dmc->assoc &&
+		       flashcache_can_clean(dmc, cache_set, nr_writes) &&
+		       nr_writes < threshold_clean) {
+			cacheblk = &dmc->cache[i];
+			if ((cacheblk->cache_state & (DIRTY | BLOCK_IO_INPROG)) == DIRTY) {
+				cacheblk->cache_state |= DISKWRITEINPROG;
+				flashcache_clear_fallow(dmc, i);
+				writes_list[nr_writes].dbn = cacheblk->dbn;
+				writes_list[nr_writes].index = i;
+				nr_writes++;
+			}
+			scanned++;
+			i++;
+			if (i == end_index)
+				i = start_index;
+		}
+		cache_set->set_clean_next = i;
+	} else { /* flashcache_reclaim_policy == FLASHCACHE_LRU */
+		int lru_rel_index;
+
+		lru_rel_index = cache_set->lru_head;
+		while (lru_rel_index != FLASHCACHE_LRU_NULL &&
+		       flashcache_can_clean(dmc, cache_set, nr_writes) &&
+		       nr_writes < threshold_clean) {
+			cacheblk = &dmc->cache[lru_rel_index + start_index];
+			if ((cacheblk->cache_state & (DIRTY | BLOCK_IO_INPROG)) == DIRTY) {
+				cacheblk->cache_state |= DISKWRITEINPROG;
+				flashcache_clear_fallow(dmc, i);
+				writes_list[nr_writes].dbn = cacheblk->dbn;
+				writes_list[nr_writes].index = cacheblk - &dmc->cache[0];
+				nr_writes++;
+			}
+			lru_rel_index = cacheblk->lru_next;
+		}
+	}
+out:
+	if (nr_writes > 0) {
+		flashcache_merge_writes(dmc, writes_list, &nr_writes, set);
+		dmc->flashcache_stats.clean_set_ios += nr_writes;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		for (i = 0 ; i < nr_writes ; i++)
+			flashcache_dirty_writeback(dmc, writes_list[i].index);
+	} else {
+		if (cache_set->nr_dirty > dmc->dirty_thresh_set)
+			do_delayed_clean = 1;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		if (do_delayed_clean)
+			schedule_delayed_work(&dmc->delayed_clean, 1*HZ);
+	}
+	kfree(writes_list);
+}
+
+static void
+flashcache_read_hit(struct cache_c *dmc, struct bio* bio, int index)
+{
+	struct cacheblock *cacheblk;
+	struct pending_job *pjob;
+
+	cacheblk = &dmc->cache[index];
+	if (!(cacheblk->cache_state & BLOCK_IO_INPROG) && (cacheblk->nr_queued == 0)) {
+		struct kcached_job *job;
+
+		cacheblk->cache_state |= CACHEREADINPROG;
+		dmc->flashcache_stats.read_hits++;
+		spin_unlock_irq(&dmc->cache_spin_lock);
+		DPRINTK("Cache read: Block %llu(%lu), index = %d:%s",
+			bio->bi_sector, bio->bi_size, index, "CACHE HIT");
+		job = new_kcached_job(dmc, bio, index);
+		if (unlikely(sysctl_flashcache_error_inject & READ_HIT_JOB_ALLOC_FAIL)) {
+			if (job)
+				flashcache_free_cache_job(job);
+			job = NULL;
+			sysctl_flashcache_error_inject &= ~READ_HIT_JOB_ALLOC_FAIL;
+		}
+		if (unlikely(job == NULL)) {
+			/*
+			 * We have a read hit, and can't allocate a job.
+			 * Since we dropped the spinlock, we have to drain any
+			 * pending jobs.
+			 */
+			DMERR("flashcache: Read (hit) failed ! Can't allocate memory for cache IO, block %lu",
+			      cacheblk->dbn);
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+			spin_lock_irq(&dmc->cache_spin_lock);
+			flashcache_free_pending_jobs(dmc, cacheblk, -EIO);
+			cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+			spin_unlock_irq(&dmc->cache_spin_lock);
+		} else {
+			job->action = READCACHE; /* Fetch data from cache */
+			atomic_inc(&dmc->nr_jobs);
+			dmc->flashcache_stats.ssd_reads++;
+			dm_io_async_bvec(1, &job->cache, READ,
+					 bio->bi_io_vec + bio->bi_idx,
+					 flashcache_io_callback, job);
+		}
+	} else {
+		pjob = flashcache_alloc_pending_job(dmc);
+		if (unlikely(sysctl_flashcache_error_inject & READ_HIT_PENDING_JOB_ALLOC_FAIL)) {
+			if (pjob) {
+				flashcache_free_pending_job(pjob);
+				pjob = NULL;
+			}
+			sysctl_flashcache_error_inject &= ~READ_HIT_PENDING_JOB_ALLOC_FAIL;
+		}
+		if (pjob == NULL)
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		else
+			flashcache_enq_pending(dmc, bio, index, READCACHE, pjob);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+	}
+}
+
+static void
+flashcache_read_miss(struct cache_c *dmc, struct bio* bio,
+		     int index)
+{
+	struct kcached_job *job;
+	struct cacheblock *cacheblk = &dmc->cache[index];
+
+	job = new_kcached_job(dmc, bio, index);
+	if (unlikely(sysctl_flashcache_error_inject & READ_MISS_JOB_ALLOC_FAIL)) {
+		if (job)
+			flashcache_free_cache_job(job);
+		job = NULL;
+		sysctl_flashcache_error_inject &= ~READ_MISS_JOB_ALLOC_FAIL;
+	}
+	if (unlikely(job == NULL)) {
+		/*
+		 * We have a read miss, and can't allocate a job.
+		 * Since we dropped the spinlock, we have to drain any
+		 * pending jobs.
+		 */
+		DMERR("flashcache: Read (miss) failed ! Can't allocate memory for cache IO, block %lu",
+		      cacheblk->dbn);
+		flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		spin_lock_irq(&dmc->cache_spin_lock);
+		dmc->cached_blocks--;
+		cacheblk->cache_state &= ~VALID;
+		cacheblk->cache_state |= INVALID;
+		flashcache_free_pending_jobs(dmc, cacheblk, -EIO);
+		cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+	} else {
+		job->action = READDISK; /* Fetch data from the source device */
+		atomic_inc(&dmc->nr_jobs);
+		dmc->flashcache_stats.disk_reads++;
+		dm_io_async_bvec(1, &job->disk, READ,
+				 bio->bi_io_vec + bio->bi_idx,
+				 flashcache_io_callback, job);
+		flashcache_clean_set(dmc, index / dmc->assoc);
+	}
+}
+
+static void
+flashcache_read(struct cache_c *dmc, struct bio *bio)
+{
+	int index;
+	int res;
+	struct cacheblock *cacheblk;
+	int queued;
+
+	DPRINTK("Got a %s for %llu  %u bytes)",
+	        (bio_rw(bio) == READ ? "READ":"READA"),
+		bio->bi_sector, bio->bi_size);
+
+	spin_lock_irq(&dmc->cache_spin_lock);
+	res = flashcache_lookup(dmc, bio, &index);
+	/*
+	 * Handle Cache Hit case first.
+	 * We need to handle 2 cases, BUSY and !BUSY. If BUSY, we enqueue the
+	 * bio for later.
+	 */
+	if (res > 0) {
+		cacheblk = &dmc->cache[index];
+		if ((cacheblk->cache_state & VALID) &&
+		    (cacheblk->dbn == bio->bi_sector)) {
+			flashcache_read_hit(dmc, bio, index);
+			return;
+		}
+	}
+	/*
+	 * In all cases except for a cache hit (and VALID), test for potential
+	 * invalidations that we need to do.
+	 */
+	queued = flashcache_inval_blocks(dmc, bio);
+	if (queued) {
+		if (unlikely(queued < 0))
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+		return;
+	}
+	if (res == -1 || flashcache_uncacheable(dmc)) {
+		/* No room or non-cacheable */
+		spin_unlock_irq(&dmc->cache_spin_lock);
+		DPRINTK("Cache read: Block %llu(%lu):%s",
+			bio->bi_sector, bio->bi_size, "CACHE MISS & NO ROOM");
+		if (res == -1)
+			flashcache_clean_set(dmc, hash_block(dmc, bio->bi_sector));
+		/* Start uncached IO */
+		flashcache_start_uncached_io(dmc, bio);
+		return;
+	}
+	/*
+	 * (res == INVALID) Cache Miss
+	 * And we found cache blocks to replace
+	 * Claim the cache blocks before giving up the spinlock
+	 */
+	if (dmc->cache[index].cache_state & VALID)
+		dmc->flashcache_stats.replace++;
+	else
+		dmc->cached_blocks++;
+	dmc->cache[index].cache_state = VALID | DISKREADINPROG;
+	dmc->cache[index].dbn = bio->bi_sector;
+	spin_unlock_irq(&dmc->cache_spin_lock);
+
+	DPRINTK("Cache read: Block %llu(%lu), index = %d:%s",
+		bio->bi_sector, bio->bi_size, index, "CACHE MISS & REPLACE");
+	flashcache_read_miss(dmc, bio, index);
+}
+
+/*
+ * Invalidate any colliding blocks if they are !BUSY and !DIRTY. If the colliding
+ * block is DIRTY, we need to kick off a write. In both cases, we need to wait
+ * until the underlying IO is finished, and then proceed with the invalidation.
+ */
+static int
+flashcache_inval_block_set(struct cache_c *dmc, int set, struct bio *bio, int rw,
+			   struct pending_job *pjob)
+{
+	sector_t io_start = bio->bi_sector;
+	sector_t io_end = bio->bi_sector + (to_sector(bio->bi_size) - 1);
+	int start_index, end_index, i;
+	struct cacheblock *cacheblk;
+
+	start_index = dmc->assoc * set;
+	end_index = start_index + dmc->assoc;
+	for (i = start_index ; i < end_index ; i++) {
+		sector_t start_dbn = dmc->cache[i].dbn;
+		sector_t end_dbn = start_dbn + dmc->block_size;
+
+		cacheblk = &dmc->cache[i];
+		if (cacheblk->cache_state & INVALID)
+			continue;
+		if ((io_start >= start_dbn && io_start < end_dbn) ||
+		    (io_end >= start_dbn && io_end < end_dbn)) {
+			/* We have a match */
+			if (rw == WRITE)
+				dmc->flashcache_stats.wr_invalidates++;
+			else
+				dmc->flashcache_stats.rd_invalidates++;
+			if (!(cacheblk->cache_state & (BLOCK_IO_INPROG | DIRTY)) &&
+			    (cacheblk->nr_queued == 0)) {
+				dmc->cached_blocks--;
+				DPRINTK("Cache invalidate (!BUSY): Block %llu %lx",
+					start_dbn, cacheblk->cache_state);
+				cacheblk->cache_state = INVALID;
+				continue;
+			}
+			/*
+			 * The conflicting block has either IO in progress or is
+			 * Dirty. In all cases, we need to add ourselves to the
+			 * pending queue. Then if the block is dirty, we kick off
+			 * an IO to clean the block.
+			 * Note that if the block is dirty and IO is in progress
+			 * on it, the do_pending handler will clean the block
+			 * and then process the pending queue.
+			 */
+			flashcache_enq_pending(dmc, bio, i, INVALIDATE, pjob);
+			if ((cacheblk->cache_state & (DIRTY | BLOCK_IO_INPROG)) == DIRTY) {
+				/*
+				 * Kick off block write.
+				 * We can't kick off the write under the spinlock.
+				 * Instead, we mark the slot DISKWRITEINPROG, drop
+				 * the spinlock and kick off the write. A block marked
+				 * DISKWRITEINPROG cannot change underneath us.
+				 * to enqueue ourselves onto it's pending queue.
+				 *
+				 * XXX - The dropping of the lock here can be avoided if
+				 * we punt the cleaning of the block to the worker thread,
+				 * at the cost of a context switch.
+				 */
+				cacheblk->cache_state |= DISKWRITEINPROG;
+				flashcache_clear_fallow(dmc, i);
+				spin_unlock_irq(&dmc->cache_spin_lock);
+				flashcache_dirty_writeback(dmc, i); /* Must inc nr_jobs */
+				spin_lock_irq(&dmc->cache_spin_lock);
+			}
+			return 1;
+		}
+	}
+	return 0;
+}
+
+/*
+ * Since md will break up IO into blocksize pieces, we only really need to check
+ * the start set and the end set for overlaps.
+ */
+static int
+flashcache_inval_blocks(struct cache_c *dmc, struct bio *bio)
+{
+	sector_t io_start = bio->bi_sector;
+	sector_t io_end = bio->bi_sector + (to_sector(bio->bi_size) - 1);
+	int start_set, end_set;
+	int queued;
+	struct pending_job *pjob1, *pjob2;
+
+	pjob1 = flashcache_alloc_pending_job(dmc);
+	if (unlikely(sysctl_flashcache_error_inject & INVAL_PENDING_JOB_ALLOC_FAIL)) {
+		if (pjob1) {
+			flashcache_free_pending_job(pjob1);
+			pjob1 = NULL;
+		}
+		sysctl_flashcache_error_inject &= ~INVAL_PENDING_JOB_ALLOC_FAIL;
+	}
+	if (pjob1 == NULL) {
+		queued = -ENOMEM;
+		goto out;
+	}
+	pjob2 = flashcache_alloc_pending_job(dmc);
+	if (pjob2 == NULL) {
+		flashcache_free_pending_job(pjob1);
+		queued = -ENOMEM;
+		goto out;
+	}
+	start_set = hash_block(dmc, io_start);
+	end_set = hash_block(dmc, io_end);
+	queued = flashcache_inval_block_set(dmc, start_set, bio,
+					    bio_data_dir(bio), pjob1);
+	if (queued) {
+		flashcache_free_pending_job(pjob2);
+		goto out;
+	} else
+		flashcache_free_pending_job(pjob1);
+	if (start_set != end_set) {
+		queued = flashcache_inval_block_set(dmc, end_set,
+						    bio, bio_data_dir(bio), pjob2);
+		if (!queued)
+			flashcache_free_pending_job(pjob2);
+	} else
+		flashcache_free_pending_job(pjob2);
+out:
+	return queued;
+}
+
+static void
+flashcache_write_miss(struct cache_c *dmc, struct bio *bio, int index)
+{
+	struct cacheblock *cacheblk;
+	struct kcached_job *job;
+	int queued;
+
+	cacheblk = &dmc->cache[index];
+	queued = flashcache_inval_blocks(dmc, bio);
+	if (queued) {
+		if (unlikely(queued < 0))
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+		return;
+	}
+	if (cacheblk->cache_state & VALID)
+		dmc->flashcache_stats.wr_replace++;
+	else
+		dmc->cached_blocks++;
+	cacheblk->cache_state = VALID | CACHEWRITEINPROG;
+	cacheblk->dbn = bio->bi_sector;
+	spin_unlock_irq(&dmc->cache_spin_lock);
+	job = new_kcached_job(dmc, bio, index);
+	if (unlikely(sysctl_flashcache_error_inject & WRITE_MISS_JOB_ALLOC_FAIL)) {
+		if (job)
+			flashcache_free_cache_job(job);
+		job = NULL;
+		sysctl_flashcache_error_inject &= ~WRITE_MISS_JOB_ALLOC_FAIL;
+	}
+	if (unlikely(job == NULL)) {
+		/*
+		 * We have a write miss, and can't allocate a job.
+		 * Since we dropped the spinlock, we have to drain any
+		 * pending jobs.
+		 */
+		DMERR("flashcache: Write (miss) failed ! Can't allocate memory for cache IO, block %lu",
+		      cacheblk->dbn);
+		flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		spin_lock_irq(&dmc->cache_spin_lock);
+		dmc->cached_blocks--;
+		cacheblk->cache_state &= ~VALID;
+		cacheblk->cache_state |= INVALID;
+		flashcache_free_pending_jobs(dmc, cacheblk, -EIO);
+		cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+	} else {
+		job->action = WRITECACHE;
+		atomic_inc(&dmc->nr_jobs);
+		dmc->flashcache_stats.ssd_writes++;
+		dm_io_async_bvec(1, &job->cache, WRITE,
+				 bio->bi_io_vec + bio->bi_idx,
+				 flashcache_io_callback, job);
+		flashcache_clean_set(dmc, index / dmc->assoc);
+	}
+}
+
+static void
+flashcache_write_hit(struct cache_c *dmc, struct bio *bio, int index)
+{
+	struct cacheblock *cacheblk;
+	struct pending_job *pjob;
+	struct kcached_job *job;
+
+	cacheblk = &dmc->cache[index];
+	if (!(cacheblk->cache_state & BLOCK_IO_INPROG) && (cacheblk->nr_queued == 0)) {
+		if (cacheblk->cache_state & DIRTY)
+			dmc->flashcache_stats.dirty_write_hits++;
+		dmc->flashcache_stats.write_hits++;
+		cacheblk->cache_state |= CACHEWRITEINPROG;
+		spin_unlock_irq(&dmc->cache_spin_lock);
+		job = new_kcached_job(dmc, bio, index);
+		if (unlikely(sysctl_flashcache_error_inject & WRITE_HIT_JOB_ALLOC_FAIL)) {
+			if (job)
+				flashcache_free_cache_job(job);
+			job = NULL;
+			sysctl_flashcache_error_inject &= ~WRITE_HIT_JOB_ALLOC_FAIL;
+		}
+		if (unlikely(job == NULL)) {
+			/*
+			 * We have a write hit, and can't allocate a job.
+			 * Since we dropped the spinlock, we have to drain any
+			 * pending jobs.
+			 */
+			DMERR("flashcache: Write (hit) failed ! Can't allocate memory for cache IO, block %lu",
+			      cacheblk->dbn);
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+			spin_lock_irq(&dmc->cache_spin_lock);
+			flashcache_free_pending_jobs(dmc, cacheblk, -EIO);
+			cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+			spin_unlock_irq(&dmc->cache_spin_lock);
+		} else {
+			job->action = WRITECACHE; /* Write data to the source device */
+			DPRINTK("Queue job for %llu", bio->bi_sector);
+			atomic_inc(&dmc->nr_jobs);
+			dmc->flashcache_stats.ssd_writes++;
+			dm_io_async_bvec(1, &job->cache, WRITE,
+					 bio->bi_io_vec + bio->bi_idx,
+					 flashcache_io_callback, job);
+			flashcache_clean_set(dmc, index / dmc->assoc);
+		}
+	} else {
+		pjob = flashcache_alloc_pending_job(dmc);
+		if (unlikely(sysctl_flashcache_error_inject & WRITE_HIT_PENDING_JOB_ALLOC_FAIL)) {
+			if (pjob) {
+				flashcache_free_pending_job(pjob);
+				pjob = NULL;
+			}
+			sysctl_flashcache_error_inject &= ~WRITE_HIT_PENDING_JOB_ALLOC_FAIL;
+		}
+		if (unlikely(pjob == NULL))
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		else
+			flashcache_enq_pending(dmc, bio, index, WRITECACHE, pjob);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+	}
+}
+
+static void
+flashcache_write(struct cache_c *dmc, struct bio *bio)
+{
+	int index;
+	int res;
+	struct cacheblock *cacheblk;
+	int queued;
+
+	spin_lock_irq(&dmc->cache_spin_lock);
+	res = flashcache_lookup(dmc, bio, &index);
+	/*
+	 * If cache hit and !BUSY, simply redirty page.
+	 * If cache hit and BUSY, must wait for IO in prog to complete.
+	 * If cache miss and found a block to recycle, we need to
+	 * (a) invalidate any partial hits,
+	 * (b) write to cache.
+	 */
+	if (res != -1) {
+		/* Cache Hit */
+		cacheblk = &dmc->cache[index];
+		if ((cacheblk->cache_state & VALID) &&
+		    (cacheblk->dbn == bio->bi_sector)) {
+			/* Cache Hit */
+			flashcache_write_hit(dmc, bio, index);
+		} else {
+			/* Cache Miss, found block to recycle */
+			flashcache_write_miss(dmc, bio, index);
+		}
+		return;
+	}
+	/*
+	 * No room in the set. We cannot write to the cache and have to
+	 * send the request to disk. Before we do that, we must check
+	 * for potential invalidations !
+	 */
+	queued = flashcache_inval_blocks(dmc, bio);
+	spin_unlock_irq(&dmc->cache_spin_lock);
+	if (queued) {
+		if (unlikely(queued < 0))
+			flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		return;
+	}
+	/* Start uncached IO */
+	flashcache_start_uncached_io(dmc, bio);
+	flashcache_clean_set(dmc, hash_block(dmc, bio->bi_sector));
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,32)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36)
+#define bio_barrier(bio)        ((bio)->bi_rw & (1 << BIO_RW_BARRIER))
+#else
+#define bio_barrier(bio)        ((bio)->bi_rw & REQ_HARDBARRIER)
+#endif
+#endif
+
+/*
+ * Decide the mapping and perform necessary cache operations for a bio request.
+ */
+int
+flashcache_map(struct dm_target *ti, struct bio *bio,
+	       union map_info *map_context)
+{
+	struct cache_c *dmc = (struct cache_c *) ti->private;
+	int sectors = to_sector(bio->bi_size);
+	int queued;
+
+	if (sectors <= 32)
+		size_hist[sectors]++;
+
+	if (bio_barrier(bio))
+		return -EOPNOTSUPP;
+
+	VERIFY(to_sector(bio->bi_size) <= dmc->block_size);
+
+	if (bio_data_dir(bio) == READ)
+		dmc->flashcache_stats.reads++;
+	else
+		dmc->flashcache_stats.writes++;
+
+	spin_lock_irq(&dmc->cache_spin_lock);
+	if (unlikely(sysctl_pid_do_expiry &&
+		     (dmc->whitelist_head || dmc->blacklist_head)))
+		flashcache_pid_expiry_all_locked(dmc);
+	if ((to_sector(bio->bi_size) != dmc->block_size) ||
+	    (bio_data_dir(bio) == WRITE && flashcache_uncacheable(dmc))) {
+		queued = flashcache_inval_blocks(dmc, bio);
+		spin_unlock_irq(&dmc->cache_spin_lock);
+		if (queued) {
+			if (unlikely(queued < 0))
+				flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		} else {
+			/* Start uncached IO */
+			flashcache_start_uncached_io(dmc, bio);
+		}
+	} else {
+		spin_unlock_irq(&dmc->cache_spin_lock);
+		if (bio_data_dir(bio) == READ)
+			flashcache_read(dmc, bio);
+		else
+			flashcache_write(dmc, bio);
+	}
+	return DM_MAPIO_SUBMITTED;
+}
+
+/* Block sync support functions */
+static void
+flashcache_kcopyd_callback_sync(int read_err, unsigned int write_err, void *context)
+{
+	struct kcached_job *job = (struct kcached_job *)context;
+	struct cache_c *dmc = job->dmc;
+	int index = job->index;
+	unsigned long flags;
+
+	VERIFY(!in_interrupt());
+	DPRINTK("kcopyd_callback_sync: Index %d", index);
+	VERIFY(job->bio == NULL);
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	VERIFY(dmc->cache[index].cache_state & (DISKWRITEINPROG | VALID | DIRTY));
+	if (likely(read_err == 0 && write_err == 0)) {
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		flashcache_md_write(job);
+	} else {
+		if (read_err)
+			read_err = -EIO;
+		if (write_err)
+			write_err = -EIO;
+		/* Disk write failed. We can not purge this cache from flash */
+		DMERR("flashcache: Disk writeback failed ! read error %d write error %d block %lu",
+		      -read_err, -write_err, job->disk.sector);
+		VERIFY(dmc->cache_sets[index / dmc->assoc].clean_inprog > 0);
+		VERIFY(dmc->clean_inprog > 0);
+		dmc->cache_sets[index / dmc->assoc].clean_inprog--;
+		dmc->clean_inprog--;
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		/* Set the error in the job and let do_pending() handle the error */
+		if (read_err) {
+			dmc->flashcache_errors.ssd_read_errors++;
+			job->error = read_err;
+		} else {
+			dmc->flashcache_errors.disk_write_errors++;
+			job->error = write_err;
+		}
+		flashcache_do_pending(job);
+		flashcache_sync_blocks(dmc);  /* Kick off more cleanings */
+		dmc->flashcache_stats.cleanings++;
+	}
+}
+
+static void
+flashcache_dirty_writeback_sync(struct cache_c *dmc, int index)
+{
+	struct kcached_job *job;
+	unsigned long flags;
+	struct cacheblock *cacheblk = &dmc->cache[index];
+	int device_removal = 0;
+
+	VERIFY((cacheblk->cache_state & FALLOW_DOCLEAN) == 0);
+	DPRINTK("flashcache_dirty_writeback_sync: Index %d", index);
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	VERIFY((cacheblk->cache_state & BLOCK_IO_INPROG) == DISKWRITEINPROG);
+	VERIFY(cacheblk->cache_state & DIRTY);
+	dmc->cache_sets[index / dmc->assoc].clean_inprog++;
+	dmc->clean_inprog++;
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	job = new_kcached_job(dmc, NULL, index);
+	/*
+	 * If the device is being (fast) removed, do not kick off any more cleanings.
+	 */
+	if (unlikely(atomic_read(&dmc->remove_in_prog) == FAST_REMOVE)) {
+		DMERR("flashcache: Dirty Writeback (for set cleaning) aborted for device removal, block %lu",
+		      cacheblk->dbn);
+		if (job)
+			flashcache_free_cache_job(job);
+		job = NULL;
+		device_removal = 1;
+	}
+	if (unlikely(job == NULL)) {
+		spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		dmc->cache_sets[index / dmc->assoc].clean_inprog--;
+		dmc->clean_inprog--;
+		flashcache_free_pending_jobs(dmc, cacheblk, -EIO);
+		cacheblk->cache_state &= ~(BLOCK_IO_INPROG);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		if (device_removal == 0)
+			DMERR("flashcache: Dirty Writeback (for sync) failed ! Can't allocate memory, block %lu",
+			      cacheblk->dbn);
+	} else {
+		job->bio = NULL;
+		job->action = WRITEDISK_SYNC;
+		atomic_inc(&dmc->nr_jobs);
+		dmc->flashcache_stats.ssd_reads++;
+		dmc->flashcache_stats.disk_writes++;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+		kcopyd_copy(dmc->kcp_client, &job->cache, 1, &job->disk, 0,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,25)
+			    flashcache_kcopyd_callback_sync,
+#else
+			    (kcopyd_notify_fn) flashcache_kcopyd_callback_sync,
+#endif
+			    job);
+#else
+		dm_kcopyd_copy(dmc->kcp_client, &job->cache, 1, &job->disk, 0,
+			       (dm_kcopyd_notify_fn)flashcache_kcopyd_callback_sync,
+			       (void *)job);
+#endif
+	}
+}
+
+/*
+ * Sync all dirty blocks. We pick off dirty blocks, sort them, merge them with
+ * any contigous blocks we can within the set and fire off the writes.
+ */
+void
+flashcache_sync_blocks(struct cache_c *dmc)
+{
+	unsigned long flags;
+	int index;
+	struct dbn_index_pair *writes_list;
+	int nr_writes;
+	int i, set;
+	struct cacheblock *cacheblk;
+
+	/*
+	 * If a (fast) removal of this device is in progress, don't kick off
+	 * any more cleanings. This isn't sufficient though. We still need to
+	 * stop cleanings inside flashcache_dirty_writeback_sync() because we could
+	 * have started a device remove after tested this here.
+	 */
+	if ((atomic_read(&dmc->remove_in_prog) == FAST_REMOVE) || sysctl_flashcache_stop_sync)
+		return;
+	writes_list = kmalloc(dmc->assoc * sizeof(struct dbn_index_pair), GFP_NOIO);
+	if (writes_list == NULL) {
+		dmc->flashcache_errors.memory_alloc_errors++;
+		return;
+	}
+	nr_writes = 0;
+	set = -1;
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	index = dmc->sync_index;
+	while (index < dmc->size &&
+	       (nr_writes + dmc->clean_inprog) < dmc->max_clean_ios_total) {
+		VERIFY(nr_writes <= dmc->assoc);
+		if (((index % dmc->assoc) == 0) && (nr_writes > 0)) {
+			/*
+			 * Crossing a set, sort/merge all the IOs collected so
+			 * far and issue the writes.
+			 */
+			VERIFY(set != -1);
+			flashcache_merge_writes(dmc, writes_list, &nr_writes, set);
+			spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+			for (i = 0 ; i < nr_writes ; i++)
+				flashcache_dirty_writeback_sync(dmc, writes_list[i].index);
+			nr_writes = 0;
+			set = -1;
+			spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+		}
+		cacheblk = &dmc->cache[index];
+		if ((cacheblk->cache_state & (DIRTY | BLOCK_IO_INPROG)) == DIRTY) {
+			cacheblk->cache_state |= DISKWRITEINPROG;
+			flashcache_clear_fallow(dmc, index);
+			writes_list[nr_writes].dbn = cacheblk->dbn;
+			writes_list[nr_writes].index = index;
+			set = index / dmc->assoc;
+			nr_writes++;
+		}
+		index++;
+	}
+	dmc->sync_index = index;
+	if (nr_writes > 0) {
+		VERIFY(set != -1);
+		flashcache_merge_writes(dmc, writes_list, &nr_writes, set);
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+		for (i = 0 ; i < nr_writes ; i++)
+			flashcache_dirty_writeback_sync(dmc, writes_list[i].index);
+	} else
+		spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	kfree(writes_list);
+}
+
+void
+flashcache_sync_all(struct cache_c *dmc)
+{
+	unsigned long flags;
+
+	sysctl_flashcache_stop_sync = 0;
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	dmc->sync_index = 0;
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	flashcache_sync_blocks(dmc);
+}
+
+/*
+ * We handle uncached IOs ourselves to deal with the problem of out of ordered
+ * IOs corrupting the cache. Consider the case where we get 2 concurent IOs
+ * for the same block Write-Read (or a Write-Write). Consider the case where
+ * the first Write is uncacheable and the second IO is cacheable. If the
+ * 2 IOs are out-of-ordered below flashcache, then we will cache inconsistent
+ * data in flashcache (persistently).
+ *
+ * We do invalidations before launching uncacheable IOs to disk. But in case
+ * of out of ordering the invalidations before launching the IOs does not help.
+ * We need to invalidate after the IO completes.
+ *
+ * Doing invalidations after the completion of an uncacheable IO will cause
+ * any overlapping dirty blocks in the cache to be written out and the IO
+ * relaunched. If the overlapping blocks are busy, the IO is relaunched to
+ * disk also (post invalidation). In these 2 cases, we will end up sending
+ * 2 disk IOs for the block. But this is a rare case.
+ *
+ * When 2 IOs for the same block are sent down (by un co-operating processes)
+ * the storage stack is allowed to re-order the IOs at will. So the applications
+ * cannot expect any ordering at all.
+ *
+ * What we try to avoid here is inconsistencies between disk and the ssd cache.
+ */
+void
+flashcache_uncached_io_complete(struct kcached_job *job)
+{
+	struct cache_c *dmc = job->dmc;
+	unsigned long flags;
+	int queued;
+	int error = job->error;
+
+	if (unlikely(error)) {
+		DMERR("flashcache uncached disk IO error: io error %d block %lu R/w %s",
+		      error, job->disk.sector,
+		      (bio_data_dir(job->bio) == WRITE) ? "WRITE" : "READ");
+		if (bio_data_dir(job->bio) == WRITE)
+			dmc->flashcache_errors.disk_write_errors++;
+		else
+			dmc->flashcache_errors.disk_read_errors++;
+	}
+	spin_lock_irqsave(&dmc->cache_spin_lock, flags);
+	queued = flashcache_inval_blocks(dmc, job->bio);
+	spin_unlock_irqrestore(&dmc->cache_spin_lock, flags);
+	if (queued) {
+		if (unlikely(queued < 0))
+			flashcache_bio_endio(job->bio, -EIO, dmc, NULL);
+		/*
+		 * The IO will be re-executed.
+		 * The do_pending logic will re-launch the
+		 * disk IO post-invalidation calling start_uncached_io.
+		 * This should be a rare occurrence.
+		 */
+		dmc->flashcache_stats.uncached_io_requeue++;
+	} else {
+		flashcache_bio_endio(job->bio, error, dmc, &job->io_start_time);
+	}
+	flashcache_free_cache_job(job);
+	if (atomic_dec_and_test(&dmc->nr_jobs))
+		wake_up(&dmc->destroyq);
+}
+
+static void
+flashcache_uncached_io_callback(unsigned long error, void *context)
+{
+	struct kcached_job *job = (struct kcached_job *) context;
+
+	VERIFY(job->index == -1);
+	if (unlikely(error))
+		job->error = -EIO;
+	else
+		job->error = 0;
+	push_uncached_io_complete(job);
+	schedule_work(&_kcached_wq);
+}
+
+static void
+flashcache_start_uncached_io(struct cache_c *dmc, struct bio *bio)
+{
+	int is_write = (bio_data_dir(bio) == WRITE);
+	struct kcached_job *job;
+
+	if (is_write) {
+		dmc->flashcache_stats.uncached_writes++;
+		dmc->flashcache_stats.disk_writes++;
+	} else {
+		dmc->flashcache_stats.uncached_reads++;
+		dmc->flashcache_stats.disk_reads++;
+	}
+	job = new_kcached_job(dmc, bio, -1);
+	if (unlikely(job == NULL)) {
+		flashcache_bio_endio(bio, -EIO, dmc, NULL);
+		return;
+	}
+	atomic_inc(&dmc->nr_jobs);
+	dm_io_async_bvec(1, &job->disk,
+			 ((is_write) ? WRITE : READ),
+			 bio->bi_io_vec + bio->bi_idx,
+			 flashcache_uncached_io_callback, job);
+}
+
+EXPORT_SYMBOL(flashcache_io_callback);
+EXPORT_SYMBOL(flashcache_do_pending_error);
+EXPORT_SYMBOL(flashcache_do_pending_noerror);
+EXPORT_SYMBOL(flashcache_do_pending);
+EXPORT_SYMBOL(flashcache_do_io);
+EXPORT_SYMBOL(flashcache_map);
+EXPORT_SYMBOL(flashcache_write);
+EXPORT_SYMBOL(flashcache_inval_blocks);
+EXPORT_SYMBOL(flashcache_inval_block_set);
+EXPORT_SYMBOL(flashcache_read);
+EXPORT_SYMBOL(flashcache_read_miss);
+EXPORT_SYMBOL(flashcache_clean_set);
+EXPORT_SYMBOL(flashcache_dirty_writeback);
+EXPORT_SYMBOL(flashcache_kcopyd_callback);
+EXPORT_SYMBOL(flashcache_lookup);
+EXPORT_SYMBOL(flashcache_alloc_md_sector);
+EXPORT_SYMBOL(flashcache_free_md_sector);
+EXPORT_SYMBOL(flashcache_md_write_callback);
+EXPORT_SYMBOL(flashcache_md_write_kickoff);
+EXPORT_SYMBOL(flashcache_md_write_done);
+EXPORT_SYMBOL(flashcache_md_write);
+
+
Index: linux-2.6.32-131.0.15.el6/drivers/md/flashcache_subr.c
===================================================================
--- /dev/null
+++ linux-2.6.32-131.0.15.el6/drivers/md/flashcache_subr.c
@@ -0,0 +1,803 @@
+/****************************************************************************
+ *  flashcache_subr.c
+ *  FlashCache: Device mapper target for block-level disk caching
+ *
+ *  Copyright 2010 Facebook, Inc.
+ *  Author: Mohan Srinivasan (mohan@fb.com)
+ *
+ *  Based on DM-Cache:
+ *   Copyright (C) International Business Machines Corp., 2006
+ *   Author: Ming Zhao (mingzhao@ufl.edu)
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; under version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ ****************************************************************************/
+
+#include <asm/atomic.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/list.h>
+#include <linux/blkdev.h>
+#include <linux/bio.h>
+#include <linux/slab.h>
+#include <linux/hash.h>
+#include <linux/spinlock.h>
+#include <linux/workqueue.h>
+#include <linux/pagemap.h>
+#include <linux/random.h>
+#include <linux/hardirq.h>
+#include <linux/sysctl.h>
+#include <linux/version.h>
+#include <linux/sort.h>
+#include <linux/time.h>
+#include <asm/kmap_types.h>
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+#include "dm.h"
+#include "dm-io.h"
+#include "dm-bio-list.h"
+#include "kcopyd.h"
+#else
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,27)
+#include "dm.h"
+#endif
+#include <linux/device-mapper.h>
+#include <linux/bio.h>
+#include <linux/dm-kcopyd.h>
+#endif
+#include "flashcache.h"
+
+static DEFINE_SPINLOCK(_job_lock);
+
+extern mempool_t *_job_pool;
+extern mempool_t *_pending_job_pool;
+
+extern atomic_t nr_cache_jobs;
+extern atomic_t nr_pending_jobs;
+
+LIST_HEAD(_pending_jobs);
+LIST_HEAD(_io_jobs);
+LIST_HEAD(_md_io_jobs);
+LIST_HEAD(_md_complete_jobs);
+LIST_HEAD(_uncached_io_complete_jobs);
+
+int
+flashcache_pending_empty(void)
+{
+	return list_empty(&_pending_jobs);
+}
+
+int
+flashcache_io_empty(void)
+{
+	return list_empty(&_io_jobs);
+}
+
+int
+flashcache_md_io_empty(void)
+{
+	return list_empty(&_md_io_jobs);
+}
+
+int
+flashcache_md_complete_empty(void)
+{
+	return list_empty(&_md_complete_jobs);
+}
+
+int
+flashcache_uncached_io_complete_empty(void)
+{
+	return list_empty(&_uncached_io_complete_jobs);
+}
+
+struct kcached_job *
+flashcache_alloc_cache_job(void)
+{
+	struct kcached_job *job;
+
+	job = mempool_alloc(_job_pool, GFP_NOIO);
+	if (likely(job))
+		atomic_inc(&nr_cache_jobs);
+	return job;
+}
+
+void
+flashcache_free_cache_job(struct kcached_job *job)
+{
+	mempool_free(job, _job_pool);
+	atomic_dec(&nr_cache_jobs);
+}
+
+struct pending_job *
+flashcache_alloc_pending_job(struct cache_c *dmc)
+{
+	struct pending_job *job;
+
+	job = mempool_alloc(_pending_job_pool, GFP_ATOMIC);
+	if (likely(job))
+		atomic_inc(&nr_pending_jobs);
+	else
+		dmc->flashcache_errors.memory_alloc_errors++;
+	return job;
+}
+
+void
+flashcache_free_pending_job(struct pending_job *job)
+{
+	mempool_free(job, _pending_job_pool);
+	atomic_dec(&nr_pending_jobs);
+}
+
+#define FLASHCACHE_PENDING_JOB_HASH(INDEX)		((INDEX) % PENDING_JOB_HASH_SIZE)
+
+void
+flashcache_enq_pending(struct cache_c *dmc, struct bio* bio,
+		       int index, int action, struct pending_job *job)
+{
+	struct pending_job **head;
+
+	head = &dmc->pending_job_hashbuckets[FLASHCACHE_PENDING_JOB_HASH(index)];
+	DPRINTK("flashcache_enq_pending: Queue to pending Q Index %d %llu",
+		index, bio->bi_sector);
+	VERIFY(job != NULL);
+	job->action = action;
+	job->index = index;
+	job->bio = bio;
+	job->prev = NULL;
+	job->next = *head;
+	if (*head)
+		(*head)->prev = job;
+	*head = job;
+	dmc->cache[index].nr_queued++;
+	dmc->flashcache_stats.enqueues++;
+	dmc->pending_jobs_count++;
+}
+
+/*
+ * Deq and move all pending jobs that match the index for this slot to list returned
+ */
+struct pending_job *
+flashcache_deq_pending(struct cache_c *dmc, int index)
+{
+	struct pending_job *node, *next, *movelist = NULL;
+	int moved = 0;
+	struct pending_job **head;
+
+	VERIFY(spin_is_locked(&dmc->cache_spin_lock));
+	head = &dmc->pending_job_hashbuckets[FLASHCACHE_PENDING_JOB_HASH(index)];
+	for (node = *head ; node != NULL ; node = next) {
+		next = node->next;
+		if (node->index == index) {
+			/*
+			 * Remove pending job from the global list of
+			 * jobs and move it to the private list for freeing
+			 */
+			if (node->prev == NULL) {
+				*head = node->next;
+				if (node->next)
+					node->next->prev = NULL;
+			} else
+				node->prev->next = node->next;
+			if (node->next == NULL) {
+				if (node->prev)
+					node->prev->next = NULL;
+			} else
+				node->next->prev = node->prev;
+			node->prev = NULL;
+			node->next = movelist;
+			movelist = node;
+			moved++;
+		}
+	}
+	VERIFY(dmc->pending_jobs_count >= moved);
+	dmc->pending_jobs_count -= moved;
+	return movelist;
+}
+
+#ifdef FLASHCACHE_DO_CHECKSUMS
+int
+flashcache_read_compute_checksum(struct cache_c *dmc, int index, void *block)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+	struct io_region where;
+#else
+	struct dm_io_region where;
+#endif
+	int error;
+	u_int64_t sum = 0, *idx;
+	int cnt;
+
+	where.bdev = dmc->cache_dev->bdev;
+	where.sector = INDEX_TO_CACHE_ADDR(dmc, index);
+	where.count = dmc->block_size;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+	error = flashcache_dm_io_sync_vm(dmc, &where, READ, block);
+#else
+	error = flashcache_dm_io_sync_vm(dmc, &where, READ, block);
+#endif
+	if (error)
+		return error;
+	cnt = dmc->block_size * 512;
+	idx = (u_int64_t *)block;
+	while (cnt > 0) {
+		sum += *idx++;
+		cnt -= sizeof(u_int64_t);
+	}
+	dmc->cache[index].checksum = sum;
+	return 0;
+}
+
+u_int64_t
+flashcache_compute_checksum(struct bio *bio)
+{
+	int i;
+	u_int64_t sum = 0, *idx;
+	int cnt;
+	int kmap_type;
+	void *kvaddr;
+
+	if (in_interrupt())
+		kmap_type = KM_SOFTIRQ0;
+	else
+		kmap_type = KM_USER0;
+	for (i = bio->bi_idx ; i < bio->bi_vcnt ; i++) {
+		kvaddr = kmap_atomic(bio->bi_io_vec[i].bv_page, kmap_type);
+		idx = (u_int64_t *)
+			((char *)kvaddr + bio->bi_io_vec[i].bv_offset);
+		cnt = bio->bi_io_vec[i].bv_len;
+		while (cnt > 0) {
+			sum += *idx++;
+			cnt -= sizeof(u_int64_t);
+		}
+		kunmap_atomic(kvaddr, kmap_type);
+	}
+	return sum;
+}
+
+void
+flashcache_store_checksum(struct kcached_job *job)
+{
+	u_int64_t sum;
+	unsigned long flags;
+
+	sum = flashcache_compute_checksum(job->bio);
+	spin_lock_irqsave(&job->dmc->cache_spin_lock, flags);
+	job->dmc->cache[job->index].checksum = sum;
+	spin_unlock_irqrestore(&job->dmc->cache_spin_lock, flags);
+}
+
+int
+flashcache_validate_checksum(struct kcached_job *job)
+{
+	u_int64_t sum;
+	int retval;
+	unsigned long flags;
+
+	sum = flashcache_compute_checksum(job->bio);
+	spin_lock_irqsave(&job->dmc->cache_spin_lock, flags);
+	if (likely(job->dmc->cache[job->index].checksum == sum)) {
+		job->dmc->flashcache_stats.checksum_valid++;
+		retval = 0;
+	} else {
+		job->dmc->flashcache_stats.checksum_invalid++;
+		retval = 1;
+	}
+	spin_unlock_irqrestore(&job->dmc->cache_spin_lock, flags);
+	return retval;
+}
+#endif
+
+/*
+ * Functions to push and pop a job onto the head of a given job list.
+ */
+struct kcached_job *
+pop(struct list_head *jobs)
+{
+	struct kcached_job *job = NULL;
+	unsigned long flags;
+
+	spin_lock_irqsave(&_job_lock, flags);
+	if (!list_empty(jobs)) {
+		job = list_entry(jobs->next, struct kcached_job, list);
+		list_del(&job->list);
+	}
+	spin_unlock_irqrestore(&_job_lock, flags);
+	return job;
+}
+
+void
+push(struct list_head *jobs, struct kcached_job *job)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&_job_lock, flags);
+	list_add_tail(&job->list, jobs);
+	spin_unlock_irqrestore(&_job_lock, flags);
+}
+
+void
+push_pending(struct kcached_job *job)
+{
+	push(&_pending_jobs, job);
+}
+
+void
+push_io(struct kcached_job *job)
+{
+	push(&_io_jobs, job);
+}
+
+void
+push_uncached_io_complete(struct kcached_job *job)
+{
+	push(&_uncached_io_complete_jobs, job);
+}
+
+void
+push_md_io(struct kcached_job *job)
+{
+	push(&_md_io_jobs, job);
+}
+
+void
+push_md_complete(struct kcached_job *job)
+{
+	push(&_md_complete_jobs, job);
+}
+
+static void
+process_jobs(struct list_head *jobs,
+	     void (*fn) (struct kcached_job *))
+{
+	struct kcached_job *job;
+
+	while ((job = pop(jobs)))
+		(void)fn(job);
+}
+
+void
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+do_work(void *unused)
+#else
+do_work(struct work_struct *unused)
+#endif
+{
+	process_jobs(&_md_complete_jobs, flashcache_md_write_done);
+	process_jobs(&_pending_jobs, flashcache_do_pending);
+	process_jobs(&_md_io_jobs, flashcache_md_write_kickoff);
+	process_jobs(&_io_jobs, flashcache_do_io);
+	process_jobs(&_uncached_io_complete_jobs, flashcache_uncached_io_complete);
+}
+
+extern int sysctl_flashcache_lat_hist;
+
+struct kcached_job *
+new_kcached_job(struct cache_c *dmc, struct bio* bio, int index)
+{
+	struct kcached_job *job;
+
+	job = flashcache_alloc_cache_job();
+	if (unlikely(job == NULL)) {
+		dmc->flashcache_errors.memory_alloc_errors++;
+		return NULL;
+	}
+	job->dmc = dmc;
+	job->index = index;
+	job->cache.bdev = dmc->cache_dev->bdev;
+	if (index != -1) {
+		job->cache.sector = INDEX_TO_CACHE_ADDR(dmc, index);
+		job->cache.count = dmc->block_size;
+	}
+	job->error = 0;
+	job->bio = bio;
+	job->disk.bdev = dmc->disk_dev->bdev;
+	if (index != -1) {
+		job->disk.sector = dmc->cache[index].dbn;
+		job->disk.count = dmc->block_size;
+	} else {
+		job->disk.sector = bio->bi_sector;
+		job->disk.count = to_sector(bio->bi_size);
+	}
+	job->next = NULL;
+	job->md_block = NULL;
+	if (sysctl_flashcache_lat_hist)
+		do_gettimeofday(&job->io_start_time);
+	else {
+		job->io_start_time.tv_sec = 0;
+		job->io_start_time.tv_usec = 0;
+	}
+	return job;
+}
+
+static void
+flashcache_record_latency(struct cache_c *dmc, struct timeval *start_tv)
+{
+	struct timeval latency;
+	int64_t us;
+
+	do_gettimeofday(&latency);
+	latency.tv_sec -= start_tv->tv_sec;
+	latency.tv_usec -= start_tv->tv_usec;
+	us = latency.tv_sec * USEC_PER_SEC + latency.tv_usec;
+	us /= IO_LATENCY_GRAN_USECS;	/* histogram 250us gran, scale 10ms total */
+	if (us < IO_LATENCY_BUCKETS)
+		/* < 10ms latency, track it */
+		dmc->latency_hist[us]++;
+	else
+		/* else count it in 10ms+ bucket */
+		dmc->latency_hist_10ms++;
+}
+
+void
+flashcache_bio_endio(struct bio *bio, int error,
+		     struct cache_c *dmc, struct timeval *start_time)
+{
+	if (unlikely(sysctl_flashcache_lat_hist &&
+		     start_time != NULL &&
+		     start_time->tv_sec != 0))
+		flashcache_record_latency(dmc, start_time);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,24)
+	bio_endio(bio, bio->bi_size, error);
+#else
+	bio_endio(bio, error);
+#endif
+}
+
+void
+flashcache_reclaim_lru_movetail(struct cache_c *dmc, int index)
+{
+	int set = index / dmc->assoc;
+	int start_index = set * dmc->assoc;
+	int my_index = index - start_index;
+	struct cacheblock *cacheblk = &dmc->cache[index];
+
+	/* Remove from LRU */
+	if (likely((cacheblk->lru_prev != FLASHCACHE_LRU_NULL) ||
+		   (cacheblk->lru_next != FLASHCACHE_LRU_NULL))) {
+		if (cacheblk->lru_prev != FLASHCACHE_LRU_NULL)
+			dmc->cache[cacheblk->lru_prev + start_index].lru_next =
+				cacheblk->lru_next;
+		else
+			dmc->cache_sets[set].lru_head = cacheblk->lru_next;
+		if (cacheblk->lru_next != FLASHCACHE_LRU_NULL)
+			dmc->cache[cacheblk->lru_next + start_index].lru_prev =
+				cacheblk->lru_prev;
+		else
+			dmc->cache_sets[set].lru_tail = cacheblk->lru_prev;
+	}
+	/* And add it to LRU Tail */
+	cacheblk->lru_next = FLASHCACHE_LRU_NULL;
+	cacheblk->lru_prev = dmc->cache_sets[set].lru_tail;
+	if (dmc->cache_sets[set].lru_tail == FLASHCACHE_LRU_NULL)
+		dmc->cache_sets[set].lru_head = my_index;
+	else
+		dmc->cache[dmc->cache_sets[set].lru_tail + start_index].lru_next =
+			my_index;
+	dmc->cache_sets[set].lru_tail = my_index;
+}
+
+static int
+cmp_dbn(const void *a, const void *b)
+{
+	if (((struct dbn_index_pair *)a)->dbn < ((struct dbn_index_pair *)b)->dbn)
+		return -1;
+	else
+		return 1;
+}
+
+static void
+swap_dbn_index_pair(void *a, void *b, int size)
+{
+	struct dbn_index_pair temp;
+
+	temp = *(struct dbn_index_pair *)a;
+	*(struct dbn_index_pair *)a = *(struct dbn_index_pair *)b;
+	*(struct dbn_index_pair *)b = temp;
+}
+
+extern int sysctl_flashcache_write_merge;
+
+/*
+ * We have a list of blocks to write out to disk.
+ * 1) Sort the blocks by dbn.
+ * 2) (sysctl'able) See if there are any other blocks in the same set
+ * that are contig to any of the blocks in step 1. If so, include them
+ * in our "to write" set, maintaining sorted order.
+ * Has to be called under the cache spinlock !
+ */
+void
+flashcache_merge_writes(struct cache_c *dmc, struct dbn_index_pair *writes_list,
+			int *nr_writes, int set)
+{
+	int start_index = set * dmc->assoc;
+	int end_index = start_index + dmc->assoc;
+	int old_writes = *nr_writes;
+	int new_inserts = 0;
+	struct dbn_index_pair *set_dirty_list = NULL;
+	int ix, nr_set_dirty;
+	struct cacheblock *cacheblk;
+
+	if (unlikely(*nr_writes == 0))
+		return;
+	sort(writes_list, *nr_writes, sizeof(struct dbn_index_pair),
+	     cmp_dbn, swap_dbn_index_pair);
+	if (sysctl_flashcache_write_merge == 0)
+		return;
+	set_dirty_list = kmalloc(dmc->assoc * sizeof(struct dbn_index_pair), GFP_ATOMIC);
+	if (set_dirty_list == NULL) {
+		dmc->flashcache_errors.memory_alloc_errors++;
+		goto out;
+	}
+	nr_set_dirty = 0;
+	for (ix = start_index ; ix < end_index ; ix++) {
+		cacheblk = &dmc->cache[ix];
+		/*
+		 * Any DIRTY block in "writes_list" will be marked as
+		 * DISKWRITEINPROG already, so we'll skip over those here.
+		 */
+		if ((cacheblk->cache_state & (DIRTY | BLOCK_IO_INPROG)) == DIRTY) {
+			set_dirty_list[nr_set_dirty].dbn = cacheblk->dbn;
+			set_dirty_list[nr_set_dirty].index = ix;
+			nr_set_dirty++;
+		}
+	}
+	if (nr_set_dirty == 0)
+		goto out;
+	sort(set_dirty_list, nr_set_dirty, sizeof(struct dbn_index_pair),
+	     cmp_dbn, swap_dbn_index_pair);
+	for (ix = 0 ; ix < nr_set_dirty ; ix++) {
+		int back_merge, k;
+		int i;
+
+		cacheblk = &dmc->cache[set_dirty_list[ix].index];
+		back_merge = -1;
+		VERIFY((cacheblk->cache_state & (DIRTY | BLOCK_IO_INPROG)) == DIRTY);
+		for (i = 0 ; i < *nr_writes ; i++) {
+			int insert;
+			int j = 0;
+
+			insert = 0;
+			if (cacheblk->dbn + dmc->block_size == writes_list[i].dbn) {
+				/* cacheblk to be inserted above i */
+				insert = 1;
+				j = i;
+				back_merge = j;
+			}
+			if (cacheblk->dbn - dmc->block_size == writes_list[i].dbn ) {
+				/* cacheblk to be inserted after i */
+				insert = 1;
+				j = i + 1;
+			}
+			VERIFY(j < dmc->assoc);
+			if (insert) {
+				cacheblk->cache_state |= DISKWRITEINPROG;
+				flashcache_clear_fallow(dmc, set_dirty_list[ix].index);
+				/*
+				 * Shift down everthing from j to ((*nr_writes) - 1) to
+				 * make room for the new entry. And add the new entry.
+				 */
+				for (k = (*nr_writes) - 1 ; k >= j ; k--)
+					writes_list[k + 1] = writes_list[k];
+				writes_list[j].dbn = cacheblk->dbn;
+				writes_list[j].index = cacheblk - &dmc->cache[0];
+				(*nr_writes)++;
+				VERIFY(*nr_writes <= dmc->assoc);
+				new_inserts++;
+				if (back_merge == -1)
+					dmc->flashcache_stats.front_merge++;
+				else
+					dmc->flashcache_stats.back_merge++;
+				VERIFY(*nr_writes <= dmc->assoc);
+				break;
+			}
+		}
+		/*
+		 * If we did a back merge, we need to walk back in the set's dirty list
+		 * to see if we can pick off any more contig blocks. Forward merges don't
+		 * need this special treatment since we are walking the 2 lists in that
+		 * direction. It would be nice to roll this logic into the above.
+		 */
+		if (back_merge != -1) {
+			for (k = ix - 1 ; k >= 0 ; k--) {
+				int n;
+
+				if (set_dirty_list[k].dbn + dmc->block_size !=
+				    writes_list[back_merge].dbn)
+					break;
+				dmc->cache[set_dirty_list[k].index].cache_state |= DISKWRITEINPROG;
+				flashcache_clear_fallow(dmc, set_dirty_list[k].index);
+				for (n = (*nr_writes) - 1 ; n >= back_merge ; n--)
+					writes_list[n + 1] = writes_list[n];
+				writes_list[back_merge].dbn = set_dirty_list[k].dbn;
+				writes_list[back_merge].index = set_dirty_list[k].index;
+				(*nr_writes)++;
+				VERIFY(*nr_writes <= dmc->assoc);
+				new_inserts++;
+				dmc->flashcache_stats.back_merge++;
+				VERIFY(*nr_writes <= dmc->assoc);
+			}
+		}
+	}
+	VERIFY((*nr_writes) == (old_writes + new_inserts));
+out:
+	if (set_dirty_list)
+		kfree(set_dirty_list);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+int
+flashcache_dm_io_async_vm(struct cache_c *dmc, unsigned int num_regions,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+			  struct io_region *where,
+#else
+			  struct dm_io_region *where,
+#endif
+			  int rw,
+			  void *data, io_notify_fn fn, void *context)
+{
+	unsigned long error_bits = 0;
+	int error;
+	struct dm_io_request io_req = {
+		.bi_rw = rw,
+		.mem.type = DM_IO_VMA,
+		.mem.ptr.vma = data,
+		.mem.offset = 0,
+		.notify.fn = fn,
+		.notify.context = context,
+		.client = dmc->io_client,
+	};
+
+	error = dm_io(&io_req, 1, where, &error_bits);
+	if (error)
+		return error;
+	if (error_bits)
+		return error_bits;
+	return 0;
+}
+#endif
+
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,29)
+/*
+ * Wrappers for doing DM sync IO, using DM async IO.
+ * It is a shame we need do this, but DM sync IO is interruptible :(
+ * And we want uninterruptible disk IO :)
+ *
+ * This is fixed in 2.6.30, where sync DM IO is uninterruptible.
+ */
+#define FLASHCACHE_DM_IO_SYNC_INPROG	0x01
+
+static DECLARE_WAIT_QUEUE_HEAD(flashcache_dm_io_sync_waitqueue);
+static DEFINE_SPINLOCK(flashcache_dm_io_sync_spinlock);
+
+struct flashcache_dm_io_sync_state {
+	int			error;
+	int			flags;
+};
+
+static void
+flashcache_dm_io_sync_vm_callback(unsigned long error, void *context)
+{
+	struct flashcache_dm_io_sync_state *state =
+		(struct flashcache_dm_io_sync_state *)context;
+	unsigned long flags;
+
+	spin_lock_irqsave(&flashcache_dm_io_sync_spinlock, flags);
+	state->flags &= ~FLASHCACHE_DM_IO_SYNC_INPROG;
+	state->error = error;
+	wake_up(&flashcache_dm_io_sync_waitqueue);
+	spin_unlock_irqrestore(&flashcache_dm_io_sync_spinlock, flags);
+}
+
+int
+flashcache_dm_io_sync_vm(struct cache_c *dmc,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+			 struct io_region *where,
+#else
+			  struct dm_io_region *where,
+#endif
+			 int rw, void *data)
+{
+        DEFINE_WAIT(wait);
+	struct flashcache_dm_io_sync_state state;
+
+	state.error = -EINTR;
+	state.flags = FLASHCACHE_DM_IO_SYNC_INPROG;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
+	dm_io_async_vm(1, where, rw, data, flashcache_dm_io_sync_vm_callback, &state);
+#else
+	flashcache_dm_io_async_vm(dmc, 1, where, rw, data, flashcache_dm_io_sync_vm_callback, &state);
+#endif
+	flashcache_unplug_device(where->bdev);
+	spin_lock_irq(&flashcache_dm_io_sync_spinlock);
+	while (state.flags & FLASHCACHE_DM_IO_SYNC_INPROG) {
+		prepare_to_wait(&flashcache_dm_io_sync_waitqueue, &wait,
+				TASK_UNINTERRUPTIBLE);
+		spin_unlock_irq(&flashcache_dm_io_sync_spinlock);
+		schedule();
+		spin_lock_irq(&flashcache_dm_io_sync_spinlock);
+	}
+	finish_wait(&flashcache_dm_io_sync_waitqueue, &wait);
+	spin_unlock_irq(&flashcache_dm_io_sync_spinlock);
+	return state.error;
+}
+#else
+int
+flashcache_dm_io_sync_vm(struct cache_c *dmc, struct dm_io_region *where, int rw, void *data)
+{
+	unsigned long error_bits = 0;
+	int error;
+	struct dm_io_request io_req = {
+		.bi_rw = rw,
+		.mem.type = DM_IO_VMA,
+		.mem.ptr.vma = data,
+		.mem.offset = 0,
+		.notify.fn = NULL,
+		.client = dmc->io_client,
+	};
+
+	error = dm_io(&io_req, 1, where, &error_bits);
+	if (error)
+		return error;
+	if (error_bits)
+		return error_bits;
+	return 0;
+}
+#endif
+
+void
+flashcache_update_sync_progress(struct cache_c *dmc)
+{
+	u_int64_t dirty_pct;
+
+	if (dmc->flashcache_stats.cleanings % 1000)
+		return;
+	if (!dmc->nr_dirty || !dmc->size || !printk_ratelimit())
+		return;
+	dirty_pct = ((u_int64_t)dmc->nr_dirty * 100) / dmc->size;
+	printk(KERN_INFO "Flashcache: Cleaning %d Dirty blocks, Dirty Blocks pct %llu%%",
+	       dmc->nr_dirty, dirty_pct);
+	printk(KERN_INFO "\r");
+}
+
+void
+flashcache_unplug_device(struct block_device *bdev)
+{
+	struct backing_dev_info *bdi;
+
+	bdi = blk_get_backing_dev_info(bdev);
+	if (bdi) {
+		if (bdi->unplug_io_fn)
+			blk_run_backing_dev(bdi, NULL);
+	}
+}
+
+EXPORT_SYMBOL(flashcache_alloc_cache_job);
+EXPORT_SYMBOL(flashcache_free_cache_job);
+EXPORT_SYMBOL(flashcache_alloc_pending_job);
+EXPORT_SYMBOL(flashcache_free_pending_job);
+EXPORT_SYMBOL(pop);
+EXPORT_SYMBOL(push);
+EXPORT_SYMBOL(push_pending);
+EXPORT_SYMBOL(push_io);
+EXPORT_SYMBOL(push_md_io);
+EXPORT_SYMBOL(push_md_complete);
+EXPORT_SYMBOL(process_jobs);
+EXPORT_SYMBOL(do_work);
+EXPORT_SYMBOL(new_kcached_job);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+EXPORT_SYMBOL(flashcache_dm_io_sync_vm_callback);
+#endif
+EXPORT_SYMBOL(flashcache_dm_io_sync_vm);
+EXPORT_SYMBOL(flashcache_reclaim_lru_movetail);
+EXPORT_SYMBOL(flashcache_merge_writes);
+EXPORT_SYMBOL(flashcache_enq_pending);
